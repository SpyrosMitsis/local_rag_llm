{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad7c728c-2efc-47e7-9770-7b7323a7135e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
    "from transformers import TextStreamer\n",
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e311aac9-2a26-450a-aebd-cb119611e89d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d3c7564d6c94284a85ee60b1eb8bc2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"google/gemma-2-2b-it\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                        bnb_4bit_compute_dtype=torch.float16)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2-2b-it\",\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=quantization_config,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "chat = [\n",
    "    { \"role\": \"user\", \"content\": \"Random prompt.\"},\n",
    "]\n",
    "question = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "question = tokenizer(question, return_tensors=\"pt\").to(device)\n",
    "\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb2e2bde-f057-43bf-8e1f-20df8b13b5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[     2,      2,    106,   1645,    108,  13020,  18335, 235265,    107,\n",
      "            108,    106,   2516,    108]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "print(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b0d0386-0375-46ca-b5a1-0110f5aaafd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Prompt: \n",
      "\n",
      "**Imagine you are a time traveler from the year 2347, visiting Earth in 2023.** What is your first impression of this place?  What do you find surprising or fascinating about it compared to what you expected based on historical records and projections for our future?\n",
      "\n",
      "\n",
      "Let me know how I can help with any details! ðŸ˜Š \n",
      "<end_of_turn>\n"
     ]
    }
   ],
   "source": [
    "_ = model.generate(**question, streamer=streamer,\n",
    "                            pad_token_id=tokenizer.eos_token_id,\n",
    "                            temperature=0.1,\n",
    "                            max_length=2048,\n",
    "                            do_sample=True,\n",
    "                            top_p=0.9,\n",
    "                            repetition_penalty=1.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff8d248b-d57c-4e99-81a5-ddf69fdde716",
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_text = tokenizer.decode(_[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3b36d57-8ffe-48c5-80b8-44da1926049d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "Random prompt.\n",
      "model\n",
      "## Prompt: \n",
      "\n",
      "**Imagine you are a time traveler from the year 2347, visiting Earth in 2023.** What is your first impression of this world?  \n",
      "\n",
      "\n",
      "Let me know what kind of details and thoughts come to mind! I'm excited to see how we can explore this fascinating scenario together. ðŸ˜Š ðŸš€ ðŸŒŽ ðŸ¤” \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(decode_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fbb4594-7f6c-4592-9dc4-2eba599bca69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([     2,      2,    106,   1645,    108,  13020,  18335, 235265,    107,\n",
       "           108,    106,   2516,    108], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question[\"input_ids\"][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_scratch",
   "language": "python",
   "name": "rag_scratch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
