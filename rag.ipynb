{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23ca1766-070c-418e-afdf-4433cc90dbdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[INFO]\u001b[0m This is an info\n",
      "\u001b[31m[ERROR]\u001b[0m This is an error\n"
     ]
    }
   ],
   "source": [
    "from colorama import Fore, Style\n",
    "\n",
    "def print_message(message_type, message):\n",
    "    if message_type == \"INFO\":\n",
    "        print(f\"{Fore.YELLOW}[INFO]{Style.RESET_ALL} {message}\")\n",
    "    elif message_type == \"ERROR\":\n",
    "        print(f\"{Fore.RED}[ERROR]{Style.RESET_ALL} {message}\")\n",
    "    elif message_type == \"SUCCESS\":\n",
    "        print(f\"{Fore.GREEN}[SUCESS]{Style.RESET_ALL} {message}\")\n",
    "    else:\n",
    "        print(f\"{message}\")\n",
    "\n",
    "print_message(\"INFO\", \"This is an info\")\n",
    "print_message(\"ERROR\", \"This is an error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d5341e2-ee6b-4d2a-bcca-53a9b2fed3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae0e13a6-bed5-4292-b542-f4c377373be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file_with_progress(url, output_path):\n",
    "   \n",
    "    response = requests.get(url, stream=True)\n",
    "    if response.status_code == 200:\n",
    "        total_size = int(response.headers.get('content-length', 0))\n",
    "        with open(output_path, 'wb') as file, tqdm(\n",
    "            desc=output_path,\n",
    "            total=total_size,\n",
    "            unit='B',\n",
    "            unit_scale=True,\n",
    "            unit_divisor=1024,\n",
    "        ) as progress_bar:\n",
    "            for chunk in response.iter_content(chunk_size=1024):\n",
    "                if chunk:\n",
    "                    file.write(chunk)\n",
    "                    progress_bar.update(len(chunk))\n",
    "        print_message(\"SUCCESS\", \"File has been successfully downloaded.\")\n",
    "    else:\n",
    "        print_message(\"ERROR\", f\"Something went wrong. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "090a7d4c-e3c2-45d3-93ce-9538d3d85c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_pdf_file(pdf_file_name: str) -> str:\n",
    "    if pdf_file_name[-4:] != \".pdf\":\n",
    "        pdf_file_name += \".pdf\"\n",
    "    if not os.path.exists(pdf_file_name):\n",
    "        print_message(\"INFO\", \"File doesn't exist, Insert Url here\")\n",
    "        url = input(\">\")\n",
    "\n",
    "        download_file_with_progress(url, pdf_file_name)\n",
    "    else:\n",
    "        print_message(\"SUCCESS\", \"The file already exists\")\n",
    "        return pdf_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9dc1ebb-e479-4156-a0c9-61d7d8a42a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[SUCESS]\u001b[0m The file already exists\n"
     ]
    }
   ],
   "source": [
    "pdf_file_name = \"Pattern Recognition and Machine - Christopher M. Bishop\"\n",
    "pdf_file_name = insert_pdf_file(pdf_file_name=pdf_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8c63aaa-a416-4d07-a965-78271aa6532e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def text_formatter(text: str) -> str:\n",
    "    cleaned_text = text.replace('\\n', ' ').strip()\n",
    "    return cleaned_text\n",
    "\n",
    "def open_and_read_pdf(pdf_file_name: str) -> list[dict]:\n",
    "    doc = fitz.open(pdf_file_name)\n",
    "    pages_and_texts = []\n",
    "    for page_number, page in tqdm(enumerate(doc)):\n",
    "        text = page.get_text()\n",
    "        text = text_formatter(text=text)\n",
    "        pages_and_texts.append({\n",
    "                \"page_number\": page_number + 1,\n",
    "                \"page_char_count\": len(text),\n",
    "                \"page_word_count\": len(text.split(' ')),\n",
    "                \"page_sentence_count_raw\": len(text.split(\". \")),\n",
    "                \"page_token_count\": len(text) / 4,\n",
    "                \"text\": text\n",
    "        })\n",
    "    return pages_and_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c54cc4d-aef6-46c1-b4b0-6f22d87adff1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e8ae929de004a29bf6d0c4c3926aceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pages_and_texts = open_and_read_pdf(pdf_file_name=pdf_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69d9be4-7f7b-4323-8f45-3d697c3281f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_and_embed_pdf_file(pdf_file_name: str):\n",
    "    pdf_file_name = insert_pdf_file(pdf_file_name=pdf_file_name)\n",
    "    pages_and_texts = open_and_read_pdf(pdf_file_name_pdf_file_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b37a8c-e3e8-4659-8258-c86604fa9c4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "random.sample(pages_and_texts, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f75b54-24ae-48c7-b962-a3fa4797e3b5",
   "metadata": {},
   "source": [
    "## Get some more info on the data of the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261e4054-f538-46e4-85b4-717c8f643e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(pages_and_texts)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7e4911-ad7f-4f99-9ec0-4e780e73ae21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2f0843-ed98-4741-9b6b-ead4496a64d4",
   "metadata": {},
   "source": [
    "## Splitting pages into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb064551-ca85-4ee9-ac47-243386fd49c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "doc = nlp(\"This is a sentence. This is another sentence. I like this.\")\n",
    "assert len(list(doc.sents)) == 3\n",
    "\n",
    "list(doc.sents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e26815d-81ca-4bb0-82f9-0f70bf9b6ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in tqdm(pages_and_texts):\n",
    "    item[\"sentences\"] = list(nlp(item['text']).sents)\n",
    "\n",
    "    item['sentences'] = [str(sentence) for sentence in item[\"sentences\"]]\n",
    "    item[\"page_sentence_count_spacy\"] = len(item[\"sentences\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16afd77-3914-4458-839b-f990b6240541",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random.sample(pages_and_texts, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd2c36d-ad80-4440-8af6-903ca30b5bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(pages_and_texts)\n",
    "df.describe().round()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feec6e04-402c-4a0e-ab8c-b2110ff6ef9a",
   "metadata": {},
   "source": [
    "### Making chunks from the text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c55355-dbdd-4d20-8411-9ed50ac48812",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sentence_chunk_size = 10 \n",
    "\n",
    "def split_list(input_list: list, slice_size: int =num_sentence_chunk_size) -> list[str]:\n",
    "    return [input_list[i:i+slice_size + 1] for i in range(0, len(input_list), slice_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969e20aa-3916-426e-ba25-576bcb90b9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "tl = list(range(25))\n",
    "split_list(tl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad95769-5bef-4ee4-a7a5-6bd3db3b00eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in tqdm(pages_and_texts):\n",
    "    item[\"sentence_chunks\"] = split_list(input_list=item[\"sentences\"],\n",
    "                                        slice_size=num_sentence_chunk_size)\n",
    "    item[\"num_chunks\"] = len(item[\"sentence_chunks\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c80c586-d3a6-4b67-833a-52fba584fc9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random.sample(pages_and_texts, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b83eea-fdd9-43be-8097-37e8a6c6ef5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(pages_and_texts)\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8023c3-4672-4b38-914b-c8ca2d061d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "pages_and_chunks = []\n",
    "for i in tqdm(pages_and_texts):\n",
    "    for sentence_chunk in i['sentence_chunks']:\n",
    "        chunk_dict = {}\n",
    "        chunk_dict[\"page_number\"] = i[\"page_number\"]\n",
    "        joined_sentence_chunk = \"\".join(sentence_chunk).replace(\"  \", \" \").strip()\n",
    "        joined_sentence_chunk = re.sub(r'\\.([A-Z])', r'. \\1', joined_sentence_chunk)\n",
    "        chunk_dict[\"sentence_chunk\"] = joined_sentence_chunk\n",
    "        chunk_dict[\"chunk_char_count\"] = len(joined_sentence_chunk)\n",
    "        chunk_dict[\"chunk_word_count\"] = len([word for word in joined_sentence_chunk.split(\" \")])\n",
    "        chunk_dict[\"chunk_token_count\"] = len(joined_sentence_chunk) / 4\n",
    "        pages_and_chunks.append(chunk_dict)\n",
    "    \n",
    "len(pages_and_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150ac884-4e04-43a9-90d5-6e6afe86e9df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pages_and_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641a8851-07aa-46f8-897e-2d263229f77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.sample(pages_and_chunks, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce5516c-c898-42ad-916f-efd7cef1fd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(pages_and_chunks)\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e002c64-19e5-467d-ada7-48623eae3a7b",
   "metadata": {},
   "source": [
    "## Filter out chunks with less than 20 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c435930a-fd6e-4ec9-82d4-f1e2d87b25da",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_token_length = 20\n",
    "for row in df[df[\"chunk_token_count\"] <= min_token_length].sample(5).iterrows():\n",
    "    print(f\"Chunk token count: {row[1]['chunk_token_count']} | Text: {row[1]['sentence_chunk']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177a531f-d5cd-4cac-994b-f678853d5de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_and_chunks_over_min_token_len = df[df[\"chunk_token_count\"] > min_token_length].to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6deb7e-bfd3-4bdb-bf60-c23b1cb7e06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.sample(pages_and_chunks_over_min_token_len, k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5134c9c8-dc3f-442d-9209-d2b7229887f7",
   "metadata": {},
   "source": [
    "## Embedding chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dedbf60-4e9b-4296-a6d4-58c8b9a2b512",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = [\"This is a test for the embedding model\",\n",
    "                 \"this is a second sentence for the model\",\n",
    "                 \"The sky is blue\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9359d1-4266-4602-93bc-53b576e72ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\",\n",
    "                                     device=\"cuda\")\n",
    "\n",
    "\n",
    "embeddings = embedding_model.encode(test_sentences,\n",
    "                                    batch_size=32,\n",
    "                                    convert_to_tensor=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bbdd38-56cd-4327-b98f-ea3f96b6ef0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_and_embeddings = dict(zip(test_sentences, embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6eb9ddc-efee-48f1-b0d6-bc1e46aae203",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentences_and_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c1f030-b6bb-4bd4-9fe6-306d0fd919e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for item in tqdm(pages_and_chunks_over_min_token_len):\n",
    "    item[\"embedding\"] = embedding_model.encode(item[\"sentence_chunk\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff771519-be97-4c74-b7b2-7266c99518ca",
   "metadata": {},
   "source": [
    "# TODO\n",
    "## Creating a chromadb client for storing embeddings\n",
    "\n",
    "> **NOTE** maybe not\n",
    ">\n",
    "> [INFO] Time taken to get scores on 1765000 embeddings: 0.00286 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf22a6f-0c18-4e74-9260-e4344a6e1bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "chroma_client = chromadb.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f45adc-b1fc-4346-949e-de2800e4b845",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = chroma_client.create_collection(name=\"embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba664f6f-cd21-412c-ae83-8611e951ee8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a52e39-3887-433e-a153-3a8d54a8e893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = collection.query(\n",
    "#     query_texts=[\"This is a query document about hawaii\"], # Chroma will embed this for you\n",
    "#     n_results=2 # how many results to return\n",
    "# )\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0021cc9-4e03-4dd1-8e22-813b4a5d31e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "text_chunks = [item[\"sentence_chunk\"] for item in pages_and_chunks_over_min_token_len]\n",
    "random.sample(text_chunks, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61e227f-b4c0-4bcc-81b0-b01891b9ebf7",
   "metadata": {},
   "source": [
    "## adding the embeddings to the chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5ab13e-2673-42b3-b975-7a9b51565a0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pages_and_chunks_over_min_token_len[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219907c5-00b1-4f28-836d-002acb85cc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks = [item[\"sentence_chunk\"] for item in pages_and_chunks_over_min_token_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58305b3-6a9d-4679-975b-44283b91acfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed all the text in batches\n",
    "from time import perf_counter as timer\n",
    "\n",
    "start_time = timer()\n",
    "text_chunks_embeddings = embedding_model.encode(text_chunks,\n",
    "                                               batch_size=32,\n",
    "                                               convert_to_tensor=True)\n",
    "end_time = timer()\n",
    "print(end_time - start_time)\n",
    "text_chunks_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9e7a7b-d735-451a-ada4-0628818f5591",
   "metadata": {},
   "source": [
    "### Saving embeddings into a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26e6a02-5774-44c9-8974-8f728738aa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks_and_embeddings_df = pd.DataFrame(pages_and_chunks_over_min_token_len)\n",
    "embeddings_df_save_path = f\"embeddings/{pdf_file_name}.csv\"\n",
    "text_chunks_and_embeddings_df.to_csv(embeddings_df_save_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54939d30-c850-421b-a354-9e8d941ba877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the csv \n",
    "text_chunks_and_embedding_df_load = pd.read_csv(embeddings_df_save_path)\n",
    "text_chunks_and_embedding_df_load.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab7f457-5ef5-4fb0-bb0c-477467a8622e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks_and_embedding_df_load[\"sentence_chunk\"].iloc[357]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a4e945-7032-4e92-acdc-443cd002cc64",
   "metadata": {},
   "source": [
    "# Rag - Search and Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1133ffac-cc97-497f-970a-9672c723d6a1",
   "metadata": {},
   "source": [
    "### Similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fb65de4-2d15-46d3-bb2b-c2ba49837354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[INFO]\u001b[0m This is an info\n",
      "\u001b[31m[ERROR]\u001b[0m This is an error\n"
     ]
    }
   ],
   "source": [
    "from colorama import Fore, Style\n",
    "\n",
    "def print_message(message_type, message):\n",
    "    if message_type == \"INFO\":\n",
    "        print(f\"{Fore.YELLOW}[INFO]{Style.RESET_ALL} {message}\")\n",
    "    elif message_type == \"ERROR\":\n",
    "        print(f\"{Fore.RED}[ERROR]{Style.RESET_ALL} {message}\")\n",
    "    elif message_type == \"SUCCESS\":\n",
    "        print(f\"{Fore.GREEN}[SUCESS]{Style.RESET_ALL} {message}\")\n",
    "    else:\n",
    "        print(f\"{message}\")\n",
    "\n",
    "print_message(\"INFO\", \"This is an info\")\n",
    "print_message(\"ERROR\", \"This is an error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c109a23-1d71-428b-a1c6-2511acfb5a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_73091/1567020032.py:6: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac7b5cf3-c3e6-447a-97c8-32b388310c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Import texts and embedding df\n",
    "text_chunks_and_embedding_df = pd.read_csv(\"embeddings/1706.03762v7.pdf.csv\")\n",
    "\n",
    "# Convert embedding column back to np.array (it got converted to string when it got saved to CSV)\n",
    "text_chunks_and_embedding_df[\"embedding\"] = text_chunks_and_embedding_df[\"embedding\"].apply(lambda x: np.fromstring(x.strip(\"[]\"), sep=\" \"))\n",
    "\n",
    "# Convert texts and embedding df to list of dicts\n",
    "pages_and_chunks = text_chunks_and_embedding_df.to_dict(orient=\"records\")\n",
    "\n",
    "# Convert embeddings to torch tensor and send to device (note: NumPy arrays are float64, torch tensors are float32 by default)\n",
    "embeddings = torch.tensor(np.array(text_chunks_and_embedding_df[\"embedding\"].tolist()), dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f91100-73b8-4d7f-aaea-89f608e80ac7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_chunks_and_embedding_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ec8893-78cc-4a19-804e-730fdbfb9c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "20d7668c-f397-4dd3-9ce9-baeb61566110",
   "metadata": {},
   "source": [
    "# creeating the model, this is just used if you havent already run the model above\n",
    "\n",
    "from sentence_transformers import util, SentenceTransformer\n",
    "\n",
    "\n",
    "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\", \n",
    "                                      device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f368d299-0b1f-4beb-a85b-0d6159c5a89d",
   "metadata": {},
   "source": [
    "## semantic search pipeline\n",
    "\n",
    "\n",
    "1. Define a query string.\n",
    "2. Turn the query string into an embedding\n",
    "3. Perform a dot product or cosine similarity function between the text embedding and the query embedding\n",
    "4. Sort the results from k in descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "166a5ed3-c7df-4fe5-8393-7db8586e8ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is an encoder?\n",
      "\u001b[33m[INFO]\u001b[0m Time taken to get scores on 41 embeddings: 0.00121 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([0.5157, 0.5099, 0.4239], device='cuda:0'),\n",
       "indices=tensor([5, 4, 9], device='cuda:0'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is an encoder?\"\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "# embed query\n",
    "query_embedding = embedding_model.encode(query, convert_to_tensor=True)\n",
    "\n",
    "\n",
    "# Get similarity scores with dot product (use cosine similarity if outputs are not normalized)\n",
    "\n",
    "from time import perf_counter as timer\n",
    "\n",
    "start_time = timer()\n",
    "dot_scores = util.dot_score(a=query_embedding, b=embeddings)[0]\n",
    "end_time = timer()\n",
    "\n",
    "print_message(\"INFO\", f\"Time taken to get scores on {len(embeddings)} embeddings: {end_time - start_time:.5f} seconds.\")\n",
    "\n",
    "# 4 get top-k results\n",
    "\n",
    "top_results_dot_product = torch.topk(dot_scores, k=3)\n",
    "top_results_dot_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9875324b-12fa-4855-a6a0-4dac4ed793a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Figure 1: The Transformer - model architecture. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.3.1 Encoder and Decoder Stacks Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position- wise fully connected feed-forward network. We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512. Decoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_chunks_and_embedding_df[\"sentence_chunk\"].iloc[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed20aac0-6e8f-4683-8822-b8720e527fad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 309 is out of bounds for dimension 0 with size 41",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dot \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdot(\u001b[43membeddings\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m309\u001b[39;49m\u001b[43m]\u001b[49m, query_embedding)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjust dot prod \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdot\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m dot \u001b[38;5;241m=\u001b[39m dot \u001b[38;5;241m/\u001b[39m (torch\u001b[38;5;241m.\u001b[39msqrt(torch\u001b[38;5;241m.\u001b[39msum(embeddings[\u001b[38;5;241m309\u001b[39m] \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)) \u001b[38;5;241m*\u001b[39m  torch\u001b[38;5;241m.\u001b[39msqrt(torch\u001b[38;5;241m.\u001b[39msum(query_embedding \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m))) \n",
      "\u001b[0;31mIndexError\u001b[0m: index 309 is out of bounds for dimension 0 with size 41"
     ]
    }
   ],
   "source": [
    "dot = torch.dot(embeddings[309], query_embedding)\n",
    "print(f\"just dot prod {dot:.4f}\")\n",
    "dot = dot / (torch.sqrt(torch.sum(embeddings[309] ** 2)) *  torch.sqrt(torch.sum(query_embedding ** 2))) \n",
    "print(f\"cosine similarity {dot:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd742427-23e9-4ec4-9494-0af6e9b5ae42",
   "metadata": {},
   "outputs": [],
   "source": [
    " (torch.sqrt(torch.sum(embeddings[309] ** 2)) *  torch.sqrt(torch.sum(query_embedding ** 2))) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc2f5b1-5b3f-4866-a6b2-36f6c883a538",
   "metadata": {},
   "source": [
    "## Testing $ \\times 1000 $ embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9463df84-6bdf-4124-8dec-27fdc77c7e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "larger_embeddings = torch.rand(1000*embeddings.shape[0], 768).to(device)\n",
    "print(f'Embeddings shape {larger_embeddings.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec32072e-65b7-4333-b546-6e4ff9d9759c",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = timer()\n",
    "dot_scores = util.dot_score(a=query_embedding, b=larger_embeddings)[0]\n",
    "end_time = timer()\n",
    "# print(dot_scores.shape)\n",
    "\n",
    "print_message(\"INFO\", f\"Time taken to get scores on {len(larger_embeddings)} embeddings: {end_time - start_time:.5f} seconds.\")\n",
    "\n",
    "\n",
    "top_results_dot_product = torch.topk(dot_scores, k=100)\n",
    "top_results_dot_product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c99967-d9e0-442d-a518-66a0ce2fb802",
   "metadata": {},
   "source": [
    "### Implementing a Re-Ranker\n",
    "\n",
    "- Re-rank the top k=100 results\n",
    "- Select the top=5 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e3a718-a597-4d35-838b-2660cfcf7f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_results_dot_product[1]\n",
    "top_k_chunks = [text_chunks_and_embedding_df[\"sentence_chunk\"].iloc[int(i)] for i in top_results_dot_product[1]]\n",
    "# top_k_chunks = [i for i in top_results_dot_product[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d518dd-efd2-49ef-b997-bfa72365ba26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_k_chunks[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31c84d9-48b1-4a35-a5f5-3ab40ea6ddec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# Load the model, here we use our base sized model\n",
    "model = CrossEncoder(\"mixedbread-ai/mxbai-rerank-base-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4300c8f4-2727-4390-8139-7ae2dd616c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.rank(query, top_k_chunks, return_documents=True, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b4a251-96db-4855-9995-8e8f87b8865c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37e507f-f01c-47b9-bf8a-93c878904d25",
   "metadata": {},
   "source": [
    "### Functionizing the semantic pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da1957f3-2e34-46cc-ad2a-06c4ed9be650",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embedding_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mretrieve_relevant_resources\u001b[39m(query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m      2\u001b[0m                               embeddings: torch\u001b[38;5;241m.\u001b[39mtensor,\n\u001b[0;32m----> 3\u001b[0m                               model: SentenceTransformer\u001b[38;5;241m=\u001b[39m\u001b[43membedding_model\u001b[49m,\n\u001b[1;32m      4\u001b[0m                               n_resources_to_return: \u001b[38;5;28mint\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m      5\u001b[0m                               print_time: \u001b[38;5;28mbool\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m      6\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m    Embeds the query with a model and returns the top k scores and indices from the embeddings.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     query_embedding \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode(query, convert_to_tensor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'embedding_model' is not defined"
     ]
    }
   ],
   "source": [
    "def retrieve_relevant_resources(query: str,\n",
    "                              embeddings: torch.tensor,\n",
    "                              model: SentenceTransformer=embedding_model,\n",
    "                              n_resources_to_return: int=5,\n",
    "                              print_time: bool=True):\n",
    "    \"\"\"\n",
    "    Embeds the query with a model and returns the top k scores and indices from the embeddings.\n",
    "    \"\"\"\n",
    "    \n",
    "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "\n",
    "    start_time = timer()\n",
    "    dot_scores = util.dot_score(query_embedding, embeddings)[0]\n",
    "    end_time = timer()\n",
    "    \n",
    "    if print_time:\n",
    "        print_message(\"INFO\", f\"Time taken to get scores on {len(embeddings)} embeddings: {end_time - start_time:.5f} seconds.\")\n",
    "\n",
    "    scores, indices = torch.topk(input=dot_scores,\n",
    "                                k=n_resources_to_return)\n",
    "    return scores, indices\n",
    "\n",
    "def print_top_results_and_scores(query: str,\n",
    "                                 embeddings: torch.tensor,\n",
    "                                 pages_and_chunks: list[dict]=pages_and_chunks,\n",
    "                                 n_resources_to_return: int=5):\n",
    "    \"\"\"\n",
    "    Takes a query, retrieves most relevant resources and prints them out in descending order.\n",
    "\n",
    "    Note: Requires pages_and_chunks to be formatted in a specific way (see above for reference).\n",
    "    \"\"\"\n",
    "    \n",
    "    scores, indices = retrieve_relevant_resources_torch(query=query,\n",
    "                                                  embeddings=embeddings,\n",
    "                                                  n_resources_to_return=n_resources_to_return)\n",
    "    \n",
    "    print(f\"Query: {query}\\n\")\n",
    "    print(\"Results:\")\n",
    "    # Loop through zipped together scores and indicie\n",
    "    for score, index in zip(scores, indices):\n",
    "        print(f\"Score: {score:.4f}\")\n",
    "        # Print relevant sentence chunk (since the scores are in descending order, the most relevant chunk will be first)\n",
    "        print_wrapped(pages_and_chunks[index][\"sentence_chunk\"])\n",
    "        # Print the page number too so we can reference the textbook further and check the results\n",
    "        print(f\"Page number: {pages_and_chunks[index]['page_number']}\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e21488-4699-4a58-940c-7b1f1f98780c",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_relevant_resources(query=\"k-means\", embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5da94b3-2901-48ae-8d22-7b88f9bfde16",
   "metadata": {},
   "source": [
    "## Connecting to an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8ab6579-2be4-494e-b8bd-aa537e5256af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[INFO]\u001b[0m Using attention implementation: sdpa\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa34367046d64a3bab849c6a1921caaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pip install bitsandbytes accelerate\n",
    "from transformers.utils import is_flash_attn_2_available\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from transformers import TextStreamer\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                        bnb_4bit_compute_dtype=torch.float16)\n",
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "## Flash attention gpu\n",
    "\n",
    "if (is_flash_attn_2_available()) and (torch.cuda.get_device_capability(0)[0] >= 8):\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "else:\n",
    "    attn_implementation = \"sdpa\"\n",
    "print_message(\"INFO\", f\"Using attention implementation: {attn_implementation}\")\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=quantization_config,\n",
    "    low_cpu_mem_usage=True,\n",
    "    attn_implementation=attn_implementation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c4ba730-38ad-4ae6-8b58-5025627bb1f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 3072)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ebad11-d8c5-4d28-abfb-9daf811a28e4",
   "metadata": {},
   "source": [
    "#### Getting numbers of parameters of the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a2fcd01-abf6-48f7-b045-2ef0f348fefc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.80346368\n"
     ]
    }
   ],
   "source": [
    "def get_model_num_params(model: torch.nn.Module):\n",
    "    return sum([param.numel() for param in model.parameters()])\n",
    "\n",
    "print(get_model_num_params(model) / 1000000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9dc2459-f185-4e88-a210-d19975c08f29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_mem_bytes': 2197648640, 'model_mem_mb': 2095.84, 'model_mem_gb': 2.05}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_model_mem_size(model: torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Get how much memory a PyTorch model takes up.\n",
    "\n",
    "    See: https://discuss.pytorch.org/t/gpu-memory-that-model-uses/56822\n",
    "    \"\"\"\n",
    "    # Get model parameters and buffer sizes\n",
    "    mem_params = sum([param.nelement() * param.element_size() for param in model.parameters()])\n",
    "    mem_buffers = sum([buf.nelement() * buf.element_size() for buf in model.buffers()])\n",
    "\n",
    "    # Calculate various model sizes\n",
    "    model_mem_bytes = mem_params + mem_buffers # in bytes\n",
    "    model_mem_mb = model_mem_bytes / (1024**2) # in megabytes\n",
    "    model_mem_gb = model_mem_bytes / (1024**3) # in gigabytes\n",
    "\n",
    "    return {\"model_mem_bytes\": model_mem_bytes,\n",
    "            \"model_mem_mb\": round(model_mem_mb, 2),\n",
    "            \"model_mem_gb\": round(model_mem_gb, 2)}\n",
    "\n",
    "get_model_mem_size(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0e7d8d-7e97-4b6f-8812-cbb1ace259ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"What is ridge regression\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848cd3e2-4131-49e6-bcce-7af7ae777df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd62d64-e638-4fa7-9678-0994d9857012",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = [\n",
    "    { \"role\": \"user\", \"content\": input_text },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845255db-4f15-4ad2-b503-c9508f6eb653",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "question = tokenizer(question, return_tensors=\"pt\").to(device)\n",
    "\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca29493d-f553-4da5-b294-34c6ddcce6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "question[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda8c035-6dcf-4079-819d-cecc2eaa3480",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.generate(**question, streamer=streamer,\n",
    "                            pad_token_id=tokenizer.eos_token_id,\n",
    "                            temperature=0.1,\n",
    "                            max_length=2048,\n",
    "                            do_sample=True,\n",
    "                            top_p=0.5,\n",
    "                            repetition_penalty=1.25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70973609-1550-4658-a7dd-3b3ae8f808f2",
   "metadata": {},
   "source": [
    "## Augment the prompt with context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0360d20a-2a07-42cc-aede-03fc1c20cbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPLETE_SYSTEM_PROMPT = \"\"\"You are an advanced AI assistant with access to specific document context. You must strictly adhere to these guidelines:\n",
    "\n",
    "FOUNDATIONAL RULES:\n",
    "1. You can ONLY provide information that is explicitly present in the given context\n",
    "2. You must NEVER use external knowledge, even if you have it\n",
    "3. You must NEVER make assumptions or inferences beyond the context\n",
    "4. Every response must include relevant quotes from the context\n",
    "5. You must indicate confidence level in your responses\n",
    "\n",
    "REQUIRED RESPONSE FORMAT:\n",
    "```\n",
    "Confidence: [High|Medium|Low]\n",
    "Relevant Quotes: [Include word-for-word quotes from the content chunks that support your answer]\n",
    "Answer: [Your response]\n",
    "```\n",
    "\n",
    "RESPONSE GUIDELINES:\n",
    "\n",
    "FOR QUESTIONS WITH AVAILABLE INFORMATION:\n",
    "- Begin with \"Based on the provided context...\"\n",
    "- Include direct quotes using `quotes`\n",
    "- Cite specific sections or page numbers when available\n",
    "- Structure complex answers with clear headings\n",
    "- End with any important caveats or limitations\n",
    "\n",
    "\n",
    "FOR QUESTIONS WITH NO INFORMATION:\n",
    "Response Template:\n",
    "\"I apologize, but I cannot find information about [query] in the provided context. I can only provide information that is explicitly present in these documents.\n",
    "\n",
    "\n",
    "ERROR HANDLING:\n",
    "- If context is corrupted: \"I'm unable to properly process the provided context. Please ensure the document text is correctly formatted.\"\n",
    "- If context is empty: \"No context has been provided. I require specific document context to provide accurate answers.\"\n",
    "\n",
    "PROHIBITED BEHAVIORS:\n",
    "1. Never speculate or extrapolate beyond the context\n",
    "2. Never combine external knowledge with context\n",
    "3. Never make assumptions about missing information\n",
    "4. Never provide personal opinions or interpretations\n",
    "5. Never claim uncertainty about factual content that is clearly present in context\n",
    "\n",
    "EXAMPLE INTERACTIONS:\n",
    "\n",
    "Good Response:\n",
    "User: \"What are the key findings of the study?\"\n",
    "Assistant: \"Confidence: High\n",
    "Relevant Quotes: ['The study found a 23% increase in efficiency', 'Cost reduction of $1.2M annually']\n",
    "Answer: Based on the provided context, the study identified two key findings. First, there was a significant efficiency improvement of 23%. Second, the implementation resulted in annual cost savings of $1.2 million.\n",
    "Source Sections: [Section 4.2, Page 12]\"\n",
    "\n",
    "Partial Information:\n",
    "User: \"What were the project costs and timeline?\"\n",
    "Assistant: \"Confidence: Medium\n",
    "Relevant Quotes: ['Project costs totaled $500,000']\n",
    "Answer: Based on the provided context, I can only speak to the project costs, which were $500,000. The timeline information is not present in the available documents. Would you like to know more about the cost breakdown, which is detailed in the context?\"\n",
    "\n",
    "\n",
    "CONTEXT:    \n",
    "{context}\n",
    "\n",
    "QUERY:\n",
    "{query}\n",
    "\n",
    "IMPLEMENTATION NOTES:\n",
    "1. Always verify context relevance before processing\n",
    "2. Maintain consistent formatting in responses\n",
    "3. Always inlcude the relevant quote section\n",
    "\n",
    "Anwser:\n",
    "\"\"\"\n",
    "\n",
    "# FOR QUESTIONS WITH PARTIAL INFORMATION:\n",
    "# - Clearly state what aspects you can and cannot address\n",
    "# - Explain which parts are missing from the context\n",
    "# - Provide the available partial information with appropriate caveats\n",
    "# - Suggest how the user might refine their question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "4fb53b76-e163-4d79-a0a9-4295940dc97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPLETE_SYSTEM_PROMPT = \"\"\"\n",
    "You are a highly knowledgeable assistant with expertise in extracting and synthesizing information. Use the provided context to answer the question as accurately and comprehensively as possible. Your response should be based strictly on the context given, without introducing external information or assumptions.\n",
    "\n",
    "If the context is insufficient to answer the question, respond clearly with: \"The provided context does not contain enough information to answer this question.\"\n",
    "\n",
    "### Instructions:s\n",
    "1. Base your answer entirely on the provided context.\n",
    "2. Do not include information not explicitly mentioned in the context.\n",
    "3. If the question asks for an explanation, summarize relevant parts of the context in your answer.\n",
    "4. Maintain a professional, concise, and accurate tone.\n",
    "\n",
    "REQUIRED RESPONSE FORMAT:\n",
    "```\n",
    "Confidence: [High|Medium|Low]\n",
    "Relevant Quotes: [Include word-for-word quotes from the retrived document chunks that support your answer]\n",
    "Answer: [Your response]\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "EXAMPLE INTERACTIONS:\n",
    "\n",
    "Good Response:\n",
    "User: \"What are the key findings of the study?\"\n",
    "Assistant: \"Confidence: High\n",
    "Relevant Quotes: ['The study found a 23% increase in efficiency', 'Cost reduction of $1.2M annually']\n",
    "Answer: Based on the provided context, the study identified two key findings. First, there was a significant efficiency improvement of 23%. Second, the implementation resulted in annual cost savings of $1.2 million.\"\n",
    "\n",
    "Partial Information:\n",
    "User: \"What were the project costs and timeline?\"\n",
    "Assistant: \"Confidence: Medium\n",
    "Relevant Quotes: ['Project costs totaled $500,000']\n",
    "Answer: Based on the provided context, I can only speak to the project costs, which were $500,000. The timeline information is not present in the available documents. Would you like to know more about the cost breakdown, which is detailed in the context?\"\n",
    "\n",
    "IMPLEMENTATION NOTES:\n",
    "1. Always verify context relevance before processing\n",
    "2. Always aintain consistent formatting in responses\n",
    "3. Always inlcude the relevant quote section\n",
    "\n",
    "\n",
    "\n",
    "### Context:\n",
    "{context}\n",
    "\n",
    "### Query:\n",
    "{query}\n",
    "\n",
    "\n",
    "\n",
    "### Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "6890a5ef-98e4-45e1-aee3-812c0d83d786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_formatter(query: str,\n",
    "                    context_items: list[dict]) -> str:\n",
    "    context = \"- \" + \"\\n- \".join([item[\"sentence_chunk\"] for item in context_items])\n",
    "\n",
    "    \n",
    "    base_prompt = COMPLETE_SYSTEM_PROMPT.format(context=context, query=query)\n",
    "\n",
    "    #prompt template for instruction tune model\n",
    "\n",
    "    dialogue_template = [\n",
    "        {\"role\": \"user\",\n",
    "        \"content\": base_prompt}\n",
    "    ]\n",
    "    prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n",
    "                                           tokenize=False,\n",
    "                                           add_generation_prompt=True)\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "597361b7-d7d4-403e-8394-ea0f0f535ac9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[INFO]\u001b[0m Time taken to get scores on 41 embeddings: 0.00023 seconds.\n",
      "Confidence: High\n",
      "Relevant Quote: \n",
      "Answer: According to the provided text, the decoder consists of a stack of N = 6 identical layers, just like the encoder. Therefore, it has six layers as well.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "# query = \"What is multi-head attention?\"\n",
    "# query = \"Expain the architecture of a transformer\"\n",
    "# query = \"How can I bake a cake?\"\n",
    "query = \"How many layers does a decoder have?\"\n",
    "scores, indices = retrieve_relevant_resources(query=query,\n",
    "                                              embeddings=embeddings,\n",
    "                                              n_resources_to_return=5\n",
    "                                            )\n",
    "\n",
    "context_items = [pages_and_chunks[i] for i in indices]\n",
    "\n",
    "prompt = prompt_formatter(query,context_items)\n",
    "\n",
    "\n",
    "question = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "_ = model.generate(**question, streamer=streamer,\n",
    "                            pad_token_id=tokenizer.eos_token_id,\n",
    "                            temperature=0.4,\n",
    "                            max_length=4096,\n",
    "                            do_sample=True,\n",
    "                            top_p=1,\n",
    "                            repetition_penalty=1.25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "eb04e6e7-a753-4db8-8561-86a33e1ac8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 04 Dec 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "You are a highly knowledgeable assistant with expertise in extracting and synthesizing information. Use the provided context to answer the question as accurately and comprehensively as possible. Your response should be based strictly on the context given, without introducing external information or assumptions.\n",
      "\n",
      "If the context is insufficient to answer the question, respond clearly with: \"The provided context does not contain enough information to answer this question.\"\n",
      "\n",
      "### Instructions:s\n",
      "1. Base your answer entirely on the provided context.\n",
      "2. Do not include information not explicitly mentioned in the context.\n",
      "3. If the question asks for an explanation, summarize relevant parts of the context in your answer.\n",
      "4. Maintain a professional, concise, and accurate tone.\n",
      "\n",
      "REQUIRED RESPONSE FORMAT:\n",
      "```\n",
      "Confidence: [High|Medium|Low]\n",
      "Relevant Quotes: [Include word-for-word quotes from the retrived document chunks that support your answer]\n",
      "Answer: [Your response]\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "EXAMPLE INTERACTIONS:\n",
      "\n",
      "Good Response:\n",
      "User: \"What are the key findings of the study?\"\n",
      "Assistant: \"Confidence: High\n",
      "Relevant Quotes: ['The study found a 23% increase in efficiency', 'Cost reduction of $1.2M annually']\n",
      "Answer: Based on the provided context, the study identified two key findings. First, there was a significant efficiency improvement of 23%. Second, the implementation resulted in annual cost savings of $1.2 million.\"\n",
      "\n",
      "Partial Information:\n",
      "User: \"What were the project costs and timeline?\"\n",
      "Assistant: \"Confidence: Medium\n",
      "Relevant Quotes: ['Project costs totaled $500,000']\n",
      "Answer: Based on the provided context, I can only speak to the project costs, which were $500,000. The timeline information is not present in the available documents. Would you like to know more about the cost breakdown, which is detailed in the context?\"\n",
      "\n",
      "IMPLEMENTATION NOTES:\n",
      "1. Always verify context relevance before processing\n",
      "2. Always aintain consistent formatting in responses\n",
      "3. Always inlcude the relevant quote section\n",
      "\n",
      "\n",
      "\n",
      "### Context:\n",
      "- Figure 1: The Transformer - model architecture. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.3.1 Encoder and Decoder Stacks Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position- wise fully connected feed-forward network. We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512. Decoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization.\n",
      "- Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i. 3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum 3\n",
      "- output values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2. Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this. MultiHead(Q, K, V ) = Concat(head1, ..., headh)W O where headi = Attention(QW Q i , KW K i , V W V i ) Where the projections are parameter matrices W Q i ∈Rdmodel×dk, W K i ∈Rdmodel×dk, W V i ∈Rdmodel×dv and W O ∈Rhdv×dmodel. In this work we employ h = 8 parallel attention layers, or heads. For each of these we use dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.3.2.3 Applications of Attention in our Model The Transformer uses multi-head attention in three different ways: • In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9]. •\n",
      "- Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence of continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output sequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive [10], consuming the previously generated symbols as additional input when generating the next.2\n",
      "- FFN(x) = max(0, xW1 + b1)W2 + b2 (2) While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality dff = 2048.3.4 Embeddings and Softmax Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor- mation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.5\n",
      "\n",
      "### Query:\n",
      "How many layers does a decoder have?\n",
      "\n",
      "\n",
      "\n",
      "### Answer:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_scratch",
   "language": "python",
   "name": "rag_scratch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
