{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "23ca1766-070c-418e-afdf-4433cc90dbdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[INFO]\u001b[0m This is an info\n",
      "\u001b[31m[ERROR]\u001b[0m This is an error\n"
     ]
    }
   ],
   "source": [
    "from colorama import Fore, Style\n",
    "\n",
    "def print_message(message_type, message):\n",
    "    if message_type == \"INFO\":\n",
    "        print(f\"{Fore.YELLOW}[INFO]{Style.RESET_ALL} {message}\")\n",
    "    elif message_type == \"ERROR\":\n",
    "        print(f\"{Fore.RED}[ERROR]{Style.RESET_ALL} {message}\")\n",
    "    elif message_type == \"SUCCESS\":\n",
    "        print(f\"{Fore.GREEN}[SUCESS]{Style.RESET_ALL} {message}\")\n",
    "    else:\n",
    "        print(f\"{message}\")\n",
    "\n",
    "print_message(\"INFO\", \"This is an info\")\n",
    "print_message(\"ERROR\", \"This is an error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d5341e2-ee6b-4d2a-bcca-53a9b2fed3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae0e13a6-bed5-4292-b542-f4c377373be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file_with_progress(url, output_path):\n",
    "   \n",
    "    response = requests.get(url, stream=True)\n",
    "    if response.status_code == 200:\n",
    "        total_size = int(response.headers.get('content-length', 0))\n",
    "        with open(output_path, 'wb') as file, tqdm(\n",
    "            desc=output_path,\n",
    "            total=total_size,\n",
    "            unit='B',\n",
    "            unit_scale=True,\n",
    "            unit_divisor=1024,\n",
    "        ) as progress_bar:\n",
    "            for chunk in response.iter_content(chunk_size=1024):\n",
    "                if chunk:\n",
    "                    file.write(chunk)\n",
    "                    progress_bar.update(len(chunk))\n",
    "        print_message(\"SUCCESS\", \"File has been successfully downloaded.\")\n",
    "    else:\n",
    "        print_message(\"ERROR\", f\"Something went wrong. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "090a7d4c-e3c2-45d3-93ce-9538d3d85c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_pdf_file(pdf_file_name: str) -> str:\n",
    "    if pdf_file_name[-4:] != \".pdf\":\n",
    "        pdf_file_name += \".pdf\"\n",
    "    if not os.path.exists(pdf_file_name):\n",
    "        print_message(\"INFO\", \"File doesn't exist, Insert Url here\")\n",
    "        url = input(\">\")\n",
    "\n",
    "        download_file_with_progress(url, pdf_file_name)\n",
    "    else:\n",
    "        print_message(\"SUCCESS\", \"The file already exists\")\n",
    "        return pdf_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9dc1ebb-e479-4156-a0c9-61d7d8a42a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[SUCESS]\u001b[0m The file already exists\n"
     ]
    }
   ],
   "source": [
    "pdf_file_name = \"Hands-On Machine Learning With - Aurelien Geron\"\n",
    "pdf_file_name = insert_pdf_file(pdf_file_name=pdf_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f8c63aaa-a416-4d07-a965-78271aa6532e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def text_formatter(text: str) -> str:\n",
    "    cleaned_text = text.replace('\\n', ' ').strip()\n",
    "    return cleaned_text\n",
    "\n",
    "def open_and_read_pdf(pdf_file_name: str) -> list[dict]:\n",
    "    doc = fitz.open(pdf_file_name)\n",
    "    pages_and_texts = []\n",
    "    for page_number, page in tqdm(enumerate(doc)):\n",
    "        text = page.get_text()\n",
    "        text = text_formatter(text=text)\n",
    "        pages_and_texts.append({\n",
    "                \"page_number\": page_number + 1,\n",
    "                \"page_char_count\": len(text),\n",
    "                \"page_word_count\": len(text.split(' ')),\n",
    "                \"page_sentence_count_raw\": len(text.split(\". \")),\n",
    "                \"page_token_count\": len(text) / 4,\n",
    "                \"text\": text\n",
    "        })\n",
    "    return pages_and_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c54cc4d-aef6-46c1-b4b0-6f22d87adff1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "179f0d4a617d4d92914d4bb78fde99aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pages_and_texts = open_and_read_pdf(pdf_file_name=pdf_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69d9be4-7f7b-4323-8f45-3d697c3281f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_and_embed_pdf_file(pdf_file_name: str):\n",
    "    pdf_file_name = insert_pdf_file(pdf_file_name=pdf_file_name)\n",
    "    pages_and_texts = open_and_read_pdf(pdf_file_name_pdf_file_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1b37a8c-e3e8-4659-8258-c86604fa9c4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': 689,\n",
       "  'page_char_count': 1562,\n",
       "  'page_word_count': 166,\n",
       "  'page_sentence_count_raw': 4,\n",
       "  'page_token_count': 390.5,\n",
       "  'text': 'Interleaving Lines from Multiple FilesFirst, suppose you’ve loaded the California housing dataset, shuffled it(unless it was already shuffled), and split it into a training set, a validationset, and a test set. Then you split each set into many CSV files that each looklike this (each row contains eight input features plus the target median housevalue):MedInc,HouseAge,AveRooms,AveBedrms,Popul…,AveOccup,Lat…,Long…,MedianHouseValue3.5214,15.0,3.050,1.107,1447.0,1.606,37.63,-122.43,1.4425.3275,5.0,6.490,0.991,3464.0,3.443,33.69,-117.39,1.6873.1,29.0,7.542,1.592,1328.0,2.251,38.44,-122.98,1.621[...]Let’s also suppose train_filepaths contains the list of training filepaths (andyou also have valid_filepaths and test_filepaths):>>> train_filepaths[\\'datasets/housing/my_train_00.csv\\', \\'datasets/housing/my_train_01.csv\\', ...]Alternatively, you could use file patterns; for example, train_filepaths =\"datasets/housing/my_train_*.csv\". Now let’s create a dataset containing onlythese filepaths:filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed=42)By default, the list_files() function returns a dataset that shuffles the filepaths.In general this is a good thing, but you can set shuffle=False if you do notwant that for some reason.Next, you can call the interleave() method to read from five files at a time andinterleave their lines. You can also skip the first line of each file—which isthe header row—using the skip() method):n_readers = 5dataset = filepath_dataset.interleave(    lambda filepath: tf.data.TextLineDataset(filepath).skip(1),'},\n",
       " {'page_number': 87,\n",
       "  'page_char_count': 1195,\n",
       "  'page_word_count': 197,\n",
       "  'page_sentence_count_raw': 5,\n",
       "  'page_token_count': 298.75,\n",
       "  'text': 'Select a Performance MeasureYour next step is to select a performance measure. A typical performancemeasure for regression problems is the root mean square error (RMSE). Itgives an idea of how much error the system typically makes in its predictions,with a higher weight given to large errors. Equation 2-1 shows themathematical formula to compute the RMSE.Equation 2-1. Root mean square error (RMSE)RMSE ( X , h ) = 1 m ∑ i=1 m h(x (i) )-y (i) 2NOTATIONSThis equation introduces several very common machine learningnotations that I will use throughout this book:m is the number of instances in the dataset you are measuring theRMSE on.For example, if you are evaluating the RMSE on a validationset of 2,000 districts, then m = 2,000.x  is a vector of all the feature values (excluding the label) of the iinstance in the dataset, and y  is its label (the desired output valuefor that instance).For example, if the first district in the dataset is located atlongitude –118.29°, latitude 33.91°, and it has 1,416 inhabitantswith a median income of $38,372, and the median house valueis $156,400 (ignoring other features for now), then:x (1) = - 118.29 33.91 1,416 38,372and:y (1) = 156,400(i)th(i)'},\n",
       " {'page_number': 193,\n",
       "  'page_char_count': 1678,\n",
       "  'page_word_count': 223,\n",
       "  'page_sentence_count_raw': 4,\n",
       "  'page_token_count': 419.5,\n",
       "  'text': '>>> y_scores = sgd_clf.decision_function([some_digit])>>> y_scoresarray([2164.22030239])>>> threshold = 0>>> y_some_digit_pred = (y_scores > threshold)array([ True])The SGDClassifier uses a threshold equal to 0, so the preceding code returnsthe same result as the predict() method (i.e., True). Let’s raise the threshold:>>> threshold = 3000>>> y_some_digit_pred = (y_scores > threshold)>>> y_some_digit_predarray([False])This confirms that raising the threshold decreases recall. The image actuallyrepresents a 5, and the classifier detects it when the threshold is 0, but itmisses it when the threshold is increased to 3,000.How do you decide which threshold to use? First, use the cross_val_predict()function to get the scores of all instances in the training set, but this timespecify that you want to return decision scores instead of predictions:y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3,                             method=\"decision_function\")With these scores, use the precision_recall_curve() function to computeprecision and recall for all possible thresholds (the function adds a lastprecision of 0 and a last recall of 1, corresponding to an infinite threshold):from sklearn.metrics import precision_recall_curveprecisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)Finally, use Matplotlib to plot precision and recall as functions of thethreshold value (Figure 3-5). Let’s show the threshold of 3,000 we selected:plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\", linewidth=2)plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\", linewidth=2)plt.vlines(threshold, 0, 1.0, \"k\", \"dotted\", label=\"threshold\")'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random \n",
    "\n",
    "random.sample(pages_and_texts, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f75b54-24ae-48c7-b962-a3fa4797e3b5",
   "metadata": {},
   "source": [
    "## Get some more info on the data of the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "261e4054-f538-46e4-85b4-717c8f643e1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count_raw</th>\n",
       "      <th>page_token_count</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>37.50</td>\n",
       "      <td>Hands-On Machine Learning withScikit-Learn, Ke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>869</td>\n",
       "      <td>93</td>\n",
       "      <td>4</td>\n",
       "      <td>217.25</td>\n",
       "      <td>Hands-On Machine Learning with Scikit-Learn, K...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>27</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>6.75</td>\n",
       "      <td>October 2022: Third Edition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1150</td>\n",
       "      <td>157</td>\n",
       "      <td>5</td>\n",
       "      <td>287.50</td>\n",
       "      <td>Revision History for the Third Edition2022-10-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   page_number  page_char_count  page_word_count  page_sentence_count_raw  \\\n",
       "0            1                0                1                        1   \n",
       "1            2              150               14                        1   \n",
       "2            3              869               93                        4   \n",
       "3            4               27                4                        1   \n",
       "4            5             1150              157                        5   \n",
       "\n",
       "   page_token_count                                               text  \n",
       "0              0.00                                                     \n",
       "1             37.50  Hands-On Machine Learning withScikit-Learn, Ke...  \n",
       "2            217.25  Hands-On Machine Learning with Scikit-Learn, K...  \n",
       "3              6.75                        October 2022: Third Edition  \n",
       "4            287.50  Revision History for the Third Edition2022-10-...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(pages_and_texts)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e7e4911-ad7f-4f99-9ec0-4e780e73ae21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count_raw</th>\n",
       "      <th>page_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1351.00</td>\n",
       "      <td>1351.00</td>\n",
       "      <td>1351.00</td>\n",
       "      <td>1351.00</td>\n",
       "      <td>1351.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>676.00</td>\n",
       "      <td>1284.83</td>\n",
       "      <td>192.12</td>\n",
       "      <td>6.09</td>\n",
       "      <td>321.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>390.14</td>\n",
       "      <td>584.26</td>\n",
       "      <td>96.68</td>\n",
       "      <td>3.94</td>\n",
       "      <td>146.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>338.50</td>\n",
       "      <td>909.50</td>\n",
       "      <td>115.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>227.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>676.00</td>\n",
       "      <td>1311.00</td>\n",
       "      <td>195.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>327.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1013.50</td>\n",
       "      <td>1749.50</td>\n",
       "      <td>267.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>437.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1351.00</td>\n",
       "      <td>3066.00</td>\n",
       "      <td>506.00</td>\n",
       "      <td>33.00</td>\n",
       "      <td>766.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  page_char_count  page_word_count  page_sentence_count_raw  \\\n",
       "count      1351.00          1351.00          1351.00                  1351.00   \n",
       "mean        676.00          1284.83           192.12                     6.09   \n",
       "std         390.14           584.26            96.68                     3.94   \n",
       "min           1.00             0.00             1.00                     1.00   \n",
       "25%         338.50           909.50           115.00                     3.00   \n",
       "50%         676.00          1311.00           195.00                     6.00   \n",
       "75%        1013.50          1749.50           267.00                     8.00   \n",
       "max        1351.00          3066.00           506.00                    33.00   \n",
       "\n",
       "       page_token_count  \n",
       "count           1351.00  \n",
       "mean             321.21  \n",
       "std              146.06  \n",
       "min                0.00  \n",
       "25%              227.38  \n",
       "50%              327.75  \n",
       "75%              437.38  \n",
       "max              766.50  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2f0843-ed98-4741-9b6b-ead4496a64d4",
   "metadata": {},
   "source": [
    "## Splitting pages into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb064551-ca85-4ee9-ac47-243386fd49c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[This is a sentence., This is another sentence., I like this.]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "doc = nlp(\"This is a sentence. This is another sentence. I like this.\")\n",
    "assert len(list(doc.sents)) == 3\n",
    "\n",
    "list(doc.sents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5e26815d-81ca-4bb0-82f9-0f70bf9b6ee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa7cabacf28b447094d860a32452642a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1351 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for item in tqdm(pages_and_texts):\n",
    "    item[\"sentences\"] = list(nlp(item['text']).sents)\n",
    "\n",
    "    item['sentences'] = [str(sentence) for sentence in item[\"sentences\"]]\n",
    "    item[\"page_sentence_count_spacy\"] = len(item[\"sentences\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a16afd77-3914-4458-839b-f990b6240541",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': 910,\n",
       "  'page_char_count': 2051,\n",
       "  'page_word_count': 313,\n",
       "  'page_sentence_count_raw': 11,\n",
       "  'page_token_count': 512.75,\n",
       "  'text': 'MaskingMaking the model ignore padding tokens is trivial using Keras: simply addmask_zero=True when creating the Embedding layer. This means thatpadding tokens (whose ID is 0) will be ignored by all downstream layers.That’s all! If you retrain the previous model for a few epochs, you will findthat the validation accuracy quickly reaches over 80%.The way this works is that the Embedding layer creates a mask tensor equalto tf.math.not_equal(inputs, 0): it is a Boolean tensor with the same shape asthe inputs, and it is equal to False anywhere the token IDs are 0, or Trueotherwise. This mask tensor is then automatically propagated by the model tothe next layer. If that layer’s call() method has a mask argument, then itautomatically receives the mask. This allows the layer to ignore theappropriate time steps. Each layer may handle the mask differently, but ingeneral they simply ignore masked time steps (i.e., time steps for which themask is False). For example, when a recurrent layer encounters a maskedtime step, it simply copies the output from the previous time step.Next, if the layer’s supports_masking attribute is True, then the mask isautomatically propagated to the next layer. It keeps propagating this way foras long as the layers have supports_masking=True. As an example, arecurrent layer’s supports_mask\\u2060 ing attribute is True whenreturn_sequences=True, but it’s False when return_sequen\\u2060 ces=False sincethere’s no need for a mask anymore in this case. So if you have a model withseveral recurrent layers with return_sequences=True, followed by a recurrentlayer with return_sequences=False, then the mask will automaticallypropagate up to the last recurrent layer: that layer will use the mask to ignoremasked steps, but it will not propagate the mask any further. Similarly, if youset mask_zero=True when creating the Embedding layer in the sentimentanalysis model we just built, then the GRU layer will receive and use themask automatically, but it will not propagate it any further, sincereturn_sequences is not set to True.',\n",
       "  'sentences': ['MaskingMaking the model ignore padding tokens is trivial using Keras: simply addmask_zero=True when creating the Embedding layer.',\n",
       "   'This means thatpadding tokens (whose ID is 0) will be ignored by all downstream layers.',\n",
       "   'That’s all!',\n",
       "   'If you retrain the previous model for a few epochs, you will findthat the validation accuracy quickly reaches over 80%.The way this works is that the Embedding layer creates a mask tensor equalto tf.math.not_equal(inputs, 0): it is a Boolean tensor with the same shape asthe inputs, and it is equal to False anywhere the token IDs are 0, or Trueotherwise.',\n",
       "   'This mask tensor is then automatically propagated by the model tothe next layer.',\n",
       "   'If that layer’s call() method has a mask argument, then itautomatically receives the mask.',\n",
       "   'This allows the layer to ignore theappropriate time steps.',\n",
       "   'Each layer may handle the mask differently, but ingeneral they simply ignore masked time steps (i.e., time steps for which themask is False).',\n",
       "   'For example, when a recurrent layer encounters a maskedtime step, it simply copies the output from the previous time step.',\n",
       "   'Next, if the layer’s supports_masking attribute is True, then the mask isautomatically propagated to the next layer.',\n",
       "   'It keeps propagating this way foras long as the layers have supports_masking=True.',\n",
       "   'As an example, arecurrent layer’s supports_mask\\u2060 ing attribute is True whenreturn_sequences=True, but it’s False when return_sequen\\u2060 ces=False sincethere’s no need for a mask anymore in this case.',\n",
       "   'So if you have a model withseveral recurrent layers with return_sequences=True, followed by a recurrentlayer with return_sequences=False, then the mask will automaticallypropagate up to the last recurrent layer: that layer will use the mask to ignoremasked steps, but it will not propagate the mask any further.',\n",
       "   'Similarly, if youset mask_zero=True when creating the Embedding layer in the sentimentanalysis model we just built, then the GRU layer will receive and use themask automatically, but it will not propagate it any further, sincereturn_sequences is not set to True.'],\n",
       "  'page_sentence_count_spacy': 14}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(pages_and_texts, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2dd2c36d-ad80-4440-8af6-903ca30b5bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count_raw</th>\n",
       "      <th>page_token_count</th>\n",
       "      <th>page_sentence_count_spacy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1351.0</td>\n",
       "      <td>1351.0</td>\n",
       "      <td>1351.0</td>\n",
       "      <td>1351.0</td>\n",
       "      <td>1351.0</td>\n",
       "      <td>1351.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>676.0</td>\n",
       "      <td>1285.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>321.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>390.0</td>\n",
       "      <td>584.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>338.0</td>\n",
       "      <td>910.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>227.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>676.0</td>\n",
       "      <td>1311.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>328.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1014.0</td>\n",
       "      <td>1750.0</td>\n",
       "      <td>267.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>437.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1351.0</td>\n",
       "      <td>3066.0</td>\n",
       "      <td>506.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>766.0</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  page_char_count  page_word_count  page_sentence_count_raw  \\\n",
       "count       1351.0           1351.0           1351.0                   1351.0   \n",
       "mean         676.0           1285.0            192.0                      6.0   \n",
       "std          390.0            584.0             97.0                      4.0   \n",
       "min            1.0              0.0              1.0                      1.0   \n",
       "25%          338.0            910.0            115.0                      3.0   \n",
       "50%          676.0           1311.0            195.0                      6.0   \n",
       "75%         1014.0           1750.0            267.0                      8.0   \n",
       "max         1351.0           3066.0            506.0                     33.0   \n",
       "\n",
       "       page_token_count  page_sentence_count_spacy  \n",
       "count            1351.0                     1351.0  \n",
       "mean              321.0                        9.0  \n",
       "std               146.0                        6.0  \n",
       "min                 0.0                        0.0  \n",
       "25%               227.0                        4.0  \n",
       "50%               328.0                        8.0  \n",
       "75%               437.0                       12.0  \n",
       "max               766.0                       52.0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(pages_and_texts)\n",
    "df.describe().round()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feec6e04-402c-4a0e-ab8c-b2110ff6ef9a",
   "metadata": {},
   "source": [
    "### Making chunks from the text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88c55355-dbdd-4d20-8411-9ed50ac48812",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sentence_chunk_size = 10 \n",
    "\n",
    "def split_list(input_list: list, slice_size: int =num_sentence_chunk_size) -> list[str]:\n",
    "    return [input_list[i:i+slice_size + 1] for i in range(0, len(input_list), slice_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "969e20aa-3916-426e-ba25-576bcb90b9ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
       " [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20],\n",
       " [20, 21, 22, 23, 24]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl = list(range(25))\n",
    "split_list(tl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ad95769-5bef-4ee4-a7a5-6bd3db3b00eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c4d979cf7534315a9bfc7851b837511",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1351 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for item in tqdm(pages_and_texts):\n",
    "    item[\"sentence_chunks\"] = split_list(input_list=item[\"sentences\"],\n",
    "                                        slice_size=num_sentence_chunk_size)\n",
    "    item[\"num_chunks\"] = len(item[\"sentence_chunks\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c80c586-d3a6-4b67-833a-52fba584fc9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': 1007,\n",
       "  'page_char_count': 1828,\n",
       "  'page_word_count': 250,\n",
       "  'page_sentence_count_raw': 7,\n",
       "  'page_token_count': 457.0,\n",
       "  'text': 'Let’s start building a variational autoencoder for Fashion MNIST (as shownin Figure 17-11, but using the γ tweak). First, we will need a custom layer tosample the codings, given μ and γ:class Sampling(tf.keras.layers.Layer):    def call(self, inputs):        mean, log_var = inputs        return tf.random.normal(tf.shape(log_var)) * tf.exp(log_var / 2) + meanThis Sampling layer takes two inputs: mean (μ) and log_var (γ). It uses thefunction tf.random.normal() to sample a random vector (of the same shape asγ) from the Gaussian distribution, with mean 0 and standard deviation 1.Then it multiplies it by exp(γ / 2) (which is equal to σ, as you can verifymathematically), and finally it adds μ and returns the result. This samples acodings vector from the Gaussian distribution with mean μ and standarddeviation σ.Next, we can create the encoder, using the functional API because the modelis not entirely sequential:codings_size = 10inputs = tf.keras.layers.Input(shape=[28, 28])Z = tf.keras.layers.Flatten()(inputs)Z = tf.keras.layers.Dense(150, activation=\"relu\")(Z)Z = tf.keras.layers.Dense(100, activation=\"relu\")(Z)codings_mean = tf.keras.layers.Dense(codings_size)(Z)  # μcodings_log_var = tf.keras.layers.Dense(codings_size)(Z)  # γcodings = Sampling()([codings_mean, codings_log_var])variational_encoder = tf.keras.Model(    inputs=[inputs], outputs=[codings_mean, codings_log_var, codings])Note that the Dense layers that output codings_mean (μ) andcodings_log_var (γ) have the same inputs (i.e., the outputs of the secondDense layer). We then pass both codings_mean and codings_log_var to theSampling layer. Finally, the variational_encoder model has three outputs.Only the codings are required, but we add codings_mean andcodings_log_var as well, in case we want to inspect their values. Now let’sbuild the decoder:',\n",
       "  'sentences': ['Let’s start building a variational autoencoder for Fashion MNIST (as shownin Figure 17-11, but using the γ tweak).',\n",
       "   'First, we will need a custom layer tosample the codings, given μ and γ:class Sampling(tf.keras.layers.',\n",
       "   'Layer):    def call(self, inputs):        mean, log_var = inputs        return tf.random.normal(tf.shape(log_var)) * tf.exp(log_var / 2) + meanThis Sampling layer takes two inputs: mean (μ) and log_var (γ).',\n",
       "   'It uses thefunction tf.random.normal() to sample a random vector (of the same shape asγ) from the Gaussian distribution, with mean 0 and standard deviation 1.Then it multiplies it by exp(γ / 2) (which is equal to σ, as you can verifymathematically), and finally it adds μ and returns the result.',\n",
       "   'This samples acodings vector from the Gaussian distribution with mean μ and standarddeviation σ.',\n",
       "   'Next, we can create the encoder, using the functional API because the modelis not entirely sequential:codings_size = 10inputs = tf.keras.layers.',\n",
       "   'Input(shape=[28, 28])Z = tf.keras.layers.',\n",
       "   'Flatten()(inputs)Z = tf.keras.layers.',\n",
       "   'Dense(150, activation=\"relu\")(Z)Z = tf.keras.layers.',\n",
       "   'Dense(100, activation=\"relu\")(Z)codings_mean = tf.keras.layers.',\n",
       "   'Dense(codings_size)(Z)  # μcodings_log_var = tf.keras.layers.',\n",
       "   'Dense(codings_size)(Z)  # γcodings = Sampling()([codings_mean, codings_log_var])variational_encoder = tf.keras.',\n",
       "   'Model(    inputs=[inputs], outputs=[codings_mean, codings_log_var, codings])Note that the Dense layers that output codings_mean (μ) andcodings_log_var (γ) have the same inputs (i.e., the outputs of the secondDense layer).',\n",
       "   'We then pass both codings_mean and codings_log_var to theSampling layer.',\n",
       "   'Finally, the variational_encoder model has three outputs.',\n",
       "   'Only the codings are required, but we add codings_mean andcodings_log_var as well, in case we want to inspect their values.',\n",
       "   'Now let’sbuild the decoder:'],\n",
       "  'page_sentence_count_spacy': 17,\n",
       "  'sentence_chunks': [['Let’s start building a variational autoencoder for Fashion MNIST (as shownin Figure 17-11, but using the γ tweak).',\n",
       "    'First, we will need a custom layer tosample the codings, given μ and γ:class Sampling(tf.keras.layers.',\n",
       "    'Layer):    def call(self, inputs):        mean, log_var = inputs        return tf.random.normal(tf.shape(log_var)) * tf.exp(log_var / 2) + meanThis Sampling layer takes two inputs: mean (μ) and log_var (γ).',\n",
       "    'It uses thefunction tf.random.normal() to sample a random vector (of the same shape asγ) from the Gaussian distribution, with mean 0 and standard deviation 1.Then it multiplies it by exp(γ / 2) (which is equal to σ, as you can verifymathematically), and finally it adds μ and returns the result.',\n",
       "    'This samples acodings vector from the Gaussian distribution with mean μ and standarddeviation σ.',\n",
       "    'Next, we can create the encoder, using the functional API because the modelis not entirely sequential:codings_size = 10inputs = tf.keras.layers.',\n",
       "    'Input(shape=[28, 28])Z = tf.keras.layers.',\n",
       "    'Flatten()(inputs)Z = tf.keras.layers.',\n",
       "    'Dense(150, activation=\"relu\")(Z)Z = tf.keras.layers.',\n",
       "    'Dense(100, activation=\"relu\")(Z)codings_mean = tf.keras.layers.',\n",
       "    'Dense(codings_size)(Z)  # μcodings_log_var = tf.keras.layers.'],\n",
       "   ['Dense(codings_size)(Z)  # μcodings_log_var = tf.keras.layers.',\n",
       "    'Dense(codings_size)(Z)  # γcodings = Sampling()([codings_mean, codings_log_var])variational_encoder = tf.keras.',\n",
       "    'Model(    inputs=[inputs], outputs=[codings_mean, codings_log_var, codings])Note that the Dense layers that output codings_mean (μ) andcodings_log_var (γ) have the same inputs (i.e., the outputs of the secondDense layer).',\n",
       "    'We then pass both codings_mean and codings_log_var to theSampling layer.',\n",
       "    'Finally, the variational_encoder model has three outputs.',\n",
       "    'Only the codings are required, but we add codings_mean andcodings_log_var as well, in case we want to inspect their values.',\n",
       "    'Now let’sbuild the decoder:']],\n",
       "  'num_chunks': 2},\n",
       " {'page_number': 1062,\n",
       "  'page_char_count': 1600,\n",
       "  'page_word_count': 239,\n",
       "  'page_sentence_count_raw': 7,\n",
       "  'page_token_count': 400.0,\n",
       "  'text': 'reward_mean = flat_rewards.mean()    reward_std = flat_rewards.std()    return [(discounted_rewards - reward_mean) / reward_std            for discounted_rewards in all_discounted_rewards]Let’s check that this works:>>> discount_rewards([10, 0, -50], discount_factor=0.8)array([-22, -40, -50])>>> discount_and_normalize_rewards([[10, 0, -50], [10, 20]],...                                discount_factor=0.8)...[array([-0.28435071, -0.86597718, -1.18910299]), array([1.26665318, 1.0727777 ])]The call to discount_rewards() returns exactly what we expect (see Figure 18-6). You can verify that the function discount_and_normalize_rewards() doesindeed return the normalized action advantages for each action in bothepisodes. Notice that the first episode was much worse than the second, so itsnormalized advantages are all negative; all actions from the first episodewould be considered bad, and conversely all actions from the second episodewould be considered good.We are almost ready to run the algorithm! Now let’s define thehyperparameters. We will run 150 training iterations, playing 10 episodes periteration, and each episode will last at most 200 steps. We will use a discountfactor of 0.95:n_iterations = 150n_episodes_per_update = 10n_max_steps = 200discount_factor = 0.95We also need an optimizer and the loss function. A regular Nadam optimizerwith learning rate 0.01 will do just fine, and we will use the binary cross-entropy loss function because we are training a binary classifier (there aretwo possible actions—left or right):optimizer = tf.keras.optimizers.Nadam(learning_rate=0.01)',\n",
       "  'sentences': ['reward_mean = flat_rewards.mean()    reward_std = flat_rewards.std()    return [(discounted_rewards - reward_mean) / reward_std            for discounted_rewards in all_discounted_rewards]Let’s check that this works:>>> discount_rewards([10, 0, -50], discount_factor=0.8)array([-22, -40, -50])>>> discount_and_normalize_rewards([[10, 0, -50], [10, 20]],...                                discount_factor=0.8)...[array([-0.28435071, -0.86597718, -1.18910299]), array([1.26665318, 1.0727777 ])]The call to discount_rewards() returns exactly what we expect (see Figure 18-6).',\n",
       "   'You can verify that the function discount_and_normalize_rewards() doesindeed return the normalized action advantages for each action in bothepisodes.',\n",
       "   'Notice that the first episode was much worse than the second, so itsnormalized advantages are all negative; all actions from the first episodewould be considered bad, and conversely all actions from the second episodewould be considered good.',\n",
       "   'We are almost ready to run the algorithm!',\n",
       "   'Now let’s define thehyperparameters.',\n",
       "   'We will run 150 training iterations, playing 10 episodes periteration, and each episode will last at most 200 steps.',\n",
       "   'We will use a discountfactor of 0.95:n_iterations = 150n_episodes_per_update = 10n_max_steps = 200discount_factor = 0.95We also need an optimizer and the loss function.',\n",
       "   'A regular Nadam optimizerwith learning rate 0.01 will do just fine, and we will use the binary cross-entropy loss function because we are training a binary classifier (there aretwo possible actions—left or right):optimizer = tf.keras.optimizers.',\n",
       "   'Nadam(learning_rate=0.01)'],\n",
       "  'page_sentence_count_spacy': 9,\n",
       "  'sentence_chunks': [['reward_mean = flat_rewards.mean()    reward_std = flat_rewards.std()    return [(discounted_rewards - reward_mean) / reward_std            for discounted_rewards in all_discounted_rewards]Let’s check that this works:>>> discount_rewards([10, 0, -50], discount_factor=0.8)array([-22, -40, -50])>>> discount_and_normalize_rewards([[10, 0, -50], [10, 20]],...                                discount_factor=0.8)...[array([-0.28435071, -0.86597718, -1.18910299]), array([1.26665318, 1.0727777 ])]The call to discount_rewards() returns exactly what we expect (see Figure 18-6).',\n",
       "    'You can verify that the function discount_and_normalize_rewards() doesindeed return the normalized action advantages for each action in bothepisodes.',\n",
       "    'Notice that the first episode was much worse than the second, so itsnormalized advantages are all negative; all actions from the first episodewould be considered bad, and conversely all actions from the second episodewould be considered good.',\n",
       "    'We are almost ready to run the algorithm!',\n",
       "    'Now let’s define thehyperparameters.',\n",
       "    'We will run 150 training iterations, playing 10 episodes periteration, and each episode will last at most 200 steps.',\n",
       "    'We will use a discountfactor of 0.95:n_iterations = 150n_episodes_per_update = 10n_max_steps = 200discount_factor = 0.95We also need an optimizer and the loss function.',\n",
       "    'A regular Nadam optimizerwith learning rate 0.01 will do just fine, and we will use the binary cross-entropy loss function because we are training a binary classifier (there aretwo possible actions—left or right):optimizer = tf.keras.optimizers.',\n",
       "    'Nadam(learning_rate=0.01)']],\n",
       "  'num_chunks': 1}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(pages_and_texts, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2b83eea-fdd9-43be-8097-37e8a6c6ef5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count_raw</th>\n",
       "      <th>page_token_count</th>\n",
       "      <th>page_sentence_count_spacy</th>\n",
       "      <th>num_chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1351.00</td>\n",
       "      <td>1351.00</td>\n",
       "      <td>1351.00</td>\n",
       "      <td>1351.00</td>\n",
       "      <td>1351.00</td>\n",
       "      <td>1351.00</td>\n",
       "      <td>1351.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>676.00</td>\n",
       "      <td>1284.83</td>\n",
       "      <td>192.12</td>\n",
       "      <td>6.09</td>\n",
       "      <td>321.21</td>\n",
       "      <td>8.68</td>\n",
       "      <td>1.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>390.14</td>\n",
       "      <td>584.26</td>\n",
       "      <td>96.68</td>\n",
       "      <td>3.94</td>\n",
       "      <td>146.06</td>\n",
       "      <td>5.71</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>338.50</td>\n",
       "      <td>909.50</td>\n",
       "      <td>115.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>227.38</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>676.00</td>\n",
       "      <td>1311.00</td>\n",
       "      <td>195.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>327.75</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1013.50</td>\n",
       "      <td>1749.50</td>\n",
       "      <td>267.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>437.38</td>\n",
       "      <td>12.00</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1351.00</td>\n",
       "      <td>3066.00</td>\n",
       "      <td>506.00</td>\n",
       "      <td>33.00</td>\n",
       "      <td>766.50</td>\n",
       "      <td>52.00</td>\n",
       "      <td>6.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  page_char_count  page_word_count  page_sentence_count_raw  \\\n",
       "count      1351.00          1351.00          1351.00                  1351.00   \n",
       "mean        676.00          1284.83           192.12                     6.09   \n",
       "std         390.14           584.26            96.68                     3.94   \n",
       "min           1.00             0.00             1.00                     1.00   \n",
       "25%         338.50           909.50           115.00                     3.00   \n",
       "50%         676.00          1311.00           195.00                     6.00   \n",
       "75%        1013.50          1749.50           267.00                     8.00   \n",
       "max        1351.00          3066.00           506.00                    33.00   \n",
       "\n",
       "       page_token_count  page_sentence_count_spacy  num_chunks  \n",
       "count           1351.00                    1351.00     1351.00  \n",
       "mean             321.21                       8.68        1.38  \n",
       "std              146.06                       5.71        0.58  \n",
       "min                0.00                       0.00        0.00  \n",
       "25%              227.38                       4.00        1.00  \n",
       "50%              327.75                       8.00        1.00  \n",
       "75%              437.38                      12.00        2.00  \n",
       "max              766.50                      52.00        6.00  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(pages_and_texts)\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea8023c3-4672-4b38-914b-c8ca2d061d88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e18211018aa40a6b1d35e8249663066",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1351 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1860"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "pages_and_chunks = []\n",
    "for i in tqdm(pages_and_texts):\n",
    "    for sentence_chunk in i['sentence_chunks']:\n",
    "        chunk_dict = {}\n",
    "        chunk_dict[\"page_number\"] = i[\"page_number\"]\n",
    "        joined_sentence_chunk = \"\".join(sentence_chunk).replace(\"  \", \" \").strip()\n",
    "        joined_sentence_chunk = re.sub(r'\\.([A-Z])', r'. \\1', joined_sentence_chunk)\n",
    "        chunk_dict[\"sentence_chunk\"] = joined_sentence_chunk\n",
    "        chunk_dict[\"chunk_char_count\"] = len(joined_sentence_chunk)\n",
    "        chunk_dict[\"chunk_word_count\"] = len([word for word in joined_sentence_chunk.split(\" \")])\n",
    "        chunk_dict[\"chunk_token_count\"] = len(joined_sentence_chunk) / 4\n",
    "        pages_and_chunks.append(chunk_dict)\n",
    "    \n",
    "len(pages_and_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "641a8851-07aa-46f8-897e-2d263229f77c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': 267,\n",
       "  'sentence_chunk': \"With stochastic and mini-batch gradient descent, the curves are not so smooth, and it maybe hard to know whether you have reached the minimum or not. One solution is to stoponly after the validation error has been above the minimum for some time (when you areconfident that the model will not do any better), then roll back the model parameters to thepoint where the validation error was at a minimum. Here is a basic implementation of early stopping:from copy import deepcopyfrom sklearn.metrics import mean_squared_errorfrom sklearn.preprocessing import StandardScalerX_train, y_train, X_valid, y_valid = [...] # split the quadratic datasetpreprocessing = make_pipeline(PolynomialFeatures(degree=90, include_bias=False),               StandardScaler())X_train_prep = preprocessing.fit_transform(X_train)X_valid_prep = preprocessing.transform(X_valid)sgd_reg = SGDRegressor(penalty=None, eta0=0.002, random_state=42)n_epochs = 500best_valid_rmse = float('inf')for epoch in range(n_epochs):  sgd_reg.partial_fit(X_train_prep, y_train)  y_valid_predict = sgd_reg.predict(X_valid_prep)  val_error = mean_squared_error(y_valid, y_valid_predict, squared=False)  if val_error < best_valid_rmse:    best_valid_rmse = val_error    best_model = deepcopy(sgd_reg)This code first adds the polynomial features and scales all the input features,both for the training set and for the validation set (the code assumes that youhave split the original training set into a smaller training set and a validationset). Then it creates an SGDRegressor model with no regularization and asmall learning rate. In the training loop, it calls partial_fit() instead of fit(), toperform incremental learning. At each epoch, it measures the RMSE on thevalidation set. If it is lower than the lowest RMSE seen so far, it saves a copyof the model in the best_model variable. This implementation does notactually stop training, but it lets you revert to the best model after training. Note that the model is copied using copy.deepcopy(), because it copies both\",\n",
       "  'chunk_char_count': 2028,\n",
       "  'chunk_word_count': 285,\n",
       "  'chunk_token_count': 507.0},\n",
       " {'page_number': 1120,\n",
       "  'sentence_chunk': 'For this we will use the google-cloud-storage library, which is preinstalled in Colab. We first create a Clientobject, which will serve as the interface with GCS, then we use it to createthe bucket:from google.cloud import storage',\n",
       "  'chunk_char_count': 230,\n",
       "  'chunk_word_count': 36,\n",
       "  'chunk_token_count': 57.5}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(pages_and_chunks, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ce5516c-c898-42ad-916f-efd7cef1fd98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>chunk_char_count</th>\n",
       "      <th>chunk_word_count</th>\n",
       "      <th>chunk_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1860.00</td>\n",
       "      <td>1860.00</td>\n",
       "      <td>1860.00</td>\n",
       "      <td>1860.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>676.23</td>\n",
       "      <td>962.78</td>\n",
       "      <td>141.11</td>\n",
       "      <td>240.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>376.24</td>\n",
       "      <td>562.09</td>\n",
       "      <td>85.76</td>\n",
       "      <td>140.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>360.75</td>\n",
       "      <td>492.75</td>\n",
       "      <td>73.00</td>\n",
       "      <td>123.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>680.00</td>\n",
       "      <td>995.50</td>\n",
       "      <td>131.00</td>\n",
       "      <td>248.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>996.00</td>\n",
       "      <td>1411.00</td>\n",
       "      <td>210.00</td>\n",
       "      <td>352.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1351.00</td>\n",
       "      <td>3052.00</td>\n",
       "      <td>492.00</td>\n",
       "      <td>763.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  chunk_char_count  chunk_word_count  chunk_token_count\n",
       "count      1860.00           1860.00           1860.00            1860.00\n",
       "mean        676.23            962.78            141.11             240.70\n",
       "std         376.24            562.09             85.76             140.52\n",
       "min           2.00              3.00              1.00               0.75\n",
       "25%         360.75            492.75             73.00             123.19\n",
       "50%         680.00            995.50            131.00             248.88\n",
       "75%         996.00           1411.00            210.00             352.75\n",
       "max        1351.00           3052.00            492.00             763.00"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(pages_and_chunks)\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e002c64-19e5-467d-ada7-48623eae3a7b",
   "metadata": {},
   "source": [
    "## Filter out chunks with less than 20 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c435930a-fd6e-4ec9-82d4-f1e2d87b25da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk token count: 19.25 | Text: Figure 19-6. Executing a TensorFlow graph across multiple devices in parallel\n",
      "Chunk token count: 11.0 | Text: the data. For now, let’s go with this model.\n",
      "Chunk token count: 6.5 | Text: How muchdoes PCA help now?\n",
      "Chunk token count: 19.25 | Text: Now let’s look at the second common building block of CNNs: the poolinglayer.\n",
      "Chunk token count: 14.75 | Text: Tensor: shape=(2, 3), dtype=int64, numpy= array([[0, 1, 2],\n"
     ]
    }
   ],
   "source": [
    "min_token_lenght = 20\n",
    "for row in df[df[\"chunk_token_count\"] <= min_token_lenght].sample(5).iterrows():\n",
    "    print(f\"Chunk token count: {row[1]['chunk_token_count']} | Text: {row[1]['sentence_chunk']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d930fabe-7c87-446f-9f2e-a286bfedf846",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_and_chunks_over_min_token_len = df[df[\"chunk_token_count\"] > min_token_lenght].to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7e6deb7e-bfd3-4bdb-bf60-c23b1cb7e06f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': 937,\n",
       "  'sentence_chunk': 'Figure 16-8. The original 2017 transformer architecture\\u2060If you use the transformer for NMT, then during training you must feed the22',\n",
       "  'chunk_char_count': 132,\n",
       "  'chunk_word_count': 20,\n",
       "  'chunk_token_count': 33.0}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(pages_and_chunks_over_min_token_len, k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5134c9c8-dc3f-442d-9209-d2b7229887f7",
   "metadata": {},
   "source": [
    "## Embedding chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8dedbf60-4e9b-4296-a6d4-58c8b9a2b512",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = [\"Hello This is a test for the embedding model\",\n",
    "                 \"this is a second sentence for the model\",\n",
    "                 \"The sky is blue\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0e9359d1-4266-4602-93bc-53b576e72ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\",\n",
    "                                     device=\"cuda\")\n",
    "\n",
    "\n",
    "embeddings = embedding_model.encode(test_sentences,\n",
    "                                    batch_size=32,\n",
    "                                    convert_to_tensor=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b4bbdd38-56cd-4327-b98f-ea3f96b6ef0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_and_embeddings = dict(zip(test_sentences, embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c6eb9ddc-efee-48f1-b0d6-bc1e46aae203",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Hello This is a test for the embedding model': tensor([-5.1741e-04, -8.9576e-02, -4.3483e-02,  6.3491e-02, -1.5166e-02,\n",
       "          3.6753e-02,  5.0416e-02,  2.7074e-02,  4.1897e-02,  1.8115e-03,\n",
       "          6.2513e-02, -9.2316e-03,  2.6844e-03,  5.2567e-02,  3.8520e-02,\n",
       "         -4.6451e-02,  5.5088e-02,  2.7253e-02, -9.0316e-03, -1.0031e-02,\n",
       "         -1.9533e-02, -5.2470e-02,  4.5585e-03,  1.2949e-02, -1.9578e-02,\n",
       "          2.3228e-02, -2.3299e-02,  5.9363e-03, -7.0727e-03, -5.1463e-02,\n",
       "         -1.7793e-02,  2.3849e-02,  5.1635e-02, -3.2830e-02,  1.4976e-06,\n",
       "         -5.8171e-02, -1.4653e-02,  6.8788e-03, -8.3841e-03,  1.3590e-02,\n",
       "          1.0240e-02,  6.2663e-02, -7.4270e-03,  2.7005e-02, -2.4735e-02,\n",
       "         -1.4349e-02, -1.6881e-02,  4.8620e-02, -1.3179e-02,  5.7703e-02,\n",
       "          6.5349e-03,  2.2798e-03, -2.4195e-02, -3.1098e-02,  5.1947e-02,\n",
       "          2.4326e-02,  3.8770e-02,  3.3131e-02, -6.9657e-03,  3.5400e-02,\n",
       "         -3.7303e-02,  1.6646e-02,  2.4014e-02,  3.6676e-02,  4.7199e-02,\n",
       "          2.9369e-02,  8.6796e-02, -2.9985e-02,  4.0537e-02,  5.5708e-03,\n",
       "          6.5119e-02,  1.4509e-02,  2.8657e-02,  8.9481e-02,  2.2189e-02,\n",
       "          6.7296e-02,  4.3063e-03, -3.0754e-02,  3.1389e-02, -9.7649e-03,\n",
       "         -8.8222e-03,  5.0234e-02,  1.1227e-02, -4.7887e-02, -1.6789e-02,\n",
       "         -2.3837e-04,  1.3134e-02, -3.0588e-02, -4.6851e-02, -1.1730e-02,\n",
       "         -1.3734e-02, -2.6956e-02, -1.4703e-03, -2.5719e-02,  1.2039e-02,\n",
       "         -3.3601e-02, -2.1750e-02, -6.5095e-03,  4.1026e-02, -3.5098e-02,\n",
       "          5.8729e-02, -7.7976e-03,  1.1163e-02,  5.6933e-02, -3.0623e-02,\n",
       "          4.7454e-02, -1.1121e-02,  3.8515e-02, -2.6151e-02,  9.1868e-02,\n",
       "         -6.2359e-02,  3.3298e-03, -1.9150e-02,  3.0149e-02,  4.6603e-02,\n",
       "         -5.5430e-02, -1.2846e-02,  1.7783e-02, -1.7369e-02,  6.0084e-02,\n",
       "         -4.0347e-02, -4.7456e-04,  2.9225e-02,  7.8668e-02, -3.0583e-02,\n",
       "         -3.5824e-02, -4.5679e-02, -2.7486e-02,  8.7179e-03,  2.5145e-02,\n",
       "         -4.1498e-03,  3.7924e-03,  1.0247e-02, -2.7623e-02, -4.6076e-03,\n",
       "          4.8509e-02,  4.7061e-02, -3.1982e-02, -2.6586e-02, -1.1234e-02,\n",
       "          3.0660e-02, -2.5561e-02,  2.7919e-02, -3.2622e-02, -1.3833e-02,\n",
       "          1.6839e-02,  1.0267e-02,  3.7019e-03, -1.0300e-02, -1.3312e-02,\n",
       "         -4.3982e-02,  5.6308e-02,  4.1751e-02, -2.2242e-02,  4.8350e-02,\n",
       "          1.1824e-02,  6.1103e-02,  5.7214e-02, -2.1682e-02, -2.1462e-02,\n",
       "          5.2020e-02, -1.4123e-02, -2.0408e-03,  9.3203e-03,  1.5221e-02,\n",
       "          5.1876e-02, -3.9700e-02, -5.7060e-03, -2.6038e-03,  6.3580e-03,\n",
       "         -2.7291e-02, -8.5843e-03, -2.7513e-02, -8.8620e-03,  2.4383e-04,\n",
       "          1.0068e-01,  6.5447e-02,  1.1463e-02, -1.7068e-02,  2.9303e-02,\n",
       "         -1.0257e-02,  3.7167e-02, -2.0620e-02,  6.4442e-03, -9.9187e-02,\n",
       "          4.2322e-02, -4.8182e-03, -3.0346e-02, -3.2955e-02, -3.4786e-02,\n",
       "         -3.5757e-02, -8.1767e-03,  2.2286e-02, -2.7997e-02, -7.1478e-04,\n",
       "          7.8046e-03, -2.5660e-02, -1.0418e-02, -1.1985e-02, -2.5260e-02,\n",
       "         -2.7969e-02, -3.8516e-02,  1.1418e-01, -1.8307e-02, -1.0973e-02,\n",
       "          9.7980e-03,  2.6654e-02, -3.0906e-02, -3.3043e-02,  1.2411e-02,\n",
       "          4.0714e-02,  2.2959e-03, -5.7870e-02,  1.5927e-02, -1.3885e-02,\n",
       "          9.2174e-03,  2.3395e-02, -2.5466e-03, -6.5759e-02, -7.9057e-03,\n",
       "          1.3101e-02, -3.0196e-02,  1.3841e-02, -6.5508e-03, -7.3960e-02,\n",
       "          3.0690e-02, -5.5186e-02,  4.4405e-02, -1.1878e-03, -2.7301e-03,\n",
       "         -2.7246e-02, -7.9239e-04,  6.3697e-02, -1.1991e-02, -3.3884e-02,\n",
       "          1.0575e-02,  1.9789e-02, -2.8523e-02,  2.5694e-02, -1.0080e-01,\n",
       "         -6.4948e-03,  1.9300e-02,  2.0170e-02, -4.2268e-03,  3.9764e-02,\n",
       "         -3.9720e-02,  5.3316e-02, -2.2441e-02,  7.8172e-03, -4.5522e-02,\n",
       "          3.4461e-03, -9.9075e-03,  1.3135e-02, -2.1538e-02,  4.2644e-02,\n",
       "         -2.6800e-03, -8.5643e-03,  4.2481e-02, -4.7788e-02, -3.4805e-03,\n",
       "          1.7614e-02, -3.2817e-02,  1.7591e-02, -2.9671e-02, -5.8553e-03,\n",
       "         -3.6791e-02, -7.4685e-03, -5.5805e-02, -1.3192e-02,  1.8202e-02,\n",
       "          2.4811e-02,  6.8183e-03, -3.0693e-02, -9.8621e-03,  2.2026e-02,\n",
       "         -8.4706e-03,  1.5254e-02,  1.1644e-02, -1.4381e-03,  1.8230e-02,\n",
       "         -1.7789e-02,  4.2445e-03, -5.9328e-02,  9.3283e-03, -3.2672e-02,\n",
       "          1.0204e-01,  1.9768e-02,  1.0964e-02, -9.9942e-03,  1.3524e-02,\n",
       "         -1.3421e-02, -1.1870e-03,  4.7052e-02, -9.7042e-02,  9.3137e-03,\n",
       "          8.9500e-03,  1.0774e-02,  8.3606e-02,  2.9623e-02,  1.0310e-01,\n",
       "         -1.0975e-02,  5.3574e-02, -1.4696e-02, -2.0240e-02, -1.9168e-02,\n",
       "          7.2391e-02, -1.6575e-03, -7.2371e-02, -4.4036e-02,  4.4082e-02,\n",
       "          1.7567e-02,  5.0200e-03, -2.9693e-02,  7.4723e-02, -4.0412e-02,\n",
       "          1.4060e-02, -4.9614e-02,  7.0181e-03, -4.9545e-02,  4.1126e-03,\n",
       "         -1.5247e-02, -1.2655e-02,  1.2737e-02, -3.2768e-03,  7.8539e-03,\n",
       "         -1.1654e-02, -3.5073e-03,  3.9867e-02, -3.5984e-03,  2.4887e-02,\n",
       "          1.1947e-02, -2.2594e-02, -1.2591e-02,  1.6425e-02,  8.3449e-03,\n",
       "         -1.2902e-02, -5.8289e-02, -2.8543e-02,  4.6036e-02, -3.0338e-02,\n",
       "          6.1654e-03,  1.7530e-02, -1.8561e-02, -6.7908e-03,  6.0556e-03,\n",
       "          2.1777e-02, -3.9666e-02, -2.6982e-03, -3.5760e-02,  1.1025e-03,\n",
       "          5.2260e-03,  1.0213e-02, -1.8933e-02, -2.1111e-02, -4.3610e-02,\n",
       "          3.9836e-02,  4.0513e-02, -5.9688e-03, -1.6513e-02, -4.7738e-02,\n",
       "          8.1397e-02, -1.3785e-02, -6.5834e-02,  1.0222e-01, -9.9315e-02,\n",
       "          3.5539e-02, -3.4159e-02,  6.5783e-03,  1.3559e-02,  1.6654e-02,\n",
       "         -1.7571e-02,  3.3041e-02, -5.5987e-04,  7.7079e-02,  1.4652e-02,\n",
       "         -2.8118e-02,  8.0673e-03, -3.6362e-02, -6.5124e-03,  5.8852e-02,\n",
       "          3.0636e-02, -4.5597e-02,  4.5532e-02,  1.5580e-02, -3.7854e-02,\n",
       "          3.0520e-02,  5.2865e-03,  3.9113e-02, -4.8027e-02,  3.4481e-02,\n",
       "         -2.1327e-03, -4.3797e-02, -5.8513e-02, -5.3495e-02,  5.9550e-03,\n",
       "          1.6909e-02,  8.3575e-03,  1.7611e-02, -3.3780e-04,  8.3315e-03,\n",
       "          2.2584e-02,  4.1335e-02, -6.9597e-02,  3.0540e-02, -9.1633e-02,\n",
       "          1.6627e-02,  1.6000e-02,  4.0497e-02, -3.8488e-02, -3.0428e-02,\n",
       "         -2.9797e-02, -5.3884e-04,  1.5816e-02, -3.3490e-02,  1.0078e-01,\n",
       "          1.1987e-02, -2.4973e-03,  1.1679e-02, -1.1208e-02, -5.5079e-04,\n",
       "          5.1852e-02, -2.0885e-02,  4.1902e-02,  1.2307e-02, -3.2866e-02,\n",
       "         -1.5866e-02,  4.4350e-02,  1.1173e-02,  4.9135e-02,  7.7859e-02,\n",
       "         -8.0359e-03,  9.8259e-03, -3.9951e-02,  3.1372e-02,  3.3666e-02,\n",
       "         -4.8512e-02, -1.6975e-03, -1.7866e-02,  3.4123e-02, -7.8872e-02,\n",
       "          1.6226e-02,  3.6809e-02,  1.0651e-01,  6.1249e-03, -4.0039e-02,\n",
       "         -1.9251e-02, -4.0490e-02, -3.7117e-02,  9.8318e-03,  5.3540e-02,\n",
       "          4.4614e-02, -3.7663e-02,  2.1105e-04,  1.4376e-02, -1.0833e-02,\n",
       "         -5.0133e-02, -1.0243e-01, -6.1907e-02, -5.9460e-02,  7.8544e-03,\n",
       "          2.2755e-02,  4.7144e-02, -8.6734e-03, -4.1631e-02, -2.2038e-02,\n",
       "          1.7260e-02, -7.8609e-02, -1.5686e-02,  2.4919e-02,  1.5853e-02,\n",
       "         -6.9563e-02, -1.3307e-02,  7.0851e-03,  3.7567e-02,  2.0932e-02,\n",
       "         -3.5008e-02, -7.5983e-02,  1.7958e-02, -5.4147e-02, -1.9581e-02,\n",
       "          1.5714e-02,  6.5203e-02,  8.0148e-02, -4.5967e-02, -2.0307e-02,\n",
       "          1.8379e-02, -7.4866e-03, -1.6249e-03,  6.3041e-03, -1.4236e-02,\n",
       "         -1.0788e-02,  2.7614e-02, -8.1731e-03, -4.4553e-02, -1.0530e-02,\n",
       "          1.9087e-02, -1.2958e-02,  3.1801e-02,  3.8866e-02, -3.1124e-02,\n",
       "         -1.6701e-02,  2.9904e-02, -5.5950e-03, -3.7753e-03, -3.1394e-02,\n",
       "          4.1315e-02, -1.5343e-02,  4.1217e-02, -2.1412e-02,  2.7605e-02,\n",
       "         -3.8371e-02,  1.3184e-02, -2.9085e-03,  1.9654e-03, -6.3788e-03,\n",
       "          1.1849e-02, -2.0978e-02, -5.3884e-02,  2.0506e-02, -1.3498e-02,\n",
       "          2.0262e-02, -2.1746e-02, -1.1879e-02, -7.7625e-02,  2.0937e-03,\n",
       "          6.3201e-02,  1.7469e-02, -7.7858e-03, -2.3737e-03,  3.6635e-02,\n",
       "          4.6044e-02, -9.1415e-03, -6.0501e-02,  9.5129e-03, -5.4561e-02,\n",
       "          8.9611e-02,  1.3883e-01,  2.2949e-02, -3.4627e-02, -1.5252e-02,\n",
       "          1.4927e-02, -7.2813e-02,  2.7549e-02,  3.2275e-02,  3.8709e-02,\n",
       "          2.6040e-02, -4.1264e-02,  3.9399e-02,  6.4339e-03, -5.1494e-02,\n",
       "         -1.9641e-02, -1.5259e-02, -2.3390e-02, -1.0278e-02,  2.3099e-02,\n",
       "         -5.9784e-33, -2.6005e-02, -5.7004e-02, -3.4476e-03, -7.2321e-02,\n",
       "         -1.3744e-02,  6.1441e-03, -1.1005e-02,  4.5438e-02, -3.2270e-02,\n",
       "         -2.8874e-02, -9.9697e-03,  1.4921e-02,  2.2435e-02,  7.4662e-03,\n",
       "         -9.2724e-03, -6.3310e-03, -2.5291e-03, -2.5509e-02,  4.4850e-03,\n",
       "         -6.6836e-02, -8.6988e-03,  2.7983e-02,  3.4283e-02,  1.4660e-02,\n",
       "         -1.7656e-02, -2.0396e-02, -5.0061e-03, -2.5807e-02, -5.5199e-02,\n",
       "          4.2905e-02, -4.1620e-02, -4.1155e-02, -1.0421e-02, -1.6425e-02,\n",
       "          2.5329e-02,  1.3550e-02, -3.0205e-02, -4.7398e-02,  1.0069e-03,\n",
       "          2.0344e-03, -8.6301e-02, -4.4392e-03, -2.8304e-03, -5.2532e-02,\n",
       "          2.5719e-04, -2.6272e-02,  2.4998e-02, -1.7822e-02, -1.6354e-02,\n",
       "          5.7475e-02, -5.7936e-02,  1.6557e-02, -9.5128e-03,  5.5306e-02,\n",
       "          6.5796e-02, -1.5616e-03,  7.9345e-03, -2.8869e-03, -6.2763e-03,\n",
       "         -6.6403e-03,  9.0889e-03,  3.2507e-02, -2.5347e-03, -4.0519e-03,\n",
       "          2.4574e-02,  3.5081e-02, -5.6168e-02,  8.1133e-03,  3.3233e-02,\n",
       "         -4.1477e-02,  4.5679e-02,  4.6645e-02,  3.2799e-02,  2.0128e-02,\n",
       "         -2.3764e-02, -6.3482e-02,  3.6916e-03,  2.9018e-02,  3.1700e-02,\n",
       "         -2.4016e-02,  1.8958e-02, -5.2400e-04, -5.3391e-02, -2.5788e-03,\n",
       "         -5.4757e-02, -7.3021e-02, -4.6921e-02, -8.6459e-02,  1.5756e-02,\n",
       "         -1.6275e-02, -1.9756e-02,  1.3305e-02,  1.2799e-02, -5.2233e-02,\n",
       "          4.7457e-03, -2.1343e-02, -1.6183e-02,  4.8519e-02, -1.3637e-02,\n",
       "         -2.5064e-02,  1.2529e-02, -1.8684e-02,  2.0649e-02, -7.8163e-03,\n",
       "          3.1622e-02, -3.0660e-03, -1.0440e-02, -3.9340e-02, -5.9176e-02,\n",
       "         -1.7654e-02,  2.1758e-02, -6.6238e-02,  2.6934e-02, -1.0373e-02,\n",
       "          4.2127e-03,  2.0652e-02,  1.5319e-02,  1.8220e-02,  6.4086e-02,\n",
       "          2.1409e-02, -4.9442e-02,  3.6212e-02, -5.4964e-02,  4.5302e-02,\n",
       "         -1.0897e-02, -1.3174e-02, -5.5072e-02,  7.0078e-02,  1.1276e-02,\n",
       "         -4.8131e-02,  4.5643e-06, -6.2324e-04,  2.1305e-07, -2.6060e-02,\n",
       "          7.5140e-02, -2.6260e-02,  1.3146e-02,  1.5953e-02, -1.5365e-02,\n",
       "         -6.2466e-02,  2.5697e-02,  5.9362e-02, -2.5600e-02,  2.1216e-02,\n",
       "         -2.9229e-02,  2.9460e-02,  1.5180e-02, -4.2473e-02, -4.3577e-03,\n",
       "         -1.2782e-02, -1.1880e-02, -2.0094e-02,  1.3873e-03, -8.9287e-03,\n",
       "          1.0459e-01,  3.4227e-02,  2.8689e-02,  1.5603e-03, -5.5679e-02,\n",
       "          1.4359e-02, -4.4711e-02,  2.0003e-03, -2.9805e-02,  7.4770e-03,\n",
       "          3.0288e-03,  2.8839e-02,  4.9741e-02, -3.9215e-02,  1.8939e-02,\n",
       "          3.0220e-02,  4.5663e-03,  3.1303e-02,  5.9107e-02,  1.8480e-02,\n",
       "         -7.5508e-03, -7.3337e-03, -7.7017e-02,  4.9755e-02, -1.2346e-02,\n",
       "          1.2251e-02, -5.9442e-02, -6.3908e-03, -2.4806e-02,  5.7017e-02,\n",
       "         -1.3984e-02,  5.6505e-03,  1.7318e-02,  5.7420e-03, -3.5224e-02,\n",
       "          3.7737e-02,  2.0808e-02,  4.7060e-02,  6.5803e-03, -5.3942e-02,\n",
       "         -4.1410e-02,  1.0645e-02,  5.3546e-03,  3.9682e-02, -9.5275e-03,\n",
       "         -2.6818e-02,  2.1862e-34, -1.2599e-03,  8.6353e-03,  4.1232e-02,\n",
       "          9.1922e-04,  2.6399e-02, -3.9402e-02,  6.4555e-02, -3.1777e-03,\n",
       "          2.1199e-02, -8.5429e-02, -2.0571e-02], device='cuda:0'),\n",
       " 'this is a second sentence for the model': tensor([ 3.9853e-02,  1.1305e-02, -1.5530e-02,  3.3976e-02, -3.2799e-02,\n",
       "          4.2354e-02,  1.6398e-02, -3.1444e-02, -2.6944e-02, -3.0731e-02,\n",
       "          4.7369e-02,  2.1103e-02,  3.3010e-02,  3.4032e-02,  3.8007e-02,\n",
       "         -6.5235e-02,  6.4036e-02, -6.4416e-03,  3.4543e-03, -9.0102e-03,\n",
       "         -1.7275e-02,  3.0512e-03,  4.5270e-03,  3.0739e-02, -7.7401e-03,\n",
       "         -2.2511e-02, -1.7440e-02,  1.8241e-02, -3.1460e-02, -2.4304e-02,\n",
       "          1.6757e-02,  4.2077e-02,  4.2986e-02, -5.7011e-02,  1.8659e-06,\n",
       "         -1.5055e-04, -3.5079e-02, -7.8792e-03, -6.3944e-02,  2.6660e-02,\n",
       "          2.7459e-02,  5.7436e-02, -2.7292e-03,  1.7998e-03, -1.7464e-02,\n",
       "          3.2718e-02,  1.7794e-02,  9.5034e-02, -5.1918e-02,  4.2457e-02,\n",
       "         -1.4360e-02, -1.8686e-02,  3.2093e-02, -2.7755e-02, -8.6776e-04,\n",
       "         -3.6427e-02,  2.8627e-02,  2.5594e-02,  5.2025e-03,  4.5981e-02,\n",
       "          1.4797e-02,  7.9453e-02,  1.4237e-02, -2.5163e-02,  8.5444e-02,\n",
       "         -2.9834e-03,  9.5523e-02, -4.8031e-02, -6.1396e-03, -1.3536e-02,\n",
       "          8.9872e-02,  1.1748e-02, -4.0469e-03,  6.4859e-02,  6.9229e-04,\n",
       "          6.1761e-03, -2.6548e-02,  1.7706e-02, -1.1893e-02,  2.8181e-03,\n",
       "          2.4235e-02,  1.0556e-02,  6.2031e-03, -1.9511e-02, -4.2652e-02,\n",
       "          4.8785e-02, -2.3748e-02,  1.5203e-02, -7.7336e-02,  1.7944e-02,\n",
       "         -6.4315e-02, -4.0665e-02, -4.3704e-03,  5.8987e-03, -1.7274e-02,\n",
       "         -1.1345e-02, -3.4770e-02,  1.4649e-03, -1.3459e-02, -7.6982e-02,\n",
       "          5.4125e-02, -5.1850e-02,  4.9801e-02,  1.1062e-02,  3.1806e-02,\n",
       "          3.0331e-02, -3.0433e-02,  1.9969e-02, -9.5139e-02,  4.2446e-02,\n",
       "         -3.2308e-02, -6.9475e-02, -7.0162e-02,  3.2449e-02, -2.7896e-02,\n",
       "         -3.8636e-02, -1.0051e-02, -5.4077e-04,  2.8931e-02, -1.5484e-02,\n",
       "         -3.0151e-02, -2.7219e-02, -2.4301e-02,  3.6257e-02,  4.5522e-03,\n",
       "          4.4990e-03, -1.4784e-02,  3.2180e-02,  4.5232e-02, -5.6787e-02,\n",
       "         -1.4046e-02, -1.5108e-02, -1.0389e-03, -9.0619e-03, -5.4341e-02,\n",
       "          3.3541e-02,  2.3445e-02,  5.5204e-03, -2.1864e-02, -2.7279e-02,\n",
       "          1.7869e-02, -6.3586e-02, -4.4193e-02, -5.5302e-03, -1.7853e-02,\n",
       "          2.9957e-02,  3.9190e-02, -4.8674e-02, -4.1793e-04,  6.2659e-04,\n",
       "         -2.6489e-02,  2.5906e-02, -4.3552e-02,  4.7569e-03, -4.0115e-03,\n",
       "         -4.5154e-03,  8.9170e-02,  4.0415e-02,  2.2299e-02, -2.4736e-02,\n",
       "          2.5039e-02,  4.3105e-02,  8.3646e-03, -2.1240e-02, -7.7749e-03,\n",
       "          8.8703e-03, -7.9582e-02,  8.9160e-03, -4.6286e-03, -2.8923e-02,\n",
       "         -2.3731e-02,  5.4706e-04, -2.2265e-02,  1.5990e-01,  3.0597e-02,\n",
       "          1.2955e-02,  2.0435e-02,  3.1622e-02, -4.2977e-02,  1.4323e-02,\n",
       "          9.7429e-04,  1.6631e-02, -3.4071e-02,  3.5967e-02, -1.1233e-01,\n",
       "          2.0006e-02, -4.9228e-02, -3.8982e-02, -3.2121e-02, -6.6063e-02,\n",
       "         -3.1331e-02,  1.6814e-02, -2.4086e-02, -9.0227e-03, -2.7560e-03,\n",
       "          4.1869e-02,  7.6388e-03,  6.6384e-02,  1.4605e-02, -1.7430e-02,\n",
       "         -9.3572e-03, -3.5865e-02,  5.2309e-02,  2.2023e-02, -3.6126e-02,\n",
       "          2.3450e-02, -3.0819e-02, -3.2380e-02, -8.0254e-03,  1.5455e-02,\n",
       "          9.6441e-02,  2.7848e-02,  8.6147e-04,  1.7256e-02, -1.8606e-02,\n",
       "         -4.1918e-02,  2.4476e-02, -2.4388e-02, -8.6696e-02, -6.1741e-03,\n",
       "         -1.5805e-02,  3.5293e-02, -2.8941e-02,  4.3219e-03, -6.5049e-02,\n",
       "          9.7916e-03, -6.5552e-02,  1.3934e-02, -2.1088e-02, -1.5394e-02,\n",
       "         -3.7195e-03,  3.6107e-02,  2.4913e-03,  2.6098e-02, -4.3429e-02,\n",
       "          1.8902e-02, -2.3169e-02, -1.3037e-02,  7.5416e-03, -4.3025e-03,\n",
       "          3.2323e-02,  1.6143e-02,  3.9453e-02,  4.6575e-03, -7.3162e-04,\n",
       "         -4.8476e-02,  4.0554e-02, -7.5506e-04,  4.1730e-02, -2.6559e-02,\n",
       "          2.4388e-02,  4.8815e-04,  3.3129e-03, -1.1612e-03, -3.4872e-02,\n",
       "         -1.2179e-02, -9.2883e-03,  1.9850e-02, -2.4988e-02,  9.3164e-03,\n",
       "          3.3207e-02, -6.3329e-03,  6.6795e-03,  7.1888e-03, -3.0185e-02,\n",
       "         -1.7846e-02,  1.6374e-03, -3.5521e-02, -9.5814e-03,  1.6352e-02,\n",
       "         -2.9561e-02, -4.3089e-03,  1.9897e-02, -2.3807e-02,  3.4024e-02,\n",
       "          1.2079e-02, -2.7157e-03, -2.9950e-03, -2.3881e-02,  2.6937e-02,\n",
       "          1.8528e-02, -2.5487e-02, -3.7470e-02, -4.0514e-03,  8.2205e-03,\n",
       "          2.2534e-02,  4.7312e-02, -3.2407e-02, -1.7173e-02,  5.8008e-02,\n",
       "         -5.8090e-03,  1.8212e-03,  2.3373e-02, -5.6652e-02,  7.4102e-03,\n",
       "         -2.4391e-02,  3.2246e-02,  3.1237e-02,  1.5179e-02,  1.3412e-01,\n",
       "         -2.4393e-02,  4.0968e-03, -1.2620e-02, -1.6514e-02, -2.6450e-02,\n",
       "          1.6528e-02, -4.5273e-02, -2.9881e-02, -2.0938e-02,  6.1509e-02,\n",
       "          3.6312e-02,  1.7396e-02,  2.1458e-02,  9.1353e-02, -7.3204e-03,\n",
       "         -2.3386e-02, -2.4082e-02, -6.5165e-03,  1.1754e-02,  5.2084e-02,\n",
       "         -9.9590e-03, -1.5071e-02, -6.5228e-03,  3.9643e-03,  2.8224e-03,\n",
       "         -1.6303e-02, -2.1513e-03,  3.2926e-02, -1.2071e-03,  6.2889e-03,\n",
       "          4.3672e-03,  3.9333e-02,  1.3804e-02,  2.5805e-02, -3.7047e-02,\n",
       "          1.4800e-02, -2.4140e-02, -4.1903e-02,  6.2090e-03,  1.5825e-02,\n",
       "         -1.1106e-02, -3.6344e-02,  4.8631e-03, -8.8057e-03, -5.0451e-02,\n",
       "          3.4278e-02, -4.2154e-02,  7.0655e-03, -8.6759e-02, -1.0548e-02,\n",
       "          4.1248e-02, -1.9380e-02, -5.3625e-02, -1.8268e-02, -6.1937e-02,\n",
       "         -2.1860e-02,  5.8745e-02, -1.5774e-02,  1.9555e-02, -8.4031e-02,\n",
       "          1.1441e-01, -2.4992e-03, -7.3775e-02,  8.2692e-02, -5.7786e-02,\n",
       "          1.1870e-02,  3.6838e-02,  7.2000e-02, -6.3100e-02, -7.4759e-03,\n",
       "          3.4238e-02,  1.9752e-02, -1.5140e-02,  6.1890e-02, -6.3183e-02,\n",
       "          3.2393e-02,  9.9806e-03, -1.5425e-02,  1.4819e-02,  3.9031e-02,\n",
       "          2.5635e-02,  2.1102e-03,  3.7029e-02,  4.0893e-02, -2.2786e-02,\n",
       "          5.3319e-03,  2.0831e-02, -4.1808e-02, -9.8035e-03,  6.5485e-02,\n",
       "          3.1954e-02, -4.1565e-02, -1.9602e-02,  6.4375e-03, -2.5579e-02,\n",
       "          7.0447e-03, -2.4169e-02,  3.7988e-02, -1.8700e-02,  1.6498e-02,\n",
       "          8.7205e-03,  6.0422e-02, -6.3196e-03, -3.4678e-02, -3.8459e-02,\n",
       "          1.3465e-02,  1.0788e-02,  1.7888e-02, -2.9886e-02,  6.7968e-02,\n",
       "          4.1838e-02,  8.2403e-03,  1.1116e-02, -3.1104e-04,  1.5779e-02,\n",
       "         -1.5699e-02, -1.7498e-02, -2.7905e-02,  5.9462e-03,  1.4504e-02,\n",
       "         -1.6118e-02, -7.9082e-03,  3.9147e-02, -4.6337e-03,  1.4434e-02,\n",
       "         -4.3789e-02,  2.3912e-02,  1.0225e-02,  1.5270e-02,  6.3828e-02,\n",
       "          6.1631e-03, -2.6617e-02, -5.4900e-02, -3.0768e-02, -3.5401e-02,\n",
       "         -1.0825e-02, -7.0304e-03, -2.4085e-02,  2.5945e-02, -3.4471e-02,\n",
       "         -1.1709e-03, -9.9391e-03,  1.1357e-01, -9.3461e-03,  2.5750e-02,\n",
       "          4.6495e-02, -4.1427e-03, -1.2249e-02, -7.0735e-03,  2.2822e-02,\n",
       "          9.9766e-03, -2.0785e-02, -9.8715e-03,  1.6122e-02, -2.8562e-02,\n",
       "         -4.3411e-02, -1.0124e-01, -5.1252e-02, -9.4552e-03,  3.2638e-03,\n",
       "          4.9235e-02,  4.6665e-02,  6.7352e-03, -1.7451e-02, -2.5992e-02,\n",
       "         -2.6486e-02, -8.8140e-03, -4.3058e-02,  1.2122e-02,  2.6457e-02,\n",
       "         -3.1122e-02,  4.5820e-03,  2.7846e-02,  2.4676e-02,  5.5400e-02,\n",
       "         -2.1554e-02, -5.3654e-02, -1.5820e-02, -5.0833e-02,  1.2347e-02,\n",
       "          3.8745e-02,  7.3890e-02,  5.5486e-02, -1.7145e-02, -3.9129e-02,\n",
       "          9.7843e-03, -4.0927e-02, -1.9026e-02, -5.2142e-02,  6.8539e-02,\n",
       "         -1.5030e-02,  2.0720e-02, -1.0242e-02, -4.5639e-02,  2.7364e-02,\n",
       "          1.5272e-02, -1.5777e-02,  2.4637e-02,  1.5942e-02, -3.9658e-02,\n",
       "         -2.8381e-03,  3.0131e-02,  1.3039e-02,  6.0629e-02, -3.2106e-02,\n",
       "         -1.4210e-02,  4.6195e-03,  8.5318e-03, -1.1917e-02,  1.2160e-02,\n",
       "         -8.6629e-02,  1.1121e-02, -3.1451e-02, -6.5299e-02, -4.4452e-03,\n",
       "          4.0792e-02, -2.6051e-02, -4.8159e-02,  4.7565e-02, -1.0418e-02,\n",
       "          9.0514e-03, -3.4119e-02,  5.4560e-02, -2.3783e-02, -5.3258e-02,\n",
       "          2.1054e-02,  6.1410e-03,  1.1378e-02, -8.9030e-02,  3.8761e-02,\n",
       "          5.1730e-02, -8.2405e-04, -9.7403e-02,  3.1316e-03, -4.5720e-02,\n",
       "         -2.1645e-02,  5.3725e-02, -7.5708e-02,  2.0489e-02,  2.0703e-02,\n",
       "          4.1119e-02, -6.2625e-02,  6.2363e-02,  2.0277e-02,  5.3635e-02,\n",
       "         -2.8824e-02, -2.0756e-02,  4.8505e-02, -6.1265e-03, -9.9842e-02,\n",
       "         -4.3463e-02, -5.6833e-03,  3.5134e-02,  3.5849e-02,  9.3612e-02,\n",
       "         -6.2474e-33, -5.3664e-02,  1.0866e-02, -4.3884e-03, -1.1630e-02,\n",
       "         -5.4441e-02, -1.6007e-02, -1.3713e-02,  2.8947e-02, -1.1638e-02,\n",
       "          3.3288e-03, -2.6542e-02,  2.2787e-02,  2.3279e-02,  6.0657e-03,\n",
       "          2.4356e-02,  2.7841e-03,  2.8626e-02, -1.4249e-02,  1.8259e-02,\n",
       "         -3.4038e-02,  5.2308e-03,  3.7962e-02,  6.1146e-02,  3.8321e-02,\n",
       "          1.8611e-02, -5.9574e-02, -3.9560e-02,  1.2600e-02, -8.0546e-02,\n",
       "          3.0029e-02, -1.5014e-02, -5.4546e-02, -5.8590e-03, -1.3316e-02,\n",
       "         -3.7077e-02,  8.4010e-03,  1.4955e-02, -2.2638e-02,  2.6839e-02,\n",
       "         -1.1520e-02, -7.0554e-02, -7.5731e-02, -5.6388e-03, -1.3329e-02,\n",
       "         -2.4963e-02, -4.2803e-02,  3.3230e-03, -7.6760e-02,  1.5658e-02,\n",
       "          1.1409e-02, -7.1467e-02, -7.4340e-03, -2.9106e-02,  5.4970e-02,\n",
       "          4.3238e-02,  1.6146e-02, -1.6422e-02, -2.7900e-02,  2.5108e-02,\n",
       "          1.9824e-02,  6.5068e-02,  5.1930e-02, -3.2640e-03,  1.7794e-02,\n",
       "          5.1171e-03,  6.4652e-04,  7.0565e-03,  1.1611e-02, -5.7526e-02,\n",
       "          3.6424e-02, -5.4919e-04,  8.5538e-02,  3.4907e-02,  3.3082e-02,\n",
       "         -3.4138e-02, -6.9273e-02, -4.9496e-02, -1.1869e-02, -2.8773e-03,\n",
       "          4.9830e-02,  2.5145e-02,  1.2393e-03, -1.6946e-02, -2.8228e-02,\n",
       "         -5.6037e-02, -8.6325e-02, -2.5273e-02,  1.0981e-02,  6.7939e-03,\n",
       "         -5.9140e-03,  1.7674e-02,  6.6238e-03, -1.2853e-02, -8.8420e-03,\n",
       "         -4.3084e-02, -5.0876e-04,  4.2477e-02,  1.0469e-02, -2.7855e-02,\n",
       "          1.1988e-02,  8.6652e-03, -7.9162e-03, -1.0821e-02,  5.0749e-02,\n",
       "          1.1994e-02,  5.7245e-02,  4.2382e-02,  1.4877e-03, -3.8429e-02,\n",
       "         -1.1310e-02, -7.9875e-03, -5.3664e-02,  3.1674e-02,  1.6109e-02,\n",
       "         -8.2438e-03, -1.6370e-02,  1.0861e-02,  7.9972e-02, -2.7136e-02,\n",
       "         -1.6811e-02, -2.8653e-02,  8.1450e-02, -3.2348e-02,  5.6956e-02,\n",
       "          9.5579e-03,  2.7384e-03, -5.3272e-02,  2.6178e-02,  3.2162e-02,\n",
       "         -5.9483e-02, -1.2844e-02, -9.7315e-03,  2.4954e-07, -1.4216e-02,\n",
       "          1.4514e-02,  1.1380e-02,  5.6232e-02,  8.4293e-03,  1.4116e-02,\n",
       "         -5.8524e-02,  2.3085e-02,  3.0009e-02,  4.3846e-02,  4.8183e-02,\n",
       "          7.2366e-03,  2.0919e-02, -4.0813e-02, -1.5853e-02,  1.5228e-02,\n",
       "         -6.1490e-02, -2.7012e-02, -2.0007e-02,  4.5877e-03, -1.6316e-02,\n",
       "          5.6832e-02,  6.1652e-02,  1.7046e-02, -1.9570e-02, -4.0925e-03,\n",
       "          9.7004e-03, -5.4825e-02,  2.9912e-03,  3.2864e-02,  5.0425e-02,\n",
       "         -6.3522e-03,  2.8457e-03,  2.3898e-02,  6.5618e-03, -1.0566e-02,\n",
       "          6.0513e-02,  7.2271e-02,  1.5968e-02,  4.7255e-02,  8.4878e-04,\n",
       "          1.7484e-03, -2.8498e-02, -3.5953e-02,  5.1570e-02, -2.7068e-02,\n",
       "          3.7393e-02, -6.1447e-02, -1.8251e-02,  1.3088e-02,  3.7689e-02,\n",
       "          2.3134e-03,  1.4958e-02, -8.6107e-03,  1.7932e-02, -4.0969e-02,\n",
       "          3.0196e-02,  1.2315e-02,  2.9049e-02, -3.1305e-04, -3.3843e-02,\n",
       "          1.3228e-02,  1.3911e-03,  4.7468e-03,  6.3460e-02, -5.4210e-02,\n",
       "         -2.7325e-02,  1.7667e-34,  2.4031e-02, -1.4227e-02,  8.9120e-03,\n",
       "          3.1289e-02,  5.7587e-03, -2.6737e-02,  4.3840e-02, -2.6024e-02,\n",
       "         -1.2408e-02, -4.9699e-02, -4.6748e-02], device='cuda:0'),\n",
       " 'The sky is blue': tensor([-2.2825e-03, -3.4933e-02,  2.9562e-03, -4.3063e-02, -3.9971e-02,\n",
       "         -3.5807e-02, -3.6068e-02, -9.4580e-03, -5.4959e-02,  3.0772e-02,\n",
       "         -3.2340e-03,  4.1537e-02,  3.1238e-02,  3.4090e-02,  5.6298e-02,\n",
       "         -1.3638e-01,  4.2279e-02, -3.8701e-03,  3.0775e-02,  2.4350e-03,\n",
       "          3.4368e-03,  2.0491e-02, -1.8735e-02, -4.2031e-02,  3.4437e-02,\n",
       "         -3.3553e-02, -4.1880e-03,  3.2970e-02,  3.9334e-02,  2.6029e-02,\n",
       "         -5.6698e-02, -3.7108e-02,  6.8113e-03, -3.7833e-02,  1.7468e-06,\n",
       "         -3.9490e-03,  4.3769e-03,  1.4412e-02,  3.7852e-02, -7.0734e-02,\n",
       "         -4.2909e-03,  3.3623e-03, -3.3486e-02,  4.7746e-02, -1.0369e-02,\n",
       "          7.8594e-03,  5.5545e-03,  8.0023e-02,  1.0587e-02,  4.8463e-03,\n",
       "         -1.4039e-04,  2.9871e-03, -4.1537e-02,  3.3504e-03,  2.6165e-02,\n",
       "         -2.3552e-02,  2.3621e-02,  3.7312e-02,  6.1450e-02,  4.3204e-02,\n",
       "         -1.1972e-02,  6.7264e-03,  6.6770e-03,  1.7248e-02,  1.5910e-02,\n",
       "          3.9992e-02,  1.7708e-02,  4.3123e-02, -2.1235e-02,  2.1698e-02,\n",
       "         -8.1313e-02, -1.1960e-02, -1.5603e-02,  1.4225e-02, -3.8444e-02,\n",
       "         -1.9401e-02,  9.5023e-03,  6.6408e-02,  4.4270e-02, -3.7558e-03,\n",
       "          4.2359e-02,  6.5853e-03, -1.0691e-02,  5.5916e-02,  1.4041e-02,\n",
       "          1.0285e-01,  4.1993e-04,  2.3820e-02,  1.1479e-02,  3.2298e-02,\n",
       "          2.0514e-02, -3.6793e-02, -4.1323e-02, -4.9383e-03, -1.2469e-02,\n",
       "          1.4996e-02, -1.5033e-02, -2.2357e-02, -4.8218e-02,  7.2232e-02,\n",
       "          8.3254e-02,  1.7673e-02,  2.3022e-02,  6.6403e-02,  1.7630e-02,\n",
       "         -1.0418e-02, -1.1241e-03, -8.8008e-02,  2.1003e-05,  6.3563e-03,\n",
       "          1.6257e-02, -1.5944e-03,  1.8828e-02, -6.9733e-02,  3.7898e-04,\n",
       "         -4.6258e-02,  1.7152e-02, -1.0642e-02,  3.1027e-02,  1.1260e-03,\n",
       "          2.7675e-02,  1.5531e-02,  1.3867e-03,  4.5955e-02, -3.7010e-03,\n",
       "         -3.0433e-02, -5.0106e-02,  3.1872e-02,  6.8573e-03, -7.9511e-02,\n",
       "         -6.6196e-03,  2.0857e-02, -8.1385e-03, -2.5583e-02, -9.7675e-03,\n",
       "         -6.5104e-02,  9.3662e-04,  8.3769e-03, -2.3396e-02, -2.4198e-02,\n",
       "          1.4035e-02, -1.6953e-02, -1.2315e-02,  1.5545e-02, -1.3231e-02,\n",
       "         -1.5094e-02,  1.3728e-02, -2.0972e-02, -9.7300e-03, -1.4862e-02,\n",
       "          1.7177e-02,  1.7143e-02, -1.0214e-01, -1.6632e-02, -2.4249e-02,\n",
       "         -7.5876e-03, -5.1679e-03,  7.0782e-03, -5.2447e-05, -1.9840e-02,\n",
       "          7.1352e-03, -1.6764e-02,  4.7146e-02, -1.1754e-02, -2.8574e-02,\n",
       "          1.2920e-02, -4.9653e-02, -4.0338e-03,  5.5704e-03,  1.5320e-02,\n",
       "          8.4139e-03, -2.6548e-02,  4.4777e-02,  6.5080e-02,  3.9609e-03,\n",
       "         -3.1207e-02, -7.4700e-02, -9.1172e-03, -6.8010e-03,  3.1626e-02,\n",
       "         -5.0209e-02, -2.2173e-02, -2.3936e-02,  3.4367e-02, -5.2217e-03,\n",
       "         -4.5040e-03,  4.5459e-02, -6.7912e-04, -3.9717e-02,  1.2685e-02,\n",
       "         -2.1683e-02,  1.4660e-02, -3.5248e-02,  1.5203e-02, -2.1409e-03,\n",
       "         -4.8604e-02,  3.0344e-02, -3.9621e-02, -1.0279e-02,  1.7211e-02,\n",
       "          6.8112e-03,  3.1580e-02,  3.8894e-02, -4.9930e-03, -4.2430e-02,\n",
       "         -6.4909e-03,  4.7240e-02, -2.5066e-02,  4.3697e-02, -4.5142e-02,\n",
       "          2.6420e-02,  8.1646e-03,  1.5767e-03,  1.5483e-02, -1.6024e-02,\n",
       "         -3.3539e-02, -1.1461e-02,  8.3144e-03, -2.2137e-02,  2.8950e-02,\n",
       "         -1.2181e-02,  2.0746e-02, -1.1959e-02,  2.8405e-02, -2.1360e-02,\n",
       "          3.8134e-02, -1.6502e-02,  6.3293e-03, -6.6321e-03, -3.2642e-03,\n",
       "         -2.6683e-02,  4.0777e-03,  4.0970e-02,  3.2725e-03, -2.7160e-02,\n",
       "          5.0842e-02,  4.3440e-03, -5.8465e-02, -4.3874e-02, -1.1498e-01,\n",
       "          4.1413e-02,  5.4210e-02, -2.3024e-02, -5.4029e-02, -1.2956e-02,\n",
       "          2.6615e-02, -8.0545e-02,  3.6652e-02, -6.8674e-03,  2.7225e-03,\n",
       "         -1.2266e-02,  2.9288e-02, -6.6803e-02, -1.3403e-02,  1.7891e-02,\n",
       "         -9.1660e-02,  7.9743e-02,  5.3128e-03, -8.9398e-02, -7.4644e-03,\n",
       "          6.9491e-03, -3.4920e-02,  6.0795e-03, -3.3058e-02,  2.4885e-02,\n",
       "         -1.7843e-02,  6.3762e-02,  5.6947e-02, -1.9847e-02, -1.6774e-02,\n",
       "          2.5326e-02,  3.4653e-02,  1.0847e-02, -2.5958e-02, -2.1221e-02,\n",
       "          6.2973e-03,  1.1216e-02, -1.4985e-03, -2.0956e-02,  1.4264e-03,\n",
       "         -1.5597e-03, -1.8684e-02, -1.1661e-02, -2.3481e-02,  1.3649e-02,\n",
       "          4.0571e-02,  3.1652e-02, -3.4197e-02, -2.6658e-02,  2.0671e-02,\n",
       "         -4.2109e-02,  1.2079e-02, -8.5386e-05,  9.1222e-02,  3.6086e-02,\n",
       "         -4.9038e-02, -5.8879e-02,  3.9603e-02,  2.7801e-02, -8.5881e-02,\n",
       "          2.5174e-02, -1.3287e-03,  1.2055e-02, -8.3580e-03,  2.4215e-02,\n",
       "          3.0564e-02,  7.6080e-04, -2.9743e-02,  1.5390e-02,  5.3103e-02,\n",
       "         -1.2024e-03,  3.5928e-05, -3.0384e-03,  5.8089e-02, -2.8156e-02,\n",
       "         -3.8615e-02, -5.4691e-02,  1.9795e-02,  9.4579e-03,  1.2146e-02,\n",
       "         -4.7964e-03, -2.7111e-02,  6.0842e-03,  3.8696e-04,  4.7004e-02,\n",
       "          1.7130e-02,  3.3382e-02,  4.4766e-02,  6.2630e-03, -7.6138e-02,\n",
       "          5.7530e-03,  1.0945e-01, -1.0176e-02, -1.3023e-02, -3.2689e-02,\n",
       "         -4.0509e-02, -6.6162e-04, -4.4832e-02, -7.1243e-02,  2.2733e-02,\n",
       "         -5.7531e-03, -1.3453e-03, -3.1444e-02, -3.1512e-02, -1.3944e-01,\n",
       "         -1.4577e-02, -3.2746e-02,  8.4679e-03,  4.0082e-02, -6.5705e-03,\n",
       "         -2.6659e-02, -1.2462e-02,  9.2320e-03,  1.0785e-02, -7.4542e-03,\n",
       "         -4.5248e-02,  7.8485e-03,  2.2816e-02,  5.5804e-02,  2.5005e-02,\n",
       "         -5.6232e-02, -1.1710e-02,  6.3076e-02,  8.1451e-02,  4.6921e-02,\n",
       "          5.0364e-02,  2.7519e-02,  3.9678e-02, -2.9139e-02,  4.5319e-03,\n",
       "          7.1570e-02,  1.1600e-02, -1.7895e-02,  3.9097e-02,  2.3615e-02,\n",
       "          6.3167e-02, -1.8048e-02, -5.1270e-02, -4.7114e-02,  1.1700e-03,\n",
       "          2.4956e-02,  2.4915e-02, -4.2496e-02,  1.4371e-02, -2.3863e-02,\n",
       "         -1.3921e-02,  6.2002e-03, -4.9871e-02,  8.5341e-03, -9.0498e-03,\n",
       "         -5.0842e-03,  2.3609e-02, -1.7137e-03, -1.0973e-02, -2.7391e-02,\n",
       "         -9.8974e-03,  1.7229e-02,  2.8927e-03,  2.6274e-02,  3.0823e-02,\n",
       "         -2.2344e-02,  6.9805e-02,  6.5384e-03,  6.8871e-02,  2.1311e-02,\n",
       "         -3.1630e-02,  7.9980e-02, -1.4534e-02, -5.6552e-02,  2.6547e-02,\n",
       "         -9.3353e-02, -4.3351e-02,  3.2137e-02, -7.1037e-03, -2.2831e-02,\n",
       "         -2.3149e-02, -1.4739e-02, -7.6403e-03,  9.6428e-03, -3.2605e-02,\n",
       "          1.1913e-02,  4.6951e-02,  5.5349e-03,  7.5545e-03, -1.4340e-02,\n",
       "          7.1334e-03,  3.2830e-02,  5.1056e-02,  1.2069e-02, -6.6378e-03,\n",
       "          1.2001e-03, -3.0820e-02, -1.0853e-02,  8.2872e-03, -1.1427e-02,\n",
       "         -1.4566e-02, -3.2552e-02,  5.9310e-02,  1.3576e-02, -1.0850e-02,\n",
       "          3.6222e-02,  1.2845e-02,  1.3626e-02, -1.4999e-02,  1.3186e-02,\n",
       "          1.8358e-02, -1.3111e-02,  2.0459e-02, -1.6627e-03,  9.8149e-03,\n",
       "          6.2373e-02, -1.4655e-02,  5.7126e-03, -1.0899e-02,  2.8023e-02,\n",
       "          1.1634e-02,  8.9251e-03, -5.6664e-02,  1.6891e-02, -2.3373e-02,\n",
       "          1.7695e-02,  8.7523e-03, -1.2794e-03,  2.5734e-02,  2.3579e-02,\n",
       "         -4.6057e-02, -8.6023e-02,  2.8676e-02,  3.4778e-02, -3.4053e-02,\n",
       "         -6.2038e-02,  2.3414e-02, -8.5060e-03,  2.2408e-02, -4.1468e-02,\n",
       "          2.0613e-02, -2.6713e-02, -2.4420e-03,  5.0983e-02, -1.5779e-02,\n",
       "         -2.0608e-02, -7.5893e-03, -4.8825e-03,  5.0468e-02, -4.9427e-03,\n",
       "         -5.2177e-02,  6.6282e-03, -3.6438e-02,  6.4898e-03,  3.2347e-02,\n",
       "         -7.1644e-03,  2.4268e-02,  4.4628e-03,  1.4003e-01, -7.5899e-03,\n",
       "         -2.5600e-02,  3.9250e-03, -3.1191e-02,  4.9241e-02, -7.3608e-02,\n",
       "         -3.0082e-02, -1.4374e-02,  3.5504e-03, -3.1300e-02,  2.9915e-03,\n",
       "         -5.3057e-03,  3.1259e-02,  5.7755e-03,  2.8416e-02, -3.7509e-02,\n",
       "          4.9972e-02, -3.9290e-03, -3.2338e-02, -4.2031e-02,  6.6439e-03,\n",
       "          3.7278e-02,  2.4235e-02,  2.4604e-02, -3.1010e-03, -2.8399e-02,\n",
       "         -1.9092e-02, -1.3866e-02,  5.2855e-03,  2.9730e-02, -1.9126e-02,\n",
       "          4.6171e-02, -9.5937e-03, -2.1837e-03,  1.0330e-02, -2.0748e-02,\n",
       "          1.9897e-02, -3.0480e-02,  3.4352e-02,  3.8011e-02, -1.2118e-02,\n",
       "          1.4271e-01, -7.1961e-02,  2.6760e-02,  1.0617e-02,  1.8807e-02,\n",
       "          3.5654e-03,  1.9628e-04,  9.1901e-02,  1.5609e-02,  2.4812e-02,\n",
       "         -1.3405e-03, -5.0612e-03, -3.5549e-05, -2.8834e-03, -6.7749e-02,\n",
       "         -1.8519e-02,  3.0409e-02, -8.0799e-02, -7.1526e-03,  1.6934e-02,\n",
       "         -6.7833e-33, -5.8653e-02,  1.3414e-03,  1.8321e-02, -1.3777e-01,\n",
       "         -3.2625e-02,  2.5221e-02, -5.0154e-02, -1.5594e-02, -6.3608e-02,\n",
       "          6.9453e-03, -5.2893e-02,  1.6209e-02,  1.7500e-02,  2.9913e-02,\n",
       "          4.6405e-02, -2.2065e-02, -9.8282e-03, -6.1530e-03, -1.2231e-02,\n",
       "         -2.3599e-02,  1.7852e-03,  1.6492e-02, -2.0501e-02,  3.5626e-02,\n",
       "          6.0900e-03,  5.4909e-02, -9.0867e-02, -1.6777e-03,  2.7810e-02,\n",
       "         -5.0843e-02, -1.7896e-02,  2.6952e-03,  6.8984e-03, -1.5884e-02,\n",
       "         -2.9024e-02,  8.6661e-02,  4.8325e-03, -2.6327e-02, -2.8751e-02,\n",
       "         -1.9790e-02, -1.3455e-01,  4.8511e-02, -2.5628e-02,  3.9622e-03,\n",
       "          1.3513e-03, -4.0998e-03, -1.3602e-02,  1.3832e-03,  2.7591e-02,\n",
       "         -1.1064e-02,  9.3728e-03, -8.7105e-03, -1.6068e-02, -5.9922e-02,\n",
       "         -4.8539e-02, -3.4479e-02,  2.0899e-02,  4.1199e-02,  3.7573e-02,\n",
       "          5.6441e-02,  1.0144e-01,  1.1501e-02,  2.4076e-03,  1.0630e-01,\n",
       "          7.1278e-02,  5.9861e-03, -4.7735e-03,  4.3774e-02, -6.7876e-02,\n",
       "          3.4724e-04,  3.7568e-02, -6.7324e-03, -6.3954e-03, -8.5206e-03,\n",
       "          1.4078e-03,  3.1230e-04, -1.9776e-02, -2.7693e-02,  6.1076e-02,\n",
       "          2.9033e-02, -3.1662e-02, -9.2380e-03, -7.1829e-03, -2.8735e-03,\n",
       "         -1.7170e-02, -3.8620e-02,  1.5233e-02,  4.9464e-02,  1.4487e-02,\n",
       "          6.9716e-03, -3.7107e-02,  2.7275e-02, -4.0072e-02,  5.0630e-03,\n",
       "         -9.0368e-02,  2.6213e-02, -9.2112e-03, -2.0397e-02,  3.2329e-02,\n",
       "          1.5870e-02, -3.3782e-02,  2.1346e-03, -2.9410e-02,  1.3844e-02,\n",
       "         -6.6348e-03,  2.4623e-02, -1.5389e-03, -7.7182e-03, -1.8630e-02,\n",
       "         -2.4298e-02,  1.1656e-02,  1.2559e-02,  1.6360e-02, -3.5071e-02,\n",
       "          8.9561e-03,  5.9214e-03,  1.2369e-02,  1.0203e-01, -1.7915e-02,\n",
       "         -3.7078e-02,  5.2096e-02, -3.6384e-02, -5.2343e-02,  3.7363e-02,\n",
       "         -6.3356e-03, -3.0898e-03,  1.0128e-02, -1.2334e-01, -3.2817e-02,\n",
       "          4.7301e-04, -2.5890e-02,  3.6734e-02,  2.3646e-07,  4.9387e-02,\n",
       "         -1.1462e-02,  3.0344e-02,  3.3989e-02, -2.7492e-02,  9.3552e-03,\n",
       "          1.4742e-03,  1.7381e-02,  1.9740e-02,  1.7266e-03, -2.5942e-02,\n",
       "         -1.5728e-02,  5.9055e-03, -4.1892e-02,  1.3337e-02,  1.1057e-01,\n",
       "          3.0714e-02, -3.9497e-02, -1.7624e-02, -2.5711e-02,  4.2791e-02,\n",
       "         -1.3531e-02,  1.8305e-02,  4.0069e-02,  2.6922e-03,  8.3644e-02,\n",
       "         -3.7694e-02, -1.8140e-02,  4.3988e-02, -4.2972e-02, -1.2004e-01,\n",
       "          3.9033e-02, -5.2765e-03, -1.0566e-02,  1.0090e-02,  3.1792e-02,\n",
       "          5.1824e-02,  1.8787e-02,  4.0979e-02,  3.6154e-02, -3.8467e-02,\n",
       "         -1.3861e-02,  6.3531e-04, -9.2401e-03,  4.2990e-02, -2.1141e-02,\n",
       "          8.4661e-03, -4.0343e-02, -2.0259e-02, -2.0130e-02, -1.5693e-02,\n",
       "          2.1164e-02,  1.1815e-02, -2.6185e-02, -1.4835e-02, -2.4472e-03,\n",
       "          3.6409e-02, -2.7575e-02, -1.5495e-02,  1.6636e-02,  2.7022e-03,\n",
       "          3.8045e-02,  1.7192e-02,  4.0091e-02, -3.1975e-02,  4.0068e-02,\n",
       "          1.7809e-02,  1.0925e-34, -7.7490e-04,  1.3340e-02, -2.8269e-02,\n",
       "         -4.6941e-02, -1.7544e-02,  2.1666e-02, -5.8457e-02, -4.1474e-02,\n",
       "          2.0868e-02,  1.5010e-02, -2.9637e-02], device='cuda:0')}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_and_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f3c1f030-b6bb-4bd4-9fe6-306d0fd919e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e9f00b0411146ab90fa8fbc73547258",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1765 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 20s, sys: 4.67 s, total: 4min 25s\n",
      "Wall time: 53.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for item in tqdm(pages_and_chunks_over_min_token_len):\n",
    "    item[\"embedding\"] = embedding_model.encode(item[\"sentence_chunk\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff771519-be97-4c74-b7b2-7266c99518ca",
   "metadata": {},
   "source": [
    "# TODO\n",
    "## Creating a chromadb client for storing embeddings\n",
    "\n",
    "> **NOTE** maybe not\n",
    ">\n",
    "> [INFO] Time taken to get scores on 1765000 embeddings: 0.00286 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0cf22a6f-0c18-4e74-9260-e4344a6e1bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "chroma_client = chromadb.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "14f45adc-b1fc-4346-949e-de2800e4b845",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = chroma_client.create_collection(name=\"embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ba664f6f-cd21-412c-ae83-8611e951ee8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a52e39-3887-433e-a153-3a8d54a8e893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = collection.query(\n",
    "#     query_texts=[\"This is a query document about hawaii\"], # Chroma will embed this for you\n",
    "#     n_results=2 # how many results to return\n",
    "# )\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61e227f-b4c0-4bcc-81b0-b01891b9ebf7",
   "metadata": {},
   "source": [
    "## adding the embeddings to the chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b0021cc9-4e03-4dd1-8e22-813b4a5d31e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 661 μs, sys: 19 μs, total: 680 μs\n",
      "Wall time: 690 μs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['The CNN applies an overall stride of 32 to theinput image (i.e., if you add up all the strides greater than 1), meaning the lastlayer outputs feature maps that are 32 times smaller than the input image. This is clearly too coarse, so they added a single upsampling layer thatmultiplies the resolution by 32. There are several solutions available for upsampling (increasing the size of an',\n",
       " 'tf.keras.layers. TextVectorization, Text Preprocessing-TextPreprocessing, Creating the Training Dataset, Building and Training theChar-RNN Model, Sentiment Analysis, Sentiment Analysis-AnEncoder–Decoder Network for Neural Machine Translationtf.keras.layers. TimeDistributed, Forecasting Using a Sequence-to-Sequence Modeltf.keras.losses. Huber, Custom Loss Functionstf.keras.losses.kullback_leibler_divergence(), Sparse Autoencoderstf.keras.losses. Loss, Saving and Loading Models That Contain CustomComponentstf.keras.losses.sparse_categorical_crossentropy(), Compiling the model,CNN Architectures, Building and Training the Char-RNN Model, AnEncoder–Decoder Network for Neural Machine Translation, HuggingFace’s Transformers Librarytf.keras.metrics. MeanIoU, Classification and Localizationtf.keras.metrics. Metric, Custom Metricstf.keras.metrics. Precision, Custom Metricstf.keras. Model, Building Complex Models Using the Functional APItf.keras.models.clone_model(), Using the Subclassing API to BuildDynamic Models, Transfer Learning with Kerastf.keras.models.load_model(), Saving and Restoring a Model, Savingand Loading Models That Contain Custom Components-Saving andLoading Models That Contain Custom Components, Custom Models,Training at Scale Using the Distribution Strategies APItf.keras.optimizers. Adam, Building a Regression MLP Using theSequential API, AdamW',\n",
       " 'All the multilayer neural networks we’ve looked at so far had layers composed of a longline of neurons, and we had to flatten input images to 1D before feeding them to the neuralnetwork. In a CNN each layer is represented in 2D, which makes it easier to matchneurons with their corresponding inputs. A neuron located in row i, column j of a given layer is connected to theoutputs of the neurons in the previous layer located in rows i to i + f – 1,columns j to j + f – 1, where f and f are the height and width of thereceptive field (see Figure 14-3). In order for a layer to have the same heightand width as the previous layer, it is common to add zeros around the inputs,as shown in the diagram. This is called zero padding. It is also possible to connect a large input layer to a much smaller layer byspacing out the receptive fields, as shown in Figure 14-4. This dramaticallyreduces the model’s computational complexity. The horizontal or verticalstep size from one receptive field to the next is called the stride. In thediagram, a 5 × 7 input layer (plus zero padding) is connected to a 3 × 4 layer,using 3 × 3 receptive fields and a stride of 2 (in this example the stride is thesame in both directions, but it does not have to be so). A neuron located inrow i, column j in the upper layer is connected to the outputs of the neuronsin the previous layer located in rows i × s to i × s + f – 1, columns j × s toj × s + f – 1, where s and s are the vertical and horizontal strides.hwhwhhhwwwhw']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "text_chunks = [item[\"sentence_chunk\"] for item in pages_and_chunks_over_min_token_len]\n",
    "random.sample(text_chunks, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f58305b3-6a9d-4679-975b-44283b91acfa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text_chunks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Embed all the text in batches\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m text_chunks_embeddings \u001b[38;5;241m=\u001b[39m embedding_model\u001b[38;5;241m.\u001b[39mencode(\u001b[43mtext_chunks\u001b[49m,\n\u001b[1;32m      4\u001b[0m                                                batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m      5\u001b[0m                                                convert_to_tensor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m text_chunks_embeddings\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text_chunks' is not defined"
     ]
    }
   ],
   "source": [
    "# Embed all the text in batches\n",
    "\n",
    "text_chunks_embeddings = embedding_model.encode(text_chunks,\n",
    "                                               batch_size=32,\n",
    "                                               convert_to_tensor=True)\n",
    "text_chunks_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9e7a7b-d735-451a-ada4-0628818f5591",
   "metadata": {},
   "source": [
    "### Saving embeddings into a file (Temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d26e6a02-5774-44c9-8974-8f728738aa70",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pages_and_chunks_over_min_token_len' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m text_chunks_and_embeddings_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mpages_and_chunks_over_min_token_len\u001b[49m)\n\u001b[1;32m      2\u001b[0m embeddings_df_save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpdf_file_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m text_chunks_and_embeddings_df\u001b[38;5;241m.\u001b[39mto_csv(embeddings_df_save_path, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pages_and_chunks_over_min_token_len' is not defined"
     ]
    }
   ],
   "source": [
    "text_chunks_and_embeddings_df = pd.DataFrame(pages_and_chunks_over_min_token_len)\n",
    "embeddings_df_save_path = f\"embedding/{pdf_file_name}.csv\"\n",
    "text_chunks_and_embeddings_df.to_csv(embeddings_df_save_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "54939d30-c850-421b-a354-9e8d941ba877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>sentence_chunk</th>\n",
       "      <th>chunk_char_count</th>\n",
       "      <th>chunk_word_count</th>\n",
       "      <th>chunk_token_count</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Hands-On Machine Learning withScikit-Learn, Ke...</td>\n",
       "      <td>150</td>\n",
       "      <td>14</td>\n",
       "      <td>37.50</td>\n",
       "      <td>[-2.78972145e-02 -1.74421235e-03 -4.81583700e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Hands-On Machine Learning with Scikit-Learn, K...</td>\n",
       "      <td>873</td>\n",
       "      <td>97</td>\n",
       "      <td>218.25</td>\n",
       "      <td>[ 2.48818621e-02  6.14474006e-02 -5.22879586e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Revision History for the Third Edition2022-10-...</td>\n",
       "      <td>1152</td>\n",
       "      <td>159</td>\n",
       "      <td>288.00</td>\n",
       "      <td>[ 9.69696417e-03 -2.23619733e-02 -4.10474986e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>The Machine Learning TsunamiIn 2006, Geoffrey ...</td>\n",
       "      <td>1173</td>\n",
       "      <td>171</td>\n",
       "      <td>293.25</td>\n",
       "      <td>[-3.19831981e-03  1.02457978e-01 -5.40223382e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>Machine Learning in Your ProjectsSo, naturally...</td>\n",
       "      <td>867</td>\n",
       "      <td>131</td>\n",
       "      <td>216.75</td>\n",
       "      <td>[ 4.60035866e-03  9.44859162e-02 -7.24032298e-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   page_number                                     sentence_chunk  \\\n",
       "0            2  Hands-On Machine Learning withScikit-Learn, Ke...   \n",
       "1            3  Hands-On Machine Learning with Scikit-Learn, K...   \n",
       "2            5  Revision History for the Third Edition2022-10-...   \n",
       "3            7  The Machine Learning TsunamiIn 2006, Geoffrey ...   \n",
       "4            8  Machine Learning in Your ProjectsSo, naturally...   \n",
       "\n",
       "   chunk_char_count  chunk_word_count  chunk_token_count  \\\n",
       "0               150                14              37.50   \n",
       "1               873                97             218.25   \n",
       "2              1152               159             288.00   \n",
       "3              1173               171             293.25   \n",
       "4               867               131             216.75   \n",
       "\n",
       "                                           embedding  \n",
       "0  [-2.78972145e-02 -1.74421235e-03 -4.81583700e-...  \n",
       "1  [ 2.48818621e-02  6.14474006e-02 -5.22879586e-...  \n",
       "2  [ 9.69696417e-03 -2.23619733e-02 -4.10474986e-...  \n",
       "3  [-3.19831981e-03  1.02457978e-01 -5.40223382e-...  \n",
       "4  [ 4.60035866e-03  9.44859162e-02 -7.24032298e-...  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading the csv \n",
    "\n",
    "text_chunks_and_embedding_df_load = pd.read_csv(embeddings_df_save_path)\n",
    "text_chunks_and_embedding_df_load.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5ab7f457-5ef5-4fb0-bb0c-477467a8622e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Similarity FeaturesAnother technique to tackle nonlinear problems is to add features computedusing a similarity function, which measures how much each instanceresembles a particular landmark, as we did in Chapter 2 when we added thegeographic similarity features. For example, let’s take the 1D dataset fromearlier and add two landmarks to it at x = –2 and x = 1 (see the left plot inFigure 5-8). Next, we’ll define the similarity function to be the Gaussian RBFwith γ = 0.3. This is a bell-shaped function varying from 0 (very far awayfrom the landmark) to 1 (at the landmark). Now we are ready to compute the new features. For example, let’s look at theinstance x = –1: it is located at a distance of 1 from the first landmark and 2from the second landmark. Therefore, its new features are x = exp(–0.3 × 1 )≈ 0.74 and x = exp(–0.3 × 2 ) ≈ 0.30. The plot on the right in Figure 5-8shows the transformed dataset (dropping the original features). As you cansee, it is now linearly separable. Figure 5-8. Similarity features using the Gaussian RBFYou may wonder how to select the landmarks. The simplest approach is tocreate a landmark at the location of each and every instance in the dataset.'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_chunks_and_embedding_df_load[\"sentence_chunk\"].iloc[357]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a4e945-7032-4e92-acdc-443cd002cc64",
   "metadata": {},
   "source": [
    "# Rag - Search and Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1133ffac-cc97-497f-970a-9672c723d6a1",
   "metadata": {},
   "source": [
    "### Similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fb65de4-2d15-46d3-bb2b-c2ba49837354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[INFO]\u001b[0m This is an info\n",
      "\u001b[31m[ERROR]\u001b[0m This is an error\n"
     ]
    }
   ],
   "source": [
    "from colorama import Fore, Style\n",
    "\n",
    "def print_message(message_type, message):\n",
    "    if message_type == \"INFO\":\n",
    "        print(f\"{Fore.YELLOW}[INFO]{Style.RESET_ALL} {message}\")\n",
    "    elif message_type == \"ERROR\":\n",
    "        print(f\"{Fore.RED}[ERROR]{Style.RESET_ALL} {message}\")\n",
    "    elif message_type == \"SUCCESS\":\n",
    "        print(f\"{Fore.GREEN}[SUCESS]{Style.RESET_ALL} {message}\")\n",
    "    else:\n",
    "        print(f\"{message}\")\n",
    "\n",
    "print_message(\"INFO\", \"This is an info\")\n",
    "print_message(\"ERROR\", \"This is an error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c109a23-1d71-428b-a1c6-2511acfb5a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac7b5cf3-c3e6-447a-97c8-32b388310c40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1765, 768])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Import texts and embedding df\n",
    "text_chunks_and_embedding_df = pd.read_csv(\"embeddings/Hands-On Machine Learning With - Aurelien Geron.pdf.csv\")\n",
    "\n",
    "# Convert embedding column back to np.array (it got converted to string when it got saved to CSV)\n",
    "text_chunks_and_embedding_df[\"embedding\"] = text_chunks_and_embedding_df[\"embedding\"].apply(lambda x: np.fromstring(x.strip(\"[]\"), sep=\" \"))\n",
    "\n",
    "# Convert texts and embedding df to list of dicts\n",
    "pages_and_chunks = text_chunks_and_embedding_df.to_dict(orient=\"records\")\n",
    "\n",
    "# Convert embeddings to torch tensor and send to device (note: NumPy arrays are float64, torch tensors are float32 by default)\n",
    "embeddings = torch.tensor(np.array(text_chunks_and_embedding_df[\"embedding\"].tolist()), dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b4ec8893-78cc-4a19-804e-730fdbfb9c08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0279, -0.0017, -0.0482,  ...,  0.0007, -0.0182, -0.0057],\n",
       "        [ 0.0249,  0.0614, -0.0523,  ...,  0.0169, -0.0166, -0.0051],\n",
       "        [ 0.0097, -0.0224, -0.0410,  ..., -0.0104, -0.0108, -0.0204],\n",
       "        ...,\n",
       "        [ 0.0017,  0.0756, -0.0428,  ...,  0.0299,  0.0373, -0.0241],\n",
       "        [-0.0071,  0.0351, -0.0043,  ...,  0.0399,  0.0262, -0.0321],\n",
       "        [ 0.0198,  0.0750, -0.0211,  ...,  0.0131, -0.0042, -0.0147]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c6f42b87-e5b9-4f83-bf46-eaca565e74db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creeating the model, this is just used if you havent already run the model above\n",
    "\n",
    "from sentence_transformers import util, SentenceTransformer\n",
    "\n",
    "\n",
    "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\", \n",
    "                                      device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f368d299-0b1f-4beb-a85b-0d6159c5a89d",
   "metadata": {},
   "source": [
    "## semantic search pipeline\n",
    "\n",
    "\n",
    "1. Define a query string.\n",
    "2. Turn the query string into an embedding\n",
    "3. Perform a dot product or cosine similarity function between the text embedding and the query embedding\n",
    "4. Sort the results from k in descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "166a5ed3-c7df-4fe5-8393-7db8586e8ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How does ridge regression work\n",
      "\u001b[33m[INFO]\u001b[0m Time taken to get scores on 1765 embeddings: 0.00126 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([0.7266, 0.6629, 0.6171, 0.5913, 0.5637, 0.5470, 0.5306, 0.5169, 0.5080,\n",
       "        0.4852, 0.4812, 0.4664, 0.4642, 0.4626, 0.4572, 0.4444, 0.4411, 0.4357,\n",
       "        0.4351, 0.4293, 0.4284, 0.4215, 0.4130, 0.4052, 0.4026, 0.4011, 0.3999,\n",
       "        0.3991, 0.3976, 0.3953, 0.3945, 0.3916, 0.3914, 0.3912, 0.3893, 0.3891,\n",
       "        0.3877, 0.3876, 0.3873, 0.3873, 0.3832, 0.3814, 0.3803, 0.3780, 0.3771,\n",
       "        0.3734, 0.3728, 0.3719, 0.3711, 0.3702, 0.3700, 0.3700, 0.3695, 0.3686,\n",
       "        0.3674, 0.3654, 0.3653, 0.3648, 0.3636, 0.3624, 0.3617, 0.3608, 0.3596,\n",
       "        0.3593, 0.3591, 0.3585, 0.3570, 0.3564, 0.3559, 0.3533, 0.3500, 0.3496,\n",
       "        0.3480, 0.3477, 0.3475, 0.3473, 0.3463, 0.3439, 0.3434, 0.3427, 0.3419,\n",
       "        0.3412, 0.3410, 0.3401, 0.3391, 0.3384, 0.3379, 0.3376, 0.3369, 0.3364,\n",
       "        0.3363, 0.3327, 0.3325, 0.3321, 0.3320, 0.3314, 0.3298, 0.3297, 0.3293,\n",
       "        0.3283], device='cuda:0'),\n",
       "indices=tensor([ 309,  311,  312,  315,  310,  313,   62,  341,  272,  314, 1694,  284,\n",
       "         495,   77,  368,  281,  337,  308,  270,  271,  380,  285,  317,  377,\n",
       "         274,  287,  330,  278,  789,  297, 1729,  752,  286,  345,  404, 1721,\n",
       "         299,  353,  376,  302,   39,  339,  280,  405,  145,  269,   53, 1272,\n",
       "          27,  279,  340,  356,  364,  450,  332,  395,  282,  366, 1693,  370,\n",
       "         288,  466,  593,  553,   91,  605,  318,  276,  601,  439,  467, 1753,\n",
       "         496,   79, 1307,  436,   75,  357,  591,  494,  600,  275, 1691,  441,\n",
       "         344,  301,  104,  636,  295,  335,  147,  379,  338,  373,  360,  306,\n",
       "         355,  618,  343, 1728], device='cuda:0'))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"How does ridge regression work\"\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "# embed query\n",
    "query_embedding = embedding_model.encode(query, convert_to_tensor=True)\n",
    "\n",
    "\n",
    "# Get similarity scores with dot product (use cosine similarity if outputs are not normalized)\n",
    "\n",
    "from time import perf_counter as timer\n",
    "\n",
    "start_time = timer()\n",
    "dot_scores = util.dot_score(a=query_embedding, b=embeddings)[0]\n",
    "end_time = timer()\n",
    "\n",
    "print_message(\"INFO\", f\"Time taken to get scores on {len(embeddings)} embeddings: {end_time - start_time:.5f} seconds.\")\n",
    "\n",
    "# 4 get top-k results\n",
    "\n",
    "top_results_dot_product = torch.topk(dot_scores, k=100)\n",
    "top_results_dot_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9875324b-12fa-4855-a6a0-4dac4ed793a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ridge RegressionRidge regression (also called Tikhonov regularization) is a regularizedversion of linear regression: a regularization term equal to αm∑i=1nθi2 isadded to the MSE. This forces the learning algorithm to not only fit the databut also keep the model weights as small as possible. Note that theregularization term should only be added to the cost function during training. Once the model is trained, you want to use the unregularized MSE (or theRMSE) to evaluate the model’s performance. The hyperparameter α controls how much you want to regularize the model. If α = 0, then ridge regression is just linear regression. If α is very large, thenall weights end up very close to zero and the result is a flat line going throughthe data’s mean. Equation 4-8 presents the ridge regression cost function.\\u2060Equation 4-8. Ridge regression cost functionJ(θ)=MSE(θ)+αm∑i=1nθi2Note that the bias term θ is not regularized (the sum starts at i = 1, not 0). Ifwe define w as the vector of feature weights (θ to θ ), then the regularizationterm is equal to α(∥ w ∥) / m, where ∥ w ∥ represents the ℓ norm ofthe weight vector.\\u2060 For batch gradient descent, just add 2αw / m to the partof the MSE gradient vector that corresponds to the feature weights, withoutadding anything to the gradient of the bias term (see Equation 4-6). WARNINGIt is important to scale the data (e.g., using a StandardScaler) before performing ridgeregression, as it is sensitive to the scale of the input features. This is true of mostregularized models.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_chunks_and_embedding_df[\"sentence_chunk\"].iloc[309]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ed20aac0-6e8f-4683-8822-b8720e527fad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "just dot prod 0.7266\n",
      "cosine similarity 0.7266\n"
     ]
    }
   ],
   "source": [
    "dot = torch.dot(embeddings[309], query_embedding)\n",
    "print(f\"just dot prod {dot:.4f}\")\n",
    "dot = dot / (torch.sqrt(torch.sum(embeddings[309] ** 2)) *  torch.sqrt(torch.sum(query_embedding ** 2))) \n",
    "print(f\"cosine similarity {dot:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dd742427-23e9-4ec4-9494-0af6e9b5ae42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000, device='cuda:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " (torch.sqrt(torch.sum(embeddings[309] ** 2)) *  torch.sqrt(torch.sum(query_embedding ** 2))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9463df84-6bdf-4124-8dec-27fdc77c7e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape torch.Size([1765000, 768])\n"
     ]
    }
   ],
   "source": [
    "larger_embeddings = torch.rand(1000*embeddings.shape[0], 768).to(device)\n",
    "print(f'Embeddings shape {larger_embeddings.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec32072e-65b7-4333-b546-6e4ff9d9759c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[INFO]\u001b[0m Time taken to get scores on 1765000 embeddings: 0.00286 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([1.5886, 1.5866, 1.5200, 1.5178, 1.5173, 1.5087, 1.4967, 1.4873, 1.4778,\n",
       "        1.4660, 1.4611, 1.4571, 1.4494, 1.4467, 1.4458, 1.4402, 1.4304, 1.4287,\n",
       "        1.4246, 1.4229, 1.4207, 1.4195, 1.4176, 1.4146, 1.4123, 1.4107, 1.4094,\n",
       "        1.4039, 1.4033, 1.4002, 1.3973, 1.3921, 1.3899, 1.3880, 1.3832, 1.3825,\n",
       "        1.3814, 1.3801, 1.3792, 1.3791, 1.3785, 1.3779, 1.3764, 1.3748, 1.3731,\n",
       "        1.3728, 1.3727, 1.3723, 1.3668, 1.3668, 1.3634, 1.3628, 1.3623, 1.3604,\n",
       "        1.3603, 1.3602, 1.3599, 1.3587, 1.3586, 1.3580, 1.3570, 1.3570, 1.3568,\n",
       "        1.3568, 1.3565, 1.3562, 1.3554, 1.3546, 1.3544, 1.3494, 1.3489, 1.3487,\n",
       "        1.3463, 1.3428, 1.3414, 1.3387, 1.3382, 1.3376, 1.3355, 1.3342, 1.3330,\n",
       "        1.3330, 1.3329, 1.3314, 1.3294, 1.3279, 1.3264, 1.3262, 1.3236, 1.3231,\n",
       "        1.3214, 1.3192, 1.3174, 1.3167, 1.3154, 1.3141, 1.3140, 1.3137, 1.3130,\n",
       "        1.3127], device='cuda:0'),\n",
       "indices=tensor([ 721033,  317068,  700443, 1130865, 1186948, 1360673, 1727258, 1124865,\n",
       "         534869, 1497477, 1029902, 1373345, 1439925,  444199, 1299250,  546276,\n",
       "         657906,  123587, 1562289,  170280,  405047,   22487,  663014,   90366,\n",
       "        1031829,  516493, 1285134, 1245268, 1758254,  114921, 1673239, 1369935,\n",
       "         486510,  461835,  418618,  933310,  941736, 1270384, 1472048, 1284426,\n",
       "        1146418, 1161902, 1682953, 1116955,  134418, 1034216, 1198416, 1024963,\n",
       "        1120876, 1616815,  248175, 1043158, 1124131, 1236892, 1634856,  231763,\n",
       "         152524,  518393,  430397, 1531492,  400312,  885911,   74235, 1229747,\n",
       "        1594692,  725692, 1563634, 1169925,  441604,  303821,  592916, 1303342,\n",
       "          89539,  715391,  584518,  434111, 1738048,  193581,  873516, 1391704,\n",
       "        1459553,   45523,  595891, 1299913,  878081, 1487730,  993679, 1330676,\n",
       "        1167358, 1337677, 1569999,  617017, 1650760,  544625, 1497255, 1582686,\n",
       "         924374, 1606489, 1374096,  393325], device='cuda:0'))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = timer()\n",
    "dot_scores = util.dot_score(a=query_embedding, b=larger_embeddings)[0]\n",
    "end_time = timer()\n",
    "# print(dot_scores.shape)\n",
    "\n",
    "print_message(\"INFO\", f\"Time taken to get scores on {len(larger_embeddings)} embeddings: {end_time - start_time:.5f} seconds.\")\n",
    "\n",
    "\n",
    "top_results_dot_product = torch.topk(dot_scores, k=100)\n",
    "top_results_dot_product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c99967-d9e0-442d-a518-66a0ce2fb802",
   "metadata": {},
   "source": [
    "### TODO Implementing a Re-Ranker\n",
    "\n",
    "- Re-rank the top k=100 results\n",
    "- Select the top=5 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "60e3a718-a597-4d35-838b-2660cfcf7f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_results_dot_product[1]\n",
    "top_k_chunks = [text_chunks_and_embedding_df[\"sentence_chunk\"].iloc[int(i)] for i in top_results_dot_product[1]]\n",
    "# top_k_chunks = [i for i in top_results_dot_product[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e5d518dd-efd2-49ef-b997-bfa72365ba26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ridge RegressionRidge regression (also called Tikhonov regularization) is a regularizedversion of linear regression: a regularization term equal to αm∑i=1nθi2 isadded to the MSE. This forces the learning algorithm to not only fit the databut also keep the model weights as small as possible. Note that theregularization term should only be added to the cost function during training. Once the model is trained, you want to use the unregularized MSE (or theRMSE) to evaluate the model’s performance. The hyperparameter α controls how much you want to regularize the model. If α = 0, then ridge regression is just linear regression. If α is very large, thenall weights end up very close to zero and the result is a flat line going throughthe data’s mean. Equation 4-8 presents the ridge regression cost function.\\u2060Equation 4-8. Ridge regression cost functionJ(θ)=MSE(θ)+αm∑i=1nθi2Note that the bias term θ is not regularized (the sum starts at i = 1, not 0). Ifwe define w as the vector of feature weights (θ to θ ), then the regularizationterm is equal to α(∥ w ∥) / m, where ∥ w ∥ represents the ℓ norm ofthe weight vector.\\u2060 For batch gradient descent, just add 2αw / m to the partof the MSE gradient vector that corresponds to the feature weights, withoutadding anything to the gradient of the bias term (see Equation 4-6). WARNINGIt is important to scale the data (e.g., using a StandardScaler) before performing ridgeregression, as it is sensitive to the scale of the input features. This is true of mostregularized models.',\n",
       " 'PolynomialFeatures(degree=10), then it is scaled using a StandardScaler, andfinally the ridge models are applied to the resulting features: this ispolynomial regression with ridge regularization. Note how increasing α leadsto flatter (i.e., less extreme, more reasonable) predictions, thus reducing themodel’s variance but increasing its bias. Figure 4-17. Linear (left) and a polynomial (right) models, both with various levels of ridgeregularizationAs with linear regression, we can perform ridge regression either bycomputing a closed-form equation or by performing gradient descent. Thepros and cons are the same. Equation 4-9 shows the closed-form solution,where A is the (n + 1) × (n + 1) identity matrix,\\u2060 except with a 0 in the top-left cell, corresponding to the bias term. Equation 4-9. Ridge regression closed-form solutionθ ^ = (X ⊺ X+αA) -1  X ⊺  yHere is how to perform ridge regression with Scikit-Learn using a closed-form solution (a variant of Equation 4-9 that uses a matrix factorizationtechnique by André-Louis Cholesky):>>> from sklearn.linear_model import Ridge>>> ridge_reg = Ridge(alpha=0.1, solver=\"cholesky\")>>> ridge_reg.fit(X, y)>>> ridge_reg.predict([[1.5]])array([[1.55325833]])910',\n",
       " 'And using stochastic gradient descent:\\u2060>>> sgd_reg = SGDRegressor(penalty=\"l2\", alpha=0.1 / m, tol=None,...            max_iter=1000, eta0=0.01, random_state=42)...>>> sgd_reg.fit(X, y.ravel()) # y.ravel() because fit() expects 1D targets>>> sgd_reg.predict([[1.5]])array([1.55302613])The penalty hyperparameter sets the type of regularization term to use. Specifying \"l2\" indicates that you want SGD to add a regularization term tothe MSE cost function equal to alpha times the square of the ℓ norm of theweight vector. This is just like ridge regression, except there’s no division bym in this case; that’s why we passed alpha=0.1 / m, to get the same result asRidge(alpha=0.1). TIPThe RidgeCV class also performs ridge regression, but it automatically tuneshyperparameters using cross-validation. It’s roughly equivalent to using GridSearchCV,but it’s optimized for ridge regression and runs much faster. Several other estimators(mostly linear) also have efficient CV variants, such as LassoCV and ElasticNetCV.102',\n",
       " 'Figure 4-19. Lasso versus ridge regularizationThe two bottom plots show the same thing but with an ℓ penalty instead. Inthe bottom-left plot, you can see that the ℓ loss decreases as we get closer tothe origin, so gradient descent just takes a straight path toward that point. Inthe bottom-right plot, the contours represent ridge regression’s cost function(i.e.,an MSE cost function plus an ℓ loss). As you can see, the gradients getsmaller as the parameters approach the global optimum, so gradient descentnaturally slows down. This limits the bouncing around, which helps ridgeconverge faster than lasso regression. Also note that the optimal parameters(represented by the red square) get closer and closer to the origin when youincrease α, but they never get eliminated entirely. TIPTo keep gradient descent from bouncing around the optimum at the end when using lasso222',\n",
       " 'This is true of mostregularized models. Figure 4-17 shows several ridge models that were trained on some very noisylinear data using different α values. On the left, plain ridge models are used,leading to linear predictions. On the right, the data is first expanded using701n2 2228',\n",
       " 'Lasso RegressionLeast absolute shrinkage and selection operator regression (usually simplycalled lasso regression) is another regularized version of linear regression:just like ridge regression, it adds a regularization term to the cost function,but it uses the ℓ norm of the weight vector instead of the square of the ℓnorm (see Equation 4-10). Notice that the ℓ norm is multiplied by 2α,whereas the ℓ norm was multiplied by α / m in ridge regression. Thesefactors were chosen to ensure that the optimal α value is independent fromthe training set size: different norms lead to different factors (see Scikit-Learnissue #15657 for more details). Equation 4-10. Lasso regression cost functionJ(θ)=MSE(θ)+2α∑i=1nθiFigure 4-18 shows the same thing as Figure 4-17 but replaces the ridgemodels with lasso models and uses different α values. Figure 4-18. Linear (left) and polynomial (right) models, both using various levels of lassoregularizationAn important characteristic of lasso regression is that it tends to eliminate theweights of the least important features (i.e., set them to zero). For example,the dashed line in the righthand plot in Figure 4-18 (with α = 0.01) looksroughly cubic: all the weights for the high-degree polynomial features areequal to zero. In other words, lasso regression automatically performs feature1212',\n",
       " 'examples; the objective is to minimize this distance. This is where the linear regression algorithm comes in: you feed it yourtraining examples, and it finds the parameters that make the linear model fitbest to your data. This is called training the model. In our case, the algorithmfinds that the optimal parameter values are θ = 3.75 and θ = 6.78 × 10 . WARNINGConfusingly, the word “model” can refer to a type of model (e.g., linear regression), to afully specified model architecture (e.g., linear regression with one input and one output),or to the final trained model ready to be used for predictions (e.g., linear regression withone input and one output, using θ = 3.75 and θ = 6.78 × 10 ). Model selection consistsin choosing the type of model and fully specifying its architecture. Training a modelmeans running an algorithm to find the model parameters that will make it best fit thetraining data, and hopefully make good predictions on new data. Now the model fits the training data as closely as possible (for a linearmodel), as you can see in Figure 1-20. Figure 1-20. The linear model that fits the training data bestYou are finally ready to run the model to make predictions. For example, say01–501–5',\n",
       " 'a. Ridge regression instead of plain linear regression (i.e., without anyregularization)?b. Lasso instead of ridge regression?c. Elastic net instead of lasso regression?11. Suppose you want to classify pictures as outdoor/indoor anddaytime/nighttime. Should you implement two logistic regressionclassifiers or one softmax regression classifier?12. Implement batch gradient descent with early stopping for softmaxregression without using Scikit-Learn, only NumPy. Use it on aclassification task such as the iris dataset. Solutions to these exercises are available at the end of this chapter’snotebook, at https://homl.info/colab3.1 A closed-form equation is only composed of a finite number of constants, variables, andstandard operations: for example, a = sin(b – c). No infinite sums, no limits, no integrals, etc.2 Technically speaking, its derivative is Lipschitz continuous.3 Since feature 1 is smaller, it takes a larger change in θ to affect the cost function, which is whythe bowl is elongated along the θ axis.4 Eta (η) is the seventh letter of the Greek alphabet.5 While the Normal equation can only perform linear regression, the gradient descent algorithmscan be used to train many other models, as you’ll see.6 This notion of bias is not to be confused with the bias term of linear models.7 It is common to use the notation J(θ) for cost functions that don’t have a short name; I’ll oftenuse this notation throughout the rest of this book. The context will make it clear which costfunction is being discussed.8 Norms are discussed in Chapter 2.9 A square matrix full of 0s except for 1s on the main diagonal (top left to bottom right).10 Alternatively, you can use the Ridge class with the \"sag\" solver. Stochastic average GD is avariant of stochastic GD.',\n",
       " 'x is the instance’s feature vector, containing x to x , with x alwaysequal to 1.θ · x is the dot product of the vectors θ and x, which is equal to θ x +θ x + θ x + ... + θ x . NOTEIn machine learning, vectors are often represented as column vectors, which are 2D arrayswith a single column. If θ and x are column vectors, then the prediction is y^=θ⊺x, whereθ⊺ is the transpose of θ (a row vector instead of a column vector) and θ⊺x is the matrixmultiplication of θ⊺ and x. It is of course the same prediction, except that it is nowrepresented as a single-cell matrix rather than a scalar value. In this book I will use thisnotation to avoid switching between dot products and matrix multiplications. OK, that’s the linear regression model—but how do we train it?Well, recallthat training a model means setting its parameters so that the model best fitsthe training set. For this purpose, we first need a measure of how well (orpoorly) the model fits the training data. In Chapter 2 we saw that the mostcommon performance measure of a regression model is the root mean squareerror (Equation 2-1). Therefore, to train a linear regression model, we need tofind the value of θ that minimizes the RMSE. In practice, it is simpler tominimize the mean squared error (MSE) than the RMSE, and it leads to thesame result (because the value that minimizes a positive function alsominimizes its square root). WARNINGLearning algorithms will often optimize a different loss function during training than theperformance measure used to evaluate the final model. This is generally because thefunction is easier to optimize and/or because it has extra terms needed during training only(e.g.,for regularization).',\n",
       " 'selection and outputs a sparse model with few nonzero feature weights. You can get a sense of why this is the case by looking at Figure 4-19: theaxes represent two model parameters, and the background contours representdifferent loss functions. In the top-left plot, the contours represent the ℓ loss(|θ | + |θ |), which drops linearly as you get closer to any axis. For example, ifyou initialize the model parameters to θ = 2 and θ = 0.5, running gradientdescent will decrement both parameters equally (as represented by the dashedyellow line); therefore θ will reach 0 first (since it was closer to 0 to beginwith). After that, gradient descent will roll down the gutter until it reaches θ= 0 (with a bit of bouncing around, since the gradients of ℓ never get close to0: they are either –1 or 1 for each parameter). In the top-right plot, thecontours represent lasso regression’s cost function (i.e., an MSE cost functionplus an ℓ loss). The small white circles show the path that gradient descenttakes to optimize some model parameters that were initialized around θ =0.25 and θ = –1: notice once again how the path quickly reaches θ = 0, thenrolls down the gutter and ends up bouncing around the global optimum(represented by the red square). If we increased α, the global optimum wouldmove left along the dashed yellow line, while if we decreased α, the globaloptimum would move right (in this example, the optimal parameters for theunregularized MSE are θ = 2 and θ = 0.5).11212211112212',\n",
       " 'gradient descent in, Gradient Descent-Mini-Batch Gradient Descentlearning curves in, Learning Curves-Learning CurvesNormal equation, The Normal Equation-The Normal Equationregularizing models (see regularization)ridge regression, Ridge Regression-Ridge Regression, Elastic NetRegressiontraining set evaluation, Train and Evaluate on the Training Setusing stochastic gradient descent, Stochastic Gradient Descentlinear SVM classification, Linear SVM Classification-Soft MarginClassification, Under the Hood of Linear SVM Classifiers-Kernelized SVMslinear threshold units (LTUs), The Perceptronlinearly separable, SVM classes, Linear SVM Classification-Linear SVMClassification, Similarity FeaturesLinearRegression, Clean the Data, Feature Scaling and Transformation, Trainand Evaluate on the Training Set, The Normal Equation, ComputationalComplexity, Polynomial RegressionLinearSVC, Soft Margin Classification, SVM Classes and ComputationalComplexity, Under the Hood of Linear SVM ClassifiersLinearSVR, SVM Regression-SVM RegressionLipschitz continuous, derivative as, Gradient DescentLLE (locally linear embedding), LLE-LLEloading and preprocessing data, Loading and Preprocessing Data withTensorFlow-The TensorFlow Datasets Projectimage preprocessing layers, Image Preprocessing Layers',\n",
       " 'Figure 4-6. Gradient descent pitfallsFortunately, the MSE cost function for a linear regression model happens tobe a convex function, which means that if you pick any two points on thecurve, the line segment joining them is never below the curve. This impliesthat there are no local minima, just one global minimum. It is also acontinuous function with a slope that never changes abruptly.\\u2060 These twofacts have a great consequence: gradient descent is guaranteed to approacharbitrarily closely the global minimum (if you wait long enough and if thelearning rate is not too high). While the cost function has the shape of a bowl, it can be an elongated bowlif the features have very different scales. Figure 4-7 shows gradient descenton a training set where features 1 and 2 have the same scale (on the left), andon a training set where feature 1 has much smaller values than feature 2 (onthe right).\\u206023',\n",
       " 'Figure 8-10. Unrolled Swiss roll using LLEHere’s how LLE works: for each training instance x , the algorithmidentifies its k-nearest neighbors (in the preceding code k = 10), then tries toreconstruct x as a linear function of these neighbors. More specifically, ittries to find the weights w such that the squared distance between x and ∑j=1 m w i,j x (j) is as small as possible, assuming w = 0 if x is not one ofthe k-nearest neighbors of x . Thus the first step of LLE is the constrainedoptimization problem described in Equation 8-4, where W is the weightmatrix containing all the weights w . The second constraint simplynormalizes the weights for each training instance x . Equation 8-4. LLE step 1: linearly modeling local relationshipsW ^ = argmin W ∑ i=1 m x (i) -∑ j=1 m w i,j x (j) 2 subject to w i,j = 0 if x(j) is not one of the k n.n.of x (i) ∑ j=1 m w i,j = 1 for i = 1 , 2 , ⋯ , mAfter this step, the weight matrix W ^ (containing the weights w^i,j) encodesthe local linear relationships between the training instances. The second stepis to map the training instances into a d-dimensional space (where d < n)while preserving these local relationships as much as possible. If z is theimage of x in this d-dimensional space, then we want the squared distancebetween z and ∑ j=1 m w ^ i,j z (j) to be as small as possible. This idea(i)(i)i,j(i)i,j(j)(i)i,j(i)(i)(i)(i)',\n",
       " 'with the same data as the first model but with a regularization constraint. Youcan see that regularization forced the model to have a smaller slope: thismodel does not fit the training data (circles) as well as the first model, but itactually generalizes better to new examples that it did not see during training(squares). Figure 1-24. Regularization reduces the risk of overfittingThe amount of regularization to apply during learning can be controlled by ahyperparameter. A hyperparameter is a parameter of a learning algorithm(not of the model). As such, it is not affected by the learning algorithm itself;it must be set prior to training and remains constant during training. If you setthe regularization hyperparameter to a very large value, you will get analmost flat model (a slope close to zero); the learning algorithm will almostcertainly not overfit the training data, but it will be less likely to find a goodsolution. Tuning hyperparameters is an important part of building a machinelearning system (you will see a detailed example in the next chapter).',\n",
       " 'Under the Hood of Linear SVM ClassifiersA linear SVM classifier predicts the class of a new instance x by firstcomputing the decision function θ x = θ x + ⋯ + θ x , where x is the biasfeature (always equal to 1). If the result is positive, then the predicted class ŷis the positive class (1); otherwise it is the negative class (0). This is exactlylike LogisticRegression (discussed in Chapter 4). NOTEUp to now, I have used the convention of putting all the model parameters in one vector θ,including the bias term θ and the input feature weights θ to θ . This required adding abias input x = 1 to all instances. Another very common convention is to separate the biasterm b (equal to θ ) and the feature weights vector w (containing θ to θ ). In this case, nobias feature needs to be added to the input feature vectors, and the linear SVM’s decisionfunction is equal to w x + b = w x + ⋯ + w x + b. I will use this convention throughoutthe rest of this book. So, making predictions with a linear SVM classifier is quite straightforward. How about training?This requires finding the weights vector w and the biasterm b that make the street, or margin, as wide as possible while limiting thenumber of margin violations. Let’s start with the width of the street: to makeit larger, we need to make w smaller. This may be easier to visualize in 2D,as shown in Figure 5-12.',\n",
       " 'Gradient DescentGradient descent is a generic optimization algorithm capable of findingoptimal solutions to a wide range of problems. The general idea of gradientdescent is to tweak parameters iteratively in order to minimize a costfunction. Suppose you are lost in the mountains in a dense fog, and you can only feelthe slope of the ground below your feet. A good strategy to get to the bottomof the valley quickly is to go downhill in the direction of the steepest slope. This is exactly what gradient descent does: it measures the local gradient ofthe error function with regard to the parameter vector θ, and it goes in thedirection of descending gradient. Once the gradient is zero, you have reacheda minimum!In practice, you start by filling θ with random values (this is called randominitialization). Then you improve it gradually, taking one baby step at a time,each step attempting to decrease the cost function (e.g., the MSE), until thealgorithm converges to a minimum (see Figure 4-3). Figure 4-3. In this depiction of gradient descent, the model parameters are initialized randomly and get',\n",
       " 'Figure 4-25. Softmax regression decision boundariesIn this chapter, you learned various ways to train linear models, both forregression and for classification. You used a closed-form equation to solvelinear regression, as well as gradient descent, and you learned how variouspenalties can be added to the cost function during training to regularize themodel. Along the way, you also learned how to plot learning curves andanalyze them, and how to implement early stopping. Finally, you learnedhow logistic regression and softmax regression work. We’ve opened up thefirst machine learning black boxes!In the next chapters we will open manymore, starting with support vector machines.',\n",
       " 'Regularized Linear ModelsAs you saw in Chapters 1 and 2, a good way to reduce overfitting is toregularize the model (i.e., to constrain it): the fewer degrees of freedom ithas, the harder it will be for it to overfit the data. A simple way to regularizea polynomial model is to reduce the number of polynomial degrees. For a linear model, regularization is typically achieved by constraining theweights of the model. We will now look at ridge regression, lasso regression,and elastic net regression, which implement three different ways to constrainthe weights.',\n",
       " 'Next we will look at polynomial regression, a more complex model that canfit nonlinear datasets. Since this model has more parameters than linearregression, it is more prone to overfitting the training data. We will explorehow to detect whether or not this is the case using learning curves, and thenwe will look at several regularization techniques that can reduce the risk ofoverfitting the training set. Finally, we will examine two more models that are commonly used forclassification tasks: logistic regression and softmax regression. WARNINGThere will be quite a few math equations in this chapter, using basic notions of linearalgebra and calculus. To understand these equations, you will need to know what vectorsand matrices are; how to transpose them, multiply them, and inverse them; and whatpartial derivatives are. If you are unfamiliar with these concepts, please go through thelinear algebra and calculus introductory tutorials available as Jupyter notebooks in theonline supplemental material. For those who are truly allergic to mathematics, you shouldstill go through this chapter and simply skip the equations; hopefully, the text will besufficient to help you understand most of the concepts.',\n",
       " 'Linear RegressionIn Chapter 1 we looked at a simple regression model of life satisfaction:life_satisfaction = θ + θ × GDP_per_capitaThis model is just a linear function of the input feature GDP_per_capita.θand θ are the model’s parameters. More generally, a linear model makes a prediction by simply computing aweighted sum of the input features, plus a constant called the bias term (alsocalled the intercept term), as shown in Equation 4-1. Equation 4-1. Linear regression model predictiony ^ = θ 0 + θ 1 x 1 + θ 2 x 2 + ⋯ + θ n x nIn this equation:ŷ is the predicted value.n is the number of features.x is the i feature value.θ is the j model parameter, including the bias term θ and the featureweights θ , θ , ⋯, θ . This can be written much more concisely using a vectorized form, as shownin Equation 4-2. Equation 4-2. Linear regression model prediction (vectorized form)y^=hθ(x)=θ·xIn this equation:h is the hypothesis function, using the model parameters θ.θ is the model’s parameter vector, containing the bias term θ and thefeature weights θ to θ .0101ithjth012nθ01n',\n",
       " 'h w ^,b ^ ϕ ( x (n) ) = w ^ ⊺ ϕ ( x (n) ) + b ^ = ∑ i=1 m α ^ (i) t (i) ϕ(x (i) ) ⊺ ϕ( x (n) ) + b ^ = ∑ i=1 m α ^ (i) t (i) ϕ (x (i) ) ⊺ ϕ ( x (n) ) + b ^ = ∑ i=1 α ^ (i)>0 m α ^ (i) t (i) K ( x (i) , x (n) ) + b ^Note that since α ≠ 0 only for support vectors, making predictions involvescomputing the dot product of the new input vector x with only the supportvectors, not all the training instances. Of course, you need to use the sametrick to compute the bias term b^ (Equation 5-9). Equation 5-9. Using the kernel trick to compute the bias termb ^ = 1 n s ∑ i=1 α ^ (i) >0 m t (i) - w ^ ⊺ ϕ ( x (i) ) = 1 n s ∑ i=1 α ^ (i) >0 mt (i) - ∑ j=1 m α ^ (j) t (j) ϕ(x (j) ) ⊺ ϕ ( x (i) ) = 1 n s ∑ i=1 α ^ (i) >0 m t (i) -∑ j=1 α ^ (j) >0 m α ^ (j) t (j) K ( x (i) , x (j) )If you are starting to get a headache, that’s perfectly normal: it’s anunfortunate side effect of the kernel trick. NOTEIt is also possible to implement online kernelized SVMs, capable of incremental learning,as described in the papers “Incremental and Decremental Support Vector MachineLearning”\\u2060 and “Fast Kernel Classifiers with Online and Active Learning”.\\u2060 Thesekernelized SVMs are implemented in Matlab and C++. But for large-scale nonlinearproblems, you may want to consider using random forests (see Chapter 7) or neuralnetworks (see Part II).(i)(n)78',\n",
       " 'Figure 4-7. Gradient descent with (left) and without (right) feature scalingAs you can see, on the left the gradient descent algorithm goes straighttoward the minimum, thereby reaching it quickly, whereas on the right it firstgoes in a direction almost orthogonal to the direction of the global minimum,and it ends with a long march down an almost flat valley. It will eventuallyreach the minimum, but it will take a long time. WARNINGWhen using gradient descent, you should ensure that all features have a similar scale (e.g.,using Scikit-Learn’s StandardScaler class), or else it will take much longer to converge. This diagram also illustrates the fact that training a model means searchingfor a combination of model parameters that minimizes a cost function (overthe training set). It is a search in the model’s parameter space. The moreparameters a model has, the more dimensions this space has, and the harderthe search is: searching for a needle in a 300-dimensional haystack is muchtrickier than in 3 dimensions. Fortunately, since the cost function is convex inthe case of linear regression, the needle is simply at the bottom of the bowl.',\n",
       " 'Elastic Net RegressionElastic net regression is a middle ground between ridge regression and lassoregression. The regularization term is a weighted sum of both ridge andlasso’s regularization terms, and you can control the mix ratio r. When r = 0,elastic net is equivalent to ridge regression, and when r = 1, it is equivalent tolasso regression (Equation 4-12). Equation 4-12. Elastic net cost functionJ(θ)=MSE(θ)+r2α∑i=1nθi+(1-r)αm∑i=1nθi2So when should you use elastic net regression, or ridge, lasso, or plain linearregression (i.e., without any regularization)?It is almost always preferable tohave at least a little bit of regularization, so generally you should avoid plainlinear regression. Ridge is a good default, but if you suspect that only a fewfeatures are useful, you should prefer lasso or elastic net because they tend toreduce the useless features’ weights down to zero, as discussed earlier. Ingeneral, elastic net is preferred over lasso because lasso may behaveerratically when the number of features is greater than the number of traininginstances or when several features are strongly correlated. Here is a short example that uses Scikit-Learn’s ElasticNet (l1_ratiocorresponds to the mix ratio r):>>> from sklearn.linear_model import ElasticNet>>> elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)>>> elastic_net.fit(X, y)>>> elastic_net.predict([[1.5]])array([1.54333232])',\n",
       " 'Inmachine learning, a kernel is a function capable of computing the dot productϕ(a) ϕ(b), based only on the original vectors a and b, without having to6⊺⊺2(i) ⊺(j)⊺2⊺',\n",
       " 'The MSE of a linear regression hypothesis h on a training set X is calculatedusing Equation 4-3. Equation 4-3. MSE cost function for a linear regression modelMSE ( X , h θ ) = 1 m ∑ i=1 m (θ ⊺ x (i) -y (i) ) 2Most of these notations were presented in Chapter 2 (see “Notations”). Theonly difference is that we write h instead of just h to make it clear that themodel is parametrized by the vector θ. To simplify notations, we will justwrite MSE(θ) instead of MSE(X, h ).θθθ',\n",
       " 'As a result, it is terribly slow on very large training sets (we will look atsome much faster gradient descent algorithms shortly). However, gradient descent scaleswell with the number of features; training a linear regression model when there arehundreds of thousands of features is much faster using gradient descent than using theNormal equation or SVD decomposition. Once you have the gradient vector, which points uphill, just go in thejjjjθ',\n",
       " 'it is a linear boundary.\\u2060 Each parallel line represents the points where themodel outputs a specific probability, from 15% (bottom left) to 90% (topright). All the flowers beyond the top-right line have over 90% chance ofbeing Iris virginica, according to the model. Figure 4-24. Linear decision boundaryNOTEThe hyperparameter controlling the regularization strength of a Scikit-LearnLogisticRegression model is not alpha (as in other linear models), but its inverse: C. Thehigher the value of C, the less the model is regularized. Just like the other linear models, logistic regression models can beregularized using ℓ or ℓ penalties. Scikit-Learn actually adds an ℓ penaltyby default.14122',\n",
       " 'Figure 4-2. Linear regression model predictionsPerforming linear regression using Scikit-Learn is relatively straightforward:>>> from sklearn.linear_model import LinearRegression>>> lin_reg = LinearRegression()>>> lin_reg.fit(X, y)>>> lin_reg.intercept_, lin_reg.coef_(array([4.21509616]), array([[2.77011339]]))>>> lin_reg.predict(X_new)array([[4.21509616],    [9.75532293]])Notice that Scikit-Learn separates the bias term (intercept_) from the featureweights (coef_). The LinearRegression class is based on thescipy.linalg.lstsq() function (the name stands for “least squares”), which youcould call directly:>>> theta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=1e-6)>>> theta_best_svdarray([[4.21509616],    [2.77011339]])',\n",
       " 'provides better uncertainty estimates. And of course, since it is just regulardropout during training, it also acts like a regularizer.',\n",
       " 'Figure 4-11. Gradient descent paths in parameter spaceTable 4-1 compares the algorithms we’ve discussed so far for linearregression\\u2060 (recall that m is the number of training instances and n is thenumber of features). Table 4-1. Comparison of algorithms for linear regressionAlgorithmLarge mOut-of-coresupportLarge nHyperparamsNormal equationFastNoSlow0SVDFastNoSlow0Batch GDSlowNoFast2Stochastic GDFastYesFast≥2Mini-batch GDFastYesFast≥2There is almost no difference after training: all these algorithms end up withvery similar models and make predictions in exactly the same way.5',\n",
       " 'sklearn.impute. SimpleImputer, Clean the Datasklearn.linear_model. ElasticNet, Elastic Net Regressionsklearn.linear_model. Lasso, Lasso Regressionsklearn.linear_model. LinearRegression, Model-based learning and atypical machine learning workflow, Clean the Data, Feature Scaling andTransformation, The Normal Equation, Computational Complexity,Polynomial Regressionsklearn.linear_model. LogisticRegression, Decision Boundaries, SoftmaxRegression, Using Clustering for Semi-Supervised Learningsklearn.linear_model. Perceptron, The Perceptronsklearn.linear_model. Ridge, Ridge Regressionsklearn.linear_model. RidgeCV, Ridge Regressionsklearn.linear_model. SGDClassifier, Training a Binary Classifier, ThePrecision/Recall Trade-off, The Precision/Recall Trade-off, The ROCCurve-The ROC Curve, Multiclass Classification, SVM Classes andComputational Complexity, Under the Hood of Linear SVM Classifiers,The Perceptronsklearn.linear_model. SGDRegressor, Stochastic Gradient Descent, EarlyStoppingsklearn.manifold.',\n",
       " 'AdaGradConsider the elongated bowl problem again: gradient descent starts byquickly going down the steepest slope, which does not point straight towardthe global optimum, then it very slowly goes down to the bottom of thevalley. It would be nice if the algorithm could correct its direction earlier topoint a bit more toward the global optimum. The AdaGrad algorithm\\u2060achieves this correction by scaling down the gradient vector along thesteepest dimensions (see Equation 11-7). Equation 11-7. AdaGrad algorithm1 .s ← s + ∇ θ J ( θ ) ⊗ ∇ θ J ( θ ) 2 .θ ← θ - η ∇ θ J ( θ ) ⊘ s + εThe first step accumulates the square of the gradients into the vector s (recallthat the ⊗ symbol represents the element-wise multiplication). This vectorizedform is equivalent to computing s ← s + (∂ J(θ) / ∂ θ ) for each element sof the vector s; in other words, each s accumulates the squares of the partialderivative of the cost function with regard to parameter θ . If the cost functionis steep along the i dimension, then s will get larger and larger at eachiteration. The second step is almost identical to gradient descent, but with one bigdifference: the gradient vector is scaled down by a factor of s+ε (the ⊘ symbolrepresents the element-wise division, and ε is a smoothing term to avoiddivision by zero, typically set to 10). This vectorized form is equivalent tosimultaneously computing θi←θi-η\\u200a∂J(θ)/∂θi/si+ε for all parameters θ . In short, this algorithm decays the learning rate, but it does so faster for steepdimensions than for dimensions with gentler slopes. This is called anadaptive learning rate.',\n",
       " 'Batch Gradient DescentTo implement gradient descent, you need to compute the gradient of the costfunction with regard to each model parameter θ . In other words, you need tocalculate how much the cost function will change if you change θ just a littlebit. This is called a partial derivative. It is like asking, “What is the slope ofthe mountain under my feet if I face east”?and then asking the same questionfacing north (and so on for all other dimensions, if you can imagine auniverse with more than three dimensions). Equation 4-5 computes the partialderivative of the MSE with regard to parameter θ , noted ∂ MSE(θ) / ∂θ . Equation 4-5. Partial derivatives of the cost function∂ ∂θ j MSE ( θ ) = 2 m ∑ i=1 m ( θ ⊺ x (i) - y (i) ) x j (i)Instead of computing these partial derivatives individually, you can useEquation 4-6 to compute them all in one go. The gradient vector, noted∇MSE(θ), contains all the partial derivatives of the cost function (one foreach model parameter). Equation 4-6. Gradient vector of the cost function∇ θ MSE ( θ ) = ∂ ∂θ 0 MSE ( θ ) ∂ ∂θ 1 MSE ( θ ) ⋮ ∂ ∂θ n MSE ( θ ) = 2 mX ⊺ ( X θ - y )WARNINGNotice that this formula involves calculations over the full training set X, at each gradientdescent step!This is why the algorithm is called batch gradient descent: it uses the wholebatch of training data at every step (actually, full gradient descent would probably be abetter name). As a result, it is terribly slow on very large training sets (we will look atsome much faster gradient descent algorithms shortly).',\n",
       " 'Linear SVM ClassificationThe fundamental idea behind SVMs is best explained with some visuals. Figure 5-1 shows part of the iris dataset that was introduced at the end ofChapter 4. The two classes can clearly be separated easily with a straight line(they are linearly separable). The left plot shows the decision boundaries ofthree possible linear classifiers. The model whose decision boundary isrepresented by the dashed line is so bad that it does not even separate theclasses properly. The other two models work perfectly on this training set,but their decision boundaries come so close to the instances that these modelswill probably not perform as well on new instances. In contrast, the solid linein the plot on the right represents the decision boundary of an SVM classifier;this line not only separates the two classes but also stays as far away from theclosest training instances as possible. You can think of an SVM classifier asfitting the widest possible street (represented by the parallel dashed lines)between the classes. This is called large margin classification. Figure 5-1. Large margin classificationNotice that adding more training instances “off the street” will not affect thedecision boundary at all: it is fully determined (or “supported”) by theinstances located on the edge of the street.',\n",
       " 'instance with x = 0.2. The root node asks whether x ≤ 0.197. Since it is not,the algorithm goes to the right child node, which asks whether x ≤ 0.772. Since it is, the algorithm goes to the left child node. This is a leaf node, and itpredicts value=0.111. This prediction is the average target value of the 110training instances associated with this leaf node, and it results in a meansquared error equal to 0.015 over these 110 instances. This model’s predictions are represented on the left in Figure 6-5. If you setmax_depth=3, you get the predictions represented on the right. Notice howthe predicted value for each region is always the average target value of theinstances in that region. The algorithm splits each region in a way that makesmost training instances as close as possible to that predicted value. Figure 6-5. Predictions of two decision tree regression modelsThe CART algorithm works as described earlier, except that instead of tryingto split the training set in a way that minimizes impurity, it now tries to splitthe training set in a way that minimizes the MSE.',\n",
       " 'reduce operation, Data parallelism using the mirrored strategyregion proposal network (RPN), You Only Look Onceregression MLPs, Regression MLPs-Regression MLPsregression modelsand classification, Supervised learning, Multioutput Classificationdecision tree tasks, Regression-Regressionforecasting example, Examples of Applicationslasso regression, Lasso Regression-Lasso Regressionlinear regression (see linear regression)logistic regression (see logistic regression)multiple regression, Frame the Problemmultivariate regression, Frame the Problempolynomial regression, Training Models, Polynomial Regression-Polynomial Regressionregression MLPs, Building a Regression MLP Using the Sequential APIridge regression, Ridge Regression-Ridge Regression, Elastic NetRegressionsoftmax regression, Softmax Regression-Softmax RegressionSVM, SVM Regression-SVM Regressionunivariate regression, Frame the Problemregression to the mean, Supervised learningregularization, Overfitting the Training Data, Avoiding Overfitting ThroughRegularization-Max-Norm Regularization',\n",
       " 'square (second-degree polynomial) of each feature in the training set as a newfeature (in this case there is just one feature):>>> from sklearn.preprocessing import PolynomialFeatures>>> poly_features = PolynomialFeatures(degree=2, include_bias=False)>>> X_poly = poly_features.fit_transform(X)>>> X[0]array([-0.75275929])>>> X_poly[0]array([-0.75275929, 0.56664654])X_poly now contains the original feature of X plus the square of this feature. Now we can fit a LinearRegression model to this extended training data(Figure 4-13):>>> lin_reg = LinearRegression()>>> lin_reg.fit(X_poly, y)>>> lin_reg.intercept_, lin_reg.coef_(array([1.78134581]), array([[0.93366893, 0.56456263]]))Figure 4-13. Polynomial regression model predictions',\n",
       " ')polynomial_svm_clf.fit(X, y)Figure 5-6. Linear SVM classifier using polynomial features',\n",
       " 'Kernelized SVMsSuppose you want to apply a second-degree polynomial transformation to atwo-dimensional training set (such as the moons training set), then train alinear SVM classifier on the transformed training set. Equation 5-5 shows thesecond-degree polynomial mapping function ϕ that you want to apply. Equation 5-5. Second-degree polynomial mappingϕ x = ϕ x 1 x 2 = x 1 2 2 x 1 x 2 x 2 2Notice that the transformed vector is 3D instead of 2D. Now let’s look atwhat happens to a couple of 2D vectors, a and b, if we apply this second-degree polynomial mapping and then compute the dot product\\u2060 of thetransformed vectors (see Equation 5-6). Equation 5-6. Kernel trick for a second-degree polynomial mappingϕ (a) ⊺ ϕ ( b ) = a 1 2 2a 1 a 2 a 2 2 ⊺ b 1 2 2 b 1 b 2 b 2 2 = a 1 2 b 1 2 + 2 a 1b 1 a 2 b 2 + a 2 2 b 2 2 = a 1 b 1 +a 2 b 2 2 = a 1 a 2 ⊺ b 1 b 2 2 = (a ⊺ b) 2How about that?The dot product of the transformed vectors is equal to thesquare of the dot product of the original vectors: ϕ(a) ϕ(b) = (a b) . Here is the key insight: if you apply the transformation ϕ to all traininginstances, then the dual problem (see Equation 5-3) will contain the dotproduct ϕ(x ) ϕ(x ). But if ϕ is the second-degree polynomialtransformation defined in Equation 5-5, then you can replace this dot productof transformed vectors simply by (x (i) ⊺ x (j) ) 2 . So, you don’t need totransform the training instances at all; just replace the dot product by itssquare in Equation 5-3. The result will be strictly the same as if you had gonethrough the trouble of transforming the training set and then fitting a linearSVM algorithm, but this trick makes the whole process much morecomputationally efficient. The function K(a, b) = (a b) is a second-degree polynomial kernel. Inmachine learning, a kernel is a function capable of computing the dot productϕ(a) ϕ(b), based only on the original vectors a and b, without having to6⊺⊺2(i) ⊺(j)⊺2⊺',\n",
       " 'In Chapter 2 you used cross-validation to get an estimate of a model’sgeneralization performance. If a model performs well on the training data butgeneralizes poorly according to the cross-validation metrics, then your modelis overfitting. If it performs poorly on both, then it is underfitting. This is oneway to tell when a model is too simple or too complex. Another way to tell is to look at the learning curves, which are plots of themodel’s training error and validation error as a function of the trainingiteration: just evaluate the model at regular intervals during training on boththe training set and the validation set, and plot the results. If the model cannotbe trained incrementally (i.e., if it does not support partial_fit() orwarm_start), then you must train it several times on gradually larger subsetsof the training set. Scikit-Learn has a useful learning_curve() function to help with this: it trainsand evaluates the model using cross-validation. By default it retrains themodel on growing subsets of the training set, but if the model supportsincremental learning you can set exploit_incremental_learning=True whencalling learning_curve() and it will train the model incrementally instead. Thefunction returns the training set sizes at which it evaluated the model, and thetraining and validation scores it measured for each size and for each cross-validation fold. Let’s use this function to look at the learning curves of theplain linear regression model (see Figure 4-15):from sklearn.model_selection import learning_curvetrain_sizes, train_scores, valid_scores = learning_curve(  LinearRegression(), X, y, train_sizes=np.linspace(0.01, 1.0, 40), cv=5,  scoring=\"neg_root_mean_squared_error\")train_errors = -train_scores.mean(axis=1)valid_errors = -valid_scores.mean(axis=1)plt.plot(train_sizes, train_errors, \"r-+\", linewidth=2, label=\"train\")plt.plot(train_sizes, valid_errors, \"b-\", linewidth=3, label=\"valid\")[...] # beautify the figure: add labels, axis, grid, and legendplt.show()',\n",
       " 'classification, as it can output a value that corresponds to the probability ofbelonging to a given class (e.g., 20% chance of being spam). Figure 1-6. A regression problem: predict a value, given an input feature (there are usually multipleinput features, and sometimes multiple output values)NOTEThe words target and label are generally treated as synonyms in supervised learning, buttarget is more common in regression tasks and label is more common in classificationtasks. Moreover, features are sometimes called predictors or attributes. These terms mayrefer to individual samples (e.g., “this car’s mileage feature is equal to 15,000”) or to allsamples (e.g., “the mileage feature is strongly correlated with price”). Unsupervised learningIn unsupervised learning, as you might guess, the training data is unlabeled(Figure 1-7). The system tries to learn without a teacher. For example, say you have a lot of data about your blog’s visitors. You maywant to run a clustering algorithm to try to detect groups of similar visitors(Figure 1-8). At no point do you tell the algorithm which group a visitorbelongs to: it finds those connections without your help. For example, itmight notice that 40% of your visitors are teenagers who love comic booksand generally read your blog after school, while 20% are adults who enjoy',\n",
       " 'Is it a good idea to stop mini-batch gradient descent immediately whenthe validation error goes up?7. Which gradient descent algorithm (among those we discussed) willreach the vicinity of the optimal solution the fastest?Which will actuallyconverge?How can you make the others converge as well?8. Suppose you are using polynomial regression. You plot the learningcurves and you notice that there is a large gap between the training errorand the validation error. What is happening?What are three ways tosolve this?9. Suppose you are using ridge regression and you notice that the trainingerror and the validation error are almost equal and fairly high. Wouldyou say that the model suffers from high bias or high variance?Shouldyou increase the regularization hyperparameter α or reduce it?10.',\n",
       " 'Computational ComplexityThe Normal equation computes the inverse of X X, which is an (n + 1) × (n+ 1) matrix (where n is the number of features). The computationalcomplexity of inverting such a matrix is typically about O(n) to O(n ),depending on the implementation. In other words, if you double the numberof features, you multiply the computation time by roughly 2 = 5.3 to 2 =8. The SVD approach used by Scikit-Learn’s LinearRegression class is aboutO(n ). If you double the number of features, you multiply the computationtime by roughly 4. WARNINGBoth the Normal equation and the SVD approach get very slow when the number offeatures grows large (e.g., 100,000). On the positive side, both are linear with regard to thenumber of instances in the training set (they are O(m)), so they handle large training setsefficiently, provided they can fit in memory. Also, once you have trained your linear regression model (using the Normalequation or any other algorithm), predictions are very fast: the computationalcomplexity is linear with regard to both the number of instances you want tomake predictions on and the number of features. In other words, makingpredictions on twice as many instances (or twice as many features) will takeroughly twice as much time. Now we will look at a very different way to train a linear regression model,which is better suited for cases where there are a large number of features ortoo many training instances to fit in memory.⊺2.432.432',\n",
       " 'Predictions of two decision tree regression modelsThe CART algorithm works as described earlier, except that instead of tryingto split the training set in a way that minimizes impurity, it now tries to splitthe training set in a way that minimizes the MSE. Equation 6-4 shows thecost function that the algorithm tries to minimize. Equation 6-4. CART cost function for regressionJ(k,tk)=mleftmMSEleft+mrightmMSErightwhereMSEnode=∑i∈node(y^node-y(i))2mnodey^node=∑i∈nodey(i)mnodeJust like for classification tasks, decision trees are prone to overfitting whendealing with regression tasks. Without any regularization (i.e., using thedefault hyperparameters), you get the predictions on the left in Figure 6-6.111',\n",
       " 'values for that feature. The distance is based on all the available features. IterativeImputer trains a regression model per feature to predict the missing valuesbased on all the other available features. It then trains the model again on theupdated data, and repeats the process several times, improving the models and thereplacement values at each iteration. SCIKIT-LEARN DESIGNScikit-Learn’s API is remarkably well designed. These are the maindesign principles:\\u2060ConsistencyAll objects share a consistent and simple interface:EstimatorsAny object that can estimate some parameters based on a dataset iscalled an estimator (e.g., a SimpleImputer is an estimator). Theestimation itself is performed by the fit() method, and it takes adataset as a parameter, or two for supervised learning algorithms—thesecond dataset contains the labels. Any other parameter needed toguide the estimation process is considered a hyperparameter (such asa SimpleImputer’s strategy), and it must be set as an instance variable(generally via a constructor parameter). TransformersSome estimators (such as a SimpleImputer) can also transform adataset; these are called transformers. Once again, the API is simple:the transformation is performed by the transform() method with thedataset to transform as a parameter. It returns the transformed dataset. This transformation generally relies on the learned parameters, as isthe case for a SimpleImputer.',\n",
       " 'Chapter 4. Training ModelsSo far we have treated machine learning models and their training algorithmsmostly like black boxes. If you went through some of the exercises in theprevious chapters, you may have been surprised by how much you can getdone without knowing anything about what’s under the hood: you optimizeda regression system, you improved a digit image classifier, and you even builta spam classifier from scratch, all without knowing how they actually work. Indeed, in many situations you don’t really need to know the implementationdetails. However, having a good understanding of how things work can help youquickly home in on the appropriate model, the right training algorithm to use,and a good set of hyperparameters for your task. Understanding what’s underthe hood will also help you debug issues and perform error analysis moreefficiently. Lastly, most of the topics discussed in this chapter will beessential in understanding, building, and training neural networks (discussedin Part II of this book). In this chapter we will start by looking at the linear regression model, one ofthe simplest models there is. We will discuss two very different ways to trainit:Using a “closed-form” equation\\u2060 that directly computes the modelparameters that best fit the model to the training set (i.e., the modelparameters that minimize the cost function over the training set). Using an iterative optimization approach called gradient descent (GD)that gradually tweaks the model parameters to minimize the costfunction over the training set, eventually converging to the same set ofparameters as the first method. We will look at a few variants of gradientdescent that we will use again and again when we study neural networksin Part II: batch GD, mini-batch GD, and stochastic GD.1',\n",
       " 'Each learning step is fast and cheap, so the system can learn aboutnew data on the fly, as it arrives (see Figure 1-14).',\n",
       " 'The model is duplicatedduring training, with one network acting as a teacher and the other acting as astudent. Gradient descent only affects the student, while the teacher’s weightsare just an exponential moving average of the student’s weights. The studentis trained to match the teacher’s predictions: since they’re almost the samemodel, this is called self-distillation. At each training step, the input imagesare augmented in different ways for the teacher and the student, so they don’tsee the exact same image, but their predictions must match. This forces themto come up with high-level representations. To prevent mode collapse, whereboth the student and the teacher would always output the same thing,completely ignoring the inputs, DINO keeps track of a moving average of theteacher’s outputs, and it tweaks the teacher’s predictions to ensure that theyremain centered on zero, on average. DINO also forces the teacher to havehigh confidence in its predictions: this is called sharpening. Together, thesetechniques preserve diversity in the teacher’s outputs. In a 2021 paper,\\u2060 Google researchers showed how to scale ViTs up or4546',\n",
       " 'only chapter without much code), all rather simple, but my goal is to ensureeverything is crystal clear to you before we continue on to the rest of thebook. So grab a coffee and let’s get started!TIPIf you are already familiar with machine learning basics, you may want to skip directly toChapter 2. If you are not sure, try to answer all the questions listed at the end of thechapter before moving on.',\n",
       " 'This function computes θ^=X+y, where X+ is the pseudoinverse of X(specifically, the Moore–Penrose inverse). You can use np.linalg.pinv() tocompute the pseudoinverse directly:>>> np.linalg.pinv(X_b) @ yarray([[4.21509616],    [2.77011339]])The pseudoinverse itself is computed using a standard matrix factorizationtechnique called singular value decomposition (SVD) that can decompose thetraining set matrix X into the matrix multiplication of three matrices U Σ V(see numpy.linalg.svd()). The pseudoinverse is computed as X+=VΣ+U⊺. Tocompute the matrix Σ+, the algorithm takes Σ and sets to zero all valuessmaller than a tiny threshold value, then it replaces all the nonzero valueswith their inverse, and finally it transposes the resulting matrix. Thisapproach is more efficient than computing the Normal equation, plus ithandles edge cases nicely: indeed, the Normal equation may not work if thematrix X X is not invertible (i.e., singular), such as if m < n or if somefeatures are redundant, but the pseudoinverse is always defined.⊺⊺',\n",
       " 'Shouldyou increase the regularization hyperparameter α or reduce it?10. Why would you want to use:',\n",
       " 'Figure 5-7. SVM classifiers with a polynomial kernelTIPAlthough hyperparameters will generally be tuned automatically (e.g., using randomizedsearch), it’s good to have a sense of what each hyperparameter actually does and how itmay interact with other hyperparameters: this way, you can narrow the search to a muchsmaller space.',\n",
       " 'SVCO(m² × n) to O(m³× n)NoYesYesSGDClassifierO(m × n)YesYesNoNow let’s see how the SVM algorithms can also be used for linear andnonlinear regression.',\n",
       " 'StackingThe last ensemble method we will discuss in this chapter is called stacking(short for stacked generalization).\\u2060 It is based on a simple idea: instead ofusing trivial functions (such as hard voting) to aggregate the predictions of allpredictors in an ensemble, why don’t we train a model to perform thisaggregation?Figure 7-11 shows such an ensemble performing a regressiontask on a new instance. Each of the bottom three predictors predicts adifferent value (3.1, 2.7, and 2.9), and then the final predictor (called ablender, or a meta learner) takes these predictions as inputs and makes thefinal prediction (3.0). Figure 7-11. Aggregating predictions using a blending predictor18',\n",
       " 'given the scores of each class for that instance. Just like the logistic regression classifier, by default the softmax regressionclassifier predicts the class with the highest estimated probability (which issimply the class with the highest score), as shown in Equation 4-21. Equation 4-21. Softmax regression classifier predictiony ^ = argmax k σ s(x) k = argmax k s k ( x ) = argmax k (θ (k) ) ⊺ xThe argmax operator returns the value of a variable that maximizes afunction. In this equation, it returns the value of k that maximizes theestimated probability σ(s(x)) . TIPThe softmax regression classifier predicts only one class at a time (i.e., it is multiclass, notmultioutput), so it should be used only with mutually exclusive classes, such as differentspecies of plants. You cannot use it to recognize multiple people in one picture. Now that you know how the model estimates probabilities and makespredictions, let’s take a look at training. The objective is to have a model thatestimates a high probability for the target class (and consequently a lowprobability for the other classes). Minimizing the cost function shown inEquation 4-22, called the cross entropy, should lead to this objective becauseit penalizes the model when it estimates a low probability for a target class. Cross entropy is frequently used to measure how well a set of estimated classprobabilities matches the target classes. Equation 4-22. Cross entropy cost functionJ(Θ)=-1m∑i=1m∑k=1Kyk(i)logp^k(i)In this equation, yk(i) is the target probability that the i instance belongs toclass k. In general, it is either equal to 1 or 0, depending on whether theinstance belongs to the class or not.',\n",
       " 'The CART Training AlgorithmScikit-Learn uses the Classification and Regression Tree (CART) algorithmto train decision trees (also called “growing” trees). The algorithm works byfirst splitting the training set into two subsets using a single feature k and athreshold t (e.g., “petal length ≤ 2.45 cm”). How does it choose k and t ?Itsearches for the pair (k, t ) that produces the purest subsets, weighted by theirsize. Equation 6-2 gives the cost function that the algorithm tries to minimize. Equation 6-2. CART cost function for classificationJ ( k , t k ) = m left m G left + m right m G right where G left/right measuresthe impurity of the left/right subset m left/right is the number of instances inthe left/right subsetOnce the CART algorithm has successfully split the training set in two, itsplits the subsets using the same logic, then the sub-subsets, and so on,recursively. It stops recursing once it reaches the maximum depth (defined bythe max_depth hyperparameter), or if it cannot find a split that will reduceimpurity. A few other hyperparameters (described in a moment) controladditional stopping conditions: min_samples_split, min_samples_leaf,min_weight_fraction_leaf, and max_leaf_nodes. WARNINGAs you can see, the CART algorithm is a greedy algorithm: it greedily searches for anoptimum split at the top level, then repeats the process at each subsequent level. It doesnot check whether or not the split will lead to the lowest possible impurity several levelsdown.',\n",
       " 'tweaked repeatedly to minimize the cost function; the learning step size is proportional to the slope ofthe cost function, so the steps gradually get smaller as the cost approaches the minimumAn important parameter in gradient descent is the size of the steps,determined by the learning rate hyperparameter. If the learning rate is toosmall, then the algorithm will have to go through many iterations toconverge, which will take a long time (see Figure 4-4). Figure 4-4. Learning rate too smallOn the other hand, if the learning rate is too high, you might jump across thevalley and end up on the other side, possibly even higher up than you werebefore. This might make the algorithm diverge, with larger and larger values,failing to find a good solution (see Figure 4-5).',\n",
       " 'LinearSVR(epsilon=0.5, random_state=42))svm_reg.fit(X, y)To tackle nonlinear regression tasks, you can use a kernelized SVM model. Figure 5-11 shows SVM regression on a random quadratic training set, usinga second-degree polynomial kernel. There is some regularization in the leftplot (i.e., a small C value), and much less in the right plot (i.e., a large Cvalue). Figure 5-11. SVM regression using a second-degree polynomial kernelThe following code uses Scikit-Learn’s SVR class (which supports the kerneltrick) to produce the model represented on the left in Figure 5-11:from sklearn.svm import SVRX, y = [...] # a quadratic datasetsvm_poly_reg = make_pipeline(StandardScaler(),               SVR(kernel=\"poly\", degree=2, C=0.01, epsilon=0.1))svm_poly_reg.fit(X, y)The SVR class is the regression equivalent of the SVC class, and theLinearSVR class is the regression equivalent of the LinearSVC class. TheLinearSVR class scales linearly with the size of the training set (just like theLinearSVC class), while the SVR class gets much too slow when the trainingset grows very large (just like the SVC class).',\n",
       " 'learning schedules, Stochastic Gradient DescentLeCun initialization, Glorot and He InitializationLeNet-5, The Architecture of the Visual Cortex, LeNet-5Levenshtein distance, Gaussian RBF Kernelliblinear library, SVM Classes and Computational Complexitylibsvm library, SVM Classes and Computational Complexitylife satisfaction dataset, Model-based learning and a typical machine learningworkflowlikelihood function, Selecting the Number of Clusters-Selecting the Numberof Clusterslinear discriminant analysis (LDA), Other Dimensionality ReductionTechniqueslinear modelsforecasting time series, Forecasting Using a Linear Modellinear regression (see linear regression)regularized, Regularized Linear Models-Early StoppingSVM, Linear SVM Classification-Soft Margin Classificationtraining and running example, Model-based learning and a typicalmachine learning workflowlinear regression, Model-based learning and a typical machine learningworkflow-Model-based learning and a typical machine learning workflow,Linear Regression-Mini-Batch Gradient Descentcomparison of algorithms, Mini-Batch Gradient Descentcomputational complexity, Computational Complexity',\n",
       " 'Figure 5-12. A smaller weight vector results in a larger marginWe also want to avoid margin violations, so we need the decision function tobe greater than 1 for all positive training instances and lower than –1 fornegative training instances. If we define t = –1 for negative instances (wheny = 0) and t = 1 for positive instances (when y = 1), then we can writethis constraint as t (w x + b) ≥ 1 for all instances. We can therefore express the hard margin linear SVM classifier objective asthe constrained optimization problem in Equation 5-1. Equation 5-1. Hard margin linear SVM classifier objectiveminimize w,b 1 2 w ⊺ w subject to t (i) ( w ⊺ x (i) + b ) ≥ 1 for i = 1 , 2 , ⋯ , mNOTEWe are minimizing ½ w w, which is equal to ½∥ w ∥, rather than minimizing ∥ w ∥(the norm of w). Indeed, ½∥ w ∥ has a nice, simple derivative (it is just w), while ∥ w∥ is not differentiable at w = 0. Optimization algorithms often work much better ondifferentiable functions. To get the soft margin objective, we need to introduce a slack variable ζ ≥ 0for each instance:\\u2060 ζ measures how much the i instance is allowed toviolate the margin. We now have two conflicting objectives: make the slackvariables as small as possible to reduce the margin violations, and make ½ ww as small as possible to increase the margin. This is where the Chyperparameter comes in: it allows us to define the trade-off between these(i)(i)(i)(i)(i)⊺(i)⊺22(i)3(i)th⊺',\n",
       " 'opposite direction to go downhill. This means subtracting ∇MSE(θ) from θ. This is where the learning rate η comes into play:\\u2060 multiply the gradientvector by η to determine the size of the downhill step (Equation 4-7). Equation 4-7. Gradient descent stepθ(next step)=θ-η∇θ\\u200aMSE(θ)Let’s look at a quick implementation of this algorithm:eta = 0.1 # learning raten_epochs = 1000m = len(X_b) # number of instancesnp.random.seed(42)theta = np.random.randn(2, 1) # randomly initialized model parametersfor epoch in range(n_epochs):  gradients = 2 / m * X_b. T @ (X_b @ theta - y)  theta = theta - eta * gradientsThat wasn’t too hard!Each iteration over the training set is called an epoch. Let’s look at the resulting theta:>>> thetaarray([[4.21509616],    [2.77011339]])Hey, that’s exactly what the Normal equation found!Gradient descentworked perfectly. But what if you had used a different learning rate (eta)?Figure 4-8 shows the first 20 steps of gradient descent using three differentlearning rates. The line at the bottom of each plot represents the randomstarting point, then each epoch is represented by a darker and darker line.θ4',\n",
       " 'ProjectionIn most real-world problems, training instances are not spread out uniformlyacross all dimensions. Many features are almost constant, while others arehighly correlated (as discussed earlier for MNIST). As a result, all traininginstances lie within (or close to) a much lower-dimensional subspace of thehigh-dimensional space. This sounds very abstract, so let’s look at anexample. In Figure 8-2 you can see a 3D dataset represented by smallspheres. Figure 8-2. A 3D dataset lying close to a 2D subspaceNotice that all training instances lie close to a plane: this is a lower-',\n",
       " 'w i,j (nextstep) = w i,j + η ( y j - y ^ j ) x iIn this equation:w is the connection weight between the i input and the j neuron.x is the i input value of the current training instance.y^j is the output of the j output neuron for the current training instance.y is the target output of the j output neuron for the current traininginstance.η is the learning rate (see Chapter 4). The decision boundary of each output neuron is linear, so perceptrons areincapable of learning complex patterns (just like logistic regressionclassifiers). However, if the training instances are linearly separable,Rosenblatt demonstrated that this algorithm would converge to a solution.\\u2060This is called the perceptron convergence theorem. Scikit-Learn provides a Perceptron class that can be used pretty much as youwould expect—for example, on the iris dataset (introduced in Chapter 4):import numpy as npfrom sklearn.datasets import load_irisfrom sklearn.linear_model import Perceptroniris = load_iris(as_frame=True)X = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].valuesy = (iris.target == 0) # Iris setosaper_clf = Perceptron(random_state=42)per_clf.fit(X, y)X_new = [[2, 0.5], [3, 1]]y_pred = per_clf.predict(X_new) # predicts True and False for these 2 flowersYou may have noticed that the perceptron learning algorithm stronglyresembles stochastic gradient descent (introduced in Chapter 4). In fact,Scikit-Learn’s Perceptron class is equivalent to using an SGDClassifier withi, jththiththjth7',\n",
       " 'Then, during the maximization step, each cluster is updatedusing all the instances in the dataset, with each instance weighted by theestimated probability that it belongs to that cluster. These probabilities arecalled the responsibilities of the clusters for the instances. During the(1)(k)(1)(k)(1)(k)',\n",
       " '15. If your model performs great on the training data but generalizes poorlyto new instances, what is happening?Can you name three possiblesolutions?16. What is a test set, and why would you want to use it?17. What is the purpose of a validation set?18. What is the train-dev set, when do you need it, and how do you use it?19. What can go wrong if you tune hyperparameters using the test set?Solutions to these exercises are available at the end of this chapter’snotebook, at https://homl.info/colab3.1 Fun fact: this odd-sounding name is a statistics term introduced by Francis Galton while he wasstudying the fact that the children of tall people tend to be shorter than their parents. Since thechildren were shorter, he called this regression to the mean. This name was then applied to themethods he used to analyze correlations between variables.2 Notice how animals are rather well separated from vehicles and how horses are close to deer butfar from birds. Figure reproduced with permission from Richard Socher et al., “Zero-ShotLearning Through Cross-Modal Transfer”, Proceedings of the 26th International Conference onNeural Information Processing Systems 1 (2013): 935–943.3 That’s when the system works perfectly.',\n",
       " 'This is very important for neural networks because they aretrained using gradient descent, and as we saw in Chapter 4, gradient descentdoes not converge very well when the features have very different scales. Finally, the code trains the model and evaluates its validation error. Themodel uses the ReLU activation function in the hidden layers, and it uses avariant of gradient descent called Adam (see Chapter 11) to minimize themean squared error, with a little bit of ℓ regularization (which you cancontrol via the alpha hyperparameter):from sklearn.datasets import fetch_california_housingfrom sklearn.metrics import mean_squared_errorfrom sklearn.model_selection import train_test_splitfrom sklearn.neural_network import MLPRegressorfrom sklearn.pipeline import make_pipelinefrom sklearn.preprocessing import StandardScaler2',\n",
       " 'Early StoppingA very different way to regularize iterative learning algorithms such asgradient descent is to stop training as soon as the validation error reaches aminimum. This is called early stopping. Figure 4-20 shows a complex model(in this case, a high-degree polynomial regression model) being trained withbatch gradient descent on the quadratic dataset we used earlier. As the epochsgo by, the algorithm learns, and its prediction error (RMSE) on the trainingset goes down, along with its prediction error on the validation set. After awhile, though, the validation error stops decreasing and starts to go back up. This indicates that the model has started to overfit the training data. Withearly stopping you just stop training as soon as the validation error reachesthe minimum. It is such a simple and efficient regularization technique thatGeoffrey Hinton called it a “beautiful free lunch”. Figure 4-20. Early stopping regularizationTIP',\n",
       " 'Figure 4-1. A randomly generated linear datasetNow let’s compute θ^ using the Normal equation. We will use the inv()function from NumPy’s linear algebra module (np.linalg) to compute theinverse of a matrix, and the dot() method for matrix multiplication:from sklearn.preprocessing import add_dummy_featureX_b = add_dummy_feature(X) # add x0 = 1 to each instancetheta_best = np.linalg.inv(X_b. T @ X_b) @ X_b. T @ yNOTEThe @ operator performs matrix multiplication. If A and B are NumPy arrays, then A @ Bis equivalent to np.matmul(A, B). Many other libraries, like TensorFlow, PyTorch, andJAX, support the @ operator as well. However, you cannot use @ on pure Python arrays(i.e.,lists of lists). The function that we used to generate the data is y = 4 + 3x + Gaussiannoise. Let’s see what the equation found:1',\n",
       " 'The algorithm then measures how much of these error contributionscame from each connection in the layer below, again using the chainrule, working backward until it reaches the input layer. As explainedearlier, this reverse pass efficiently measures the error gradient across allthe connection weights and biases in the network by propagating theerror gradient backward through the network (hence the name of thealgorithm). Finally, the algorithm performs a gradient descent step to tweak all theconnection weights in the network, using the error gradients it justcomputed. WARNINGIt is important to initialize all the hidden layers’ connection weights randomly, or elsetraining will fail. For example, if you initialize all weights and biases to zero, then allneurons in a given layer will be perfectly identical, and thus backpropagation will affectthem in exactly the same way, so they will remain identical. In other words, despite havinghundreds of neurons per layer, your model will act as if it had only one neuron per layer: itwon’t be too smart. If instead you randomly initialize the weights, you break the symmetryand allow backpropagation to train a diverse team of neurons. In short, backpropagation makes predictions for a mini-batch (forward pass),measures the error, then goes through each layer in reverse to measure theerror contribution from each parameter (reverse pass), and finally tweaks theconnection weights and biases to reduce the error (gradient descent step). In order for backprop to work properly, Rumelhart and his colleagues made akey change to the MLP’s architecture: they replaced the step function withthe logistic function, σ(z) = 1 / (1 + exp(–z)), also called the sigmoid function. This was essential because the step function contains only flat segments, sothere is no gradient to work with (gradient descent cannot move on a flatsurface), while the sigmoid function has a well-defined nonzero derivativeeverywhere, allowing gradient descent to make some progress at every step. In fact, the backpropagation algorithm works well with many other activationfunctions, not just the sigmoid function. Here are two other popular choices:',\n",
       " 'Weight update rulefor i = 1 , 2 , ⋯ , m w (i) ← w (i) if y j ^ (i) = y (i) w (i) exp ( α j ) if y j ^ (i) ≠y (i)Then all the instance weights are normalized (i.e., divided by ∑i=1mw(i)). Finally, a new predictor is trained using the updated weights, and the wholeprocess is repeated: the new predictor’s weight is computed, the instanceweights are updated, then another predictor is trained, and so on. Thealgorithm stops when the desired number of predictors is reached, or when aperfect predictor is found. To make predictions, AdaBoost simply computes the predictions of all the(i)1thj15',\n",
       " 'dimensional (2D) subspace of the higher-dimensional (3D) space. If weproject every training instance perpendicularly onto this subspace (asrepresented by the short dashed lines connecting the instances to the plane),we get the new 2D dataset shown in Figure 8-3. Ta-da!We have just reducedthe dataset’s dimensionality from 3D to 2D. Note that the axes correspond tonew features z and z : they are the coordinates of the projections on theplane. Figure 8-3. The new 2D dataset after projection12',\n",
       " 'training models, Training Models-Softmax Regressionlearning curves in, Learning Curves-Learning Curveslinear regression, Training Models, Linear Regression-Mini-BatchGradient Descentlogistic regression, Logistic Regression-Softmax Regressionperceptrons, The Perceptron-The Perceptronpolynomial regression, Training Models, Polynomial Regression-Polynomial Regressiontraining set, What Is Machine Learning?,Testing and Validatingcost function of, Training and Cost Function-Training and Cost Functioninsufficient quantities, Insufficient Quantity of Training Datairrelevant features, Irrelevant Featuresmin-max scaling, Feature Scaling and Transformationnonrepresentative, Nonrepresentative Training Dataoverfitting, Overfitting the Training Data-Overfitting the Training Datapreparing for ML algorithms, Prepare the Data for Machine LearningAlgorithmstraining and evaluating on, Train and Evaluate on the Training Set-Trainand Evaluate on the Training Settransforming data, Feature Scaling and Transformationunderfitting, Underfitting the Training Datavisualizing data, Explore and Visualize the Data to Gain Insightstraining set expansion, Exercises, AlexNet, Pretrained Models for TransferLearning',\n",
       " 'leads to the unconstrained optimization problem described in Equation 8-5. Itlooks very similar to the first step, but instead of keeping the instances fixedand finding the optimal weights, we are doing the reverse: keeping theweights fixed and finding the optimal position of the instances’ images in thelow-dimensional space. Note that Z is the matrix containing all z . Equation 8-5. LLE step 2: reducing dimensionality while preserving relationshipsZ ^ = argmin Z ∑ i=1 m z (i) -∑ j=1 m w ^ i,j z (j) 2Scikit-Learn’s LLE implementation has the following computationalcomplexity: O(m log(m)n log(k)) for finding the k-nearest neighbors, O(mnk )for optimizing the weights, and O(dm ) for constructing the low-dimensionalrepresentations. Unfortunately, the m in the last term makes this algorithmscale poorly to very large datasets. As you can see, LLE is quite different from the projection techniques, and it’ssignificantly more complex, but it can also construct much better low-dimensional representations, especially if the data is nonlinear.(i)322',\n",
       " 'Stepping BackBy now you know a lot about machine learning. However, we went throughso many concepts that you may be feeling a little lost, so let’s step back andlook at the big picture:Machine learning is about making machines get better at some task bylearning from data, instead of having to explicitly code rules. There are many different types of ML systems: supervised or not, batchor online, instance-based or model-based. In an ML project you gather data in a training set, and you feed thetraining set to a learning algorithm. If the algorithm is model-based, ittunes some parameters to fit the model to the training set (i.e., to makegood predictions on the training set itself), and then hopefully it will beable to make good predictions on new cases as well. If the algorithm isinstance-based, it just learns the examples by heart and generalizes tonew instances by using a similarity measure to compare them to thelearned instances. The system will not perform well if your training set is too small, or ifthe data is not representative, is noisy, or is polluted with irrelevantfeatures (garbage in, garbage out). Lastly, your model needs to beneither too simple (in which case it will underfit) nor too complex (inwhich case it will overfit). There’s just one last important topic to cover: once you have trained a model,you don’t want to just “hope” it generalizes to new cases. You want toevaluate it and fine-tune it if necessary. Let’s see how to do that.',\n",
       " 'Finally, we train the model using X_train as both the inputs and thetargets. Similarly, we use X_valid as both the validation inputs andtargets.',\n",
       " 'AdaBoostOne way for a new predictor to correct its predecessor is to pay a bit moreattention to the training instances that the predecessor underfit. This results innew predictors focusing more and more on the hard cases. This is thetechnique used by AdaBoost. For example, when training an AdaBoost classifier, the algorithm first trainsa base classifier (such as a decision tree) and uses it to make predictions onthe training set. The algorithm then increases the relative weight ofmisclassified training instances. Then it trains a second classifier, using theupdated weights, and again makes predictions on the training set, updates theinstance weights, and so on (see Figure 7-7). Figure 7-8 shows the decision boundaries of five consecutive predictors onthe moons dataset (in this example, each predictor is a highly regularizedSVM classifier with an RBF kernel).\\u2060 The first classifier gets manyinstances wrong, so their weights get boosted. The second classifier thereforedoes a better job on these instances, and so on. The plot on the rightrepresents the same sequence of predictors, except that the learning rate ishalved (i.e., the misclassified instance weights are boosted much less at everyiteration). As you can see, this sequential learning technique has somesimilarities with gradient descent, except that instead of tweaking a singlepredictor’s parameters to minimize a cost function, AdaBoost adds predictorsto the ensemble, gradually making it better.14',\n",
       " 'chance, but the model has no way to tell whether a pattern is real or simplythe result of noise in the data. WARNINGOverfitting happens when the model is too complex relative to the amount and noisinessof the training data. Here are possible solutions:Simplify the model by selecting one with fewer parameters (e.g., a linear modelrather than a high-degree polynomial model), by reducing the number of attributesin the training data, or by constraining the model. Gather more training data. Reduce the noise in the training data (e.g., fix data errors and remove outliers). Constraining a model to make it simpler and reduce the risk of overfitting iscalled regularization. For example, the linear model we defined earlier hastwo parameters, θ and θ . This gives the learning algorithm two degrees offreedom to adapt the model to the training data: it can tweak both the height(θ ) and the slope (θ ) of the line. If we forced θ = 0, the algorithm wouldhave only one degree of freedom and would have a much harder time fittingthe data properly: all it could do is move the line up or down to get as closeas possible to the training instances, so it would end up around the mean. Avery simple model indeed!If we allow the algorithm to modify θ but weforce it to keep it small, then the learning algorithm will effectively havesomewhere in between one and two degrees of freedom. It will produce amodel that’s simpler than one with two degrees of freedom, but morecomplex than one with just one.',\n",
       " 'Similarity FeaturesAnother technique to tackle nonlinear problems is to add features computedusing a similarity function, which measures how much each instanceresembles a particular landmark, as we did in Chapter 2 when we added thegeographic similarity features. For example, let’s take the 1D dataset fromearlier and add two landmarks to it at x = –2 and x = 1 (see the left plot inFigure 5-8). Next, we’ll define the similarity function to be the Gaussian RBFwith γ = 0.3. This is a bell-shaped function varying from 0 (very far awayfrom the landmark) to 1 (at the landmark). Now we are ready to compute the new features. For example, let’s look at theinstance x = –1: it is located at a distance of 1 from the first landmark and 2from the second landmark. Therefore, its new features are x = exp(–0.3 × 1 )≈ 0.74 and x = exp(–0.3 × 2 ) ≈ 0.30. The plot on the right in Figure 5-8shows the transformed dataset (dropping the original features). As you cansee, it is now linearly separable. Figure 5-8. Similarity features using the Gaussian RBFYou may wonder how to select the landmarks. The simplest approach is tocreate a landmark at the location of each and every instance in the dataset.',\n",
       " 'The weight matrix W contains all the connection weights. It has one rowper input and one column per neuron. The bias vector b contains all the bias terms: one per neuron. The function ϕ is called the activation function: when the artificialneurons are TLUs, it is a step function (we will discuss other activationfunctions shortly). NOTEIn mathematics, the sum of a matrix and a vector is undefined. However, in data science,we allow “broadcasting”: adding a vector to a matrix means adding it to every row in thematrix. So, XW + b first multiplies X by W—which results in a matrix with one row perinstance and one column per output—then adds the vector b to every row of that matrix,which adds each bias term to the corresponding output, for every instance. Moreover, ϕ isthen applied itemwise to each item in the resulting matrix. So, how is a perceptron trained?The perceptron training algorithm proposedby Rosenblatt was largely inspired by Hebb’s rule. In his 1949 book TheOrganization of Behavior (Wiley), Donald Hebb suggested that when abiological neuron triggers another neuron often, the connection between thesetwo neurons grows stronger. Siegrid Löwel later summarized Hebb’s idea inthe catchy phrase, “Cells that fire together, wire together”; that is, theconnection weight between two neurons tends to increase when they firesimultaneously.',\n",
       " 'LLELocally linear embedding (LLE)\\u2060 is a nonlinear dimensionality reduction(NLDR) technique. It is a manifold learning technique that does not rely onprojections, unlike PCA and random projection. In a nutshell, LLE works byfirst measuring how each training instance linearly relates to its nearestneighbors, and then looking for a low-dimensional representation of thetraining set where these local relationships are best preserved (more detailsshortly). This approach makes it particularly good at unrolling twistedmanifolds, especially when there is not too much noise. The following code makes a Swiss roll, then uses Scikit-Learn’sLocallyLinearEmbedding class to unroll it:from sklearn.datasets import make_swiss_rollfrom sklearn.manifold import LocallyLinearEmbeddingX_swiss, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42)lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10, random_state=42)X_unrolled = lle.fit_transform(X_swiss)The variable t is a 1D NumPy array containing the position of each instancealong the rolled axis of the Swiss roll. We don’t use it in this example, but itcan be used as a target for a nonlinear regression task. The resulting 2D dataset is shown in Figure 8-10. As you can see, the Swissroll is completely unrolled, and the distances between instances are locallywell preserved. However, distances are not preserved on a larger scale: theunrolled Swiss roll should be a rectangle, not this kind of stretched andtwisted band. Nevertheless, LLE did a pretty good job of modeling themanifold.8',\n",
       " 'The result is passed on to thenext layer, its output is computed and passed to the next layer, and so onuntil we get the output of the last layer, the output layer. This is theforward pass: it is exactly like making predictions, except allintermediate results are preserved since they are needed for thebackward pass. Next, the algorithm measures the network’s output error (i.e., it uses aloss function that compares the desired output and the actual output ofthe network, and returns some measure of the error). Then it computes how much each output bias and each connection to theoutput layer contributed to the error. This is done analytically byapplying the chain rule (perhaps the most fundamental rule in calculus),which makes this step fast and precise.10',\n",
       " 'The Normal EquationTo find the value of θ that minimizes the MSE, there exists a closed-formsolution—in other words, a mathematical equation that gives the resultdirectly. This is called the Normal equation (Equation 4-4). Equation 4-4. Normal equationθ ^ = (X ⊺ X) -1  X ⊺  yIn this equation:θ^ is the value of θ that minimizes the cost function.y is the vector of target values containing y to y. Let’s generate some linear-looking data to test this equation on (Figure 4-1):import numpy as npnp.random.seed(42) # to make this code example reproduciblem = 100 # number of instancesX = 2 * np.random.rand(m, 1) # column vectory = 4 + 3 * X + np.random.randn(m, 1) # column vector(1)(m)',\n",
       " 'kernel trick, Polynomial Kernel-SVM Classes and ComputationalComplexity, Kernelized SVMs-Kernelized SVMskernelized SVMs, Kernelized SVMs-Kernelized SVMskernels (convolution kernels), Filters, CNN Architectureskernels (runtimes), Running the Code Examples Using Google ColabKL (Kullback-Leibler) divergence, Softmax Regression, SparseAutoencodersKLDivergenceRegularizer, Sparse AutoencodersKMeans, Custom TransformersKNeighborsClassifier, Multilabel Classification, Multioutput ClassificationKNNImputer, Clean the DataKullback-Leibler (KL) divergence, Softmax Regression, SparseAutoencodersLlabel propagation, Using Clustering for Semi-Supervised Learning-UsingClustering for Semi-Supervised Learninglabels, Frame the Problemin clustering, k-meansimage classification, Classification and Localizationsupervised learning, Supervised learningunlabeled data issue, Unsupervised Pretraining Using StackedAutoencoderslandmarks, Similarity Features',\n",
       " 'Gradient BoostingAnother very popular boosting algorithm is gradient boosting.\\u2060 Just likeAdaBoost, gradient boosting works by sequentially adding predictors to anensemble, each one correcting its predecessor. However, instead of tweakingthe instance weights at every iteration like AdaBoost does, this method triesto fit the new predictor to the residual errors made by the previous predictor. Let’s go through a simple regression example, using decision trees as thebase predictors; this is called gradient tree boosting, or gradient boostedregression trees (GBRT). First, let’s generate a noisy quadratic dataset and fita DecisionTreeRegressor to it:import numpy as npfrom sklearn.tree import DecisionTreeRegressornp.random.seed(42)X = np.random.rand(100, 1) - 0.5y = 3 * X[:, 0] ** 2 + 0.05 * np.random.randn(100) # y = 3x² + Gaussian noisetree_reg1 = DecisionTreeRegressor(max_depth=2, random_state=42)tree_reg1.fit(X, y)Next, we’ll train a second DecisionTreeRegressor on the residual errors madeby the first predictor:y2 = y - tree_reg1.predict(X)tree_reg2 = DecisionTreeRegressor(max_depth=2, random_state=43)tree_reg2.fit(X, y2)And then we’ll train a third regressor on the residual errors made by thesecond predictor:y3 = y2 - tree_reg2.predict(X)tree_reg3 = DecisionTreeRegressor(max_depth=2, random_state=44)tree_reg3.fit(X, y3)Now we have an ensemble containing three trees. It can make predictions on17',\n",
       " 'Chapter 5. Support VectorMachinesA support vector machine (SVM) is a powerful and versatile machinelearning model, capable of performing linear or nonlinear classification,regression, and even novelty detection. SVMs shine with small to medium-sized nonlinear datasets (i.e., hundreds to thousands of instances), especiallyfor classification tasks. However, they don’t scale very well to very largedatasets, as you will see. This chapter will explain the core concepts of SVMs, how to use them, andhow they work. Let’s jump right in!',\n",
       " 'Learning CurvesIf you perform high-degree polynomial regression, you will likely fit thetraining data much better than with plain linear regression. For example,Figure 4-14 applies a 300-degree polynomial model to the preceding trainingdata, and compares the result with a pure linear model and a quadratic model(second-degree polynomial). Notice how the 300-degree polynomial modelwiggles around to get as close as possible to the training instances. Figure 4-14. High-degree polynomial regressionThis high-degree polynomial regression model is severely overfitting thetraining data, while the linear model is underfitting it. The model that willgeneralize best in this case is the quadratic model, which makes sensebecause the data was generated using a quadratic model. But in general youwon’t know what function generated the data, so how can you decide howcomplex your model should be?How can you tell that your model isoverfitting or underfitting the data?',\n",
       " 'X is a matrix containing all the feature values (excluding labels) ofall instances in the dataset. There is one row per instance, and the irow is equal to the transpose of x , noted (x ) .\\u2060For example, if the first district is as just described, then thematrix X looks like this:X = (x (1) ) ⊺ (x (2) ) ⊺ ⋮ (x (1999) ) ⊺ (x (2000) ) ⊺ = - 118.2933.91 1,416 38,372 ⋮ ⋮ ⋮ ⋮h is your system’s prediction function, also called a hypothesis. When your system is given an instance’s feature vector x , itoutputs a predicted value ŷ = h(x ) for that instance (ŷ ispronounced “y-hat”). For example, if your system predicts that the median housingprice in the first district is $158,400, then ŷ = h(x) =158,400. The prediction error for this district is ŷ – y =2,000. RMSE(X,h) is the cost function measured on the set of examplesusing your hypothesis h. We use lowercase italic font for scalar values (such as m or y ) andfunction names (such as h), lowercase bold font for vectors (such as x ),and uppercase bold font for matrices (such as X). Although the RMSE is generally the preferred performance measure forregression tasks, in some contexts you may prefer to use another function. For example, if there are many outlier districts. In that case, you mayconsider using the mean absolute error (MAE, also called the averageabsolute deviation), shown in Equation 2-2:Equation 2-2. Mean absolute error (MAE)MAE ( X , h ) = 1 m ∑ i=1 m h ( x (i) ) - y (i)Both the RMSE and the MAE are ways to measure the distance between twoth(i)(i) ⊺3(i)(i)(i)(1)(1)(1)(1)(i)(i)',\n",
       " 'Figure 10-12. Correctly classified Fashion MNIST imagesNow you know how to use the sequential API to build, train, and evaluate aclassification MLP. But what about regression?',\n",
       " '(n_iter_no_change). It starts with a learning rate of 0.01 (eta0), using thedefault learning schedule (different from the one we used). Lastly, it does notuse any regularization (penalty=None; more details on this shortly):from sklearn.linear_model import SGDRegressorsgd_reg = SGDRegressor(max_iter=1000, tol=1e-5, penalty=None, eta0=0.01,            n_iter_no_change=100, random_state=42)sgd_reg.fit(X, y.ravel()) # y.ravel() because fit() expects 1D targetsOnce again, you find a solution quite close to the one returned by the Normalequation:>>> sgd_reg.intercept_, sgd_reg.coef_(array([4.21278812]), array([2.77270267]))TIPAll Scikit-Learn estimators can be trained using the fit() method, but some estimators alsohave a partial_fit() method that you can call to run a single round of training on one ormore instances (it ignores hyperparameters like max_iter or tol). Repeatedly callingpartial_fit() will gradually train the model. This is useful when you need more control overthe training process. Other models have a warm_start hyperparameter instead (and somehave both): if you set warm_start=True, calling the fit() method on a trained model willnot reset the model; it will just continue training where it left off, respectinghyperparameters like max_iter and tol. Note that fit() resets the iteration counter used bythe learning schedule, while partial_fit() does not.',\n",
       " 'The gradient vector of this cost function with regard to θ is given byEquation 4-23. Equation 4-23. Cross entropy gradient vector for class k∇ θ (k) J ( Θ ) = 1 m ∑ i=1 m p ^ k (i) - y k (i) x (i)Now you can compute the gradient vector for every class, then use gradientdescent (or any other optimization algorithm) to find the parameter matrix Θthat minimizes the cost function. Let’s use softmax regression to classify the iris plants into all three classes. Scikit-Learn’s LogisticRegression classifier uses softmax regressionautomatically when you train it on more than two classes (assuming you usesolver=\"lbfgs\", which is the default). It also applies ℓ regularization by3x(k)2',\n",
       " 'optimized and runs much faster). PredictorsFinally, some estimators, given a dataset, are capable of makingpredictions; they are called predictors. For example, theLinearRegression model in the previous chapter was a predictor:given a country’s GDP per capita, it predicted life satisfaction. Apredictor has a predict() method that takes a dataset of new instancesand returns a dataset of corresponding predictions. It also has ascore() method that measures the quality of the predictions, given atest set (and the corresponding labels, in the case of supervisedlearning algorithms). InspectionAll the estimator’s hyperparameters are accessible directly via publicinstance variables (e.g., imputer.strategy), and all the estimator’s learnedparameters are accessible via public instance variables with anunderscore suffix (e.g., imputer.statistics_). Nonproliferation of classesDatasets are represented as NumPy arrays or SciPy sparse matrices,instead of homemade classes. Hyperparameters are just regular Pythonstrings or numbers. CompositionExisting building blocks are reused as much as possible. For example, itis easy to create a Pipeline estimator from an arbitrary sequence oftransformers followed by a final estimator, as you will see. Sensible defaultsScikit-Learn provides reasonable default values for most parameters,making it easy to quickly create a baseline working system. Scikit-Learn transformers output NumPy arrays (or sometimes SciPy sparsematrices) even when they are fed Pandas DataFrames as input.\\u2060 So, the1011',\n",
       " 'Well, the good news is thatyou can plug the formula for w ^ from Equation 5-4 into the decisionfunction for a new instance x, and you get an equation with only dotproducts between input vectors. This makes it possible to use the kernel trick(Equation 5-8). Equation 5-8. Making predictions with a kernelized SVM⊺(i)(i)(n)',\n",
       " 'Exercises1. Which linear regression training algorithm can you use if you have atraining set with millions of features?2. Suppose the features in your training set have very different scales. Which algorithms might suffer from this, and how?What can you doabout it?3. Can gradient descent get stuck in a local minimum when training alogistic regression model?4. Do all gradient descent algorithms lead to the same model, provided youlet them run long enough?5. Suppose you use batch gradient descent and you plot the validation errorat every epoch. If you notice that the validation error consistently goesup, what is likely going on?How can you fix this?6. Is it a good idea to stop mini-batch gradient descent immediately whenthe validation error goes up?7.',\n",
       " 'Figure 5-13. The hinge loss (left) and the squared hinge loss (right)Next, we’ll look at yet another way to train a linear SVM classifier: solvingthe dual problem.',\n",
       " 'Gaussian RBF KernelJust like the polynomial features method, the similarity features method canbe useful with any machine learning algorithm, but it may be computationallyexpensive to compute all the additional features (especially on large trainingsets). Once again the kernel trick does its SVM magic, making it possible toobtain a similar result as if you had added many similarity features, butwithout actually doing so. Let’s try the SVC class with the Gaussian RBFkernel:rbf_kernel_svm_clf = make_pipeline(StandardScaler(),                  SVC(kernel=\"rbf\", gamma=5, C=0.001))rbf_kernel_svm_clf.fit(X, y)This model is represented at the bottom left in Figure 5-9. The other plotsshow models trained with different values of hyperparameters gamma (γ) andC. Increasing gamma makes the bell-shaped curve narrower (see the lefthandplots in Figure 5-8). As a result, each instance’s range of influence is smaller:the decision boundary ends up being more irregular, wiggling aroundindividual instances. Conversely, a small gamma value makes the bell-shapedcurve wider: instances have a larger range of influence, and the decisionboundary ends up smoother. So γ acts like a regularization hyperparameter: ifyour model is overfitting, you should reduce γ; if it is underfitting, you shouldincrease γ (similar to the C hyperparameter).',\n",
       " 'These learning curves look a bit like the previous ones, but there are two veryimportant differences:The error on the training data is much lower than before. There is a gap between the curves. This means that the model performssignificantly better on the training data than on the validation data,which is the hallmark of an overfitting model. If you used a much largertraining set, however, the two curves would continue to get closer. TIPOne way to improve an overfitting model is to feed it more training data until thevalidation error reaches the training error. THE BIAS/VARIANCE TRADE-OFFAn important theoretical result of statistics and machine learning is thefact that a model’s generalization error can be expressed as the sum ofthree very different errors:BiasThis part of the generalization error is due to wrong assumptions,such as assuming that the data is linear when it is actually quadratic. A high-bias model is most likely to underfit the training data.\\u2060VarianceThis part is due to the model’s excessive sensitivity to smallvariations in the training data. A model with many degrees offreedom (such as a high-degree polynomial model) is likely to havehigh variance and thus overfit the training data. Irreducible errorThis part is due to the noisiness of the data itself. The only way to6',\n",
       " 'The hyperparameter coef0 controlshow much the model is influenced by high-degree terms versus low-degreeterms.',\n",
       " 'their inputs. It also manages a vector of bias terms (one per neuron). When it receives some input data, it computes Equation 10-2. Then we add a second Dense hidden layer with 100 neurons, also usingthe ReLU activation function. Finally, we add a Dense output layer with 10 neurons (one per class),using the softmax activation function because the classes are exclusive. TIPSpecifying activation=\"relu\" is equivalent to specifyingactivation=tf.keras.activations.relu. Other activation functions are available in thetf.keras.activations package. We will use many of them in this book; seehttps://keras.io/api/layers/activations for the full list. We will also define our own customactivation functions in Chapter 12. Instead of adding the layers one by one as we just did, it’s often moreconvenient to pass a list of layers when creating the Sequential model. Youcan also drop the Input layer and instead specify the input_shape in the firstlayer:model = tf.keras. Sequential([  tf.keras.layers. Flatten(input_shape=[28, 28]),  tf.keras.layers. Dense(300, activation=\"relu\"),  tf.keras.layers.',\n",
       " 'between the gradient vectors around that point.12 Photos reproduced from the corresponding Wikipedia pages. Iris virginica photo by FrankMayfield (Creative Commons BY-SA 2.0), Iris versicolor photo by D. Gordon E. Robertson(Creative Commons BY-SA 3.0), Iris setosa photo public domain.13 NumPy’s reshape() function allows one dimension to be –1, which means “automatic”: thevalue is inferred from the length of the array and the remaining dimensions.14 It is the set of points x such that θ + θ x + θ x = 0, which defines a straight line.01 12 2',\n",
       " 'StackingClassifier, Stackingsklearn.ensemble. StackingRegressor, Stackingsklearn.ensemble. VotingClassifier, Voting Classifierssklearn.externals.joblib, Launch, Monitor, and Maintain Your System-Launch, Monitor, and Maintain Your Systemsklearn.feature_selection. SelectFromModel, Analyzing the Best Modelsand Their Errorssklearn.impute. IterativeImputer, Clean the Datasklearn.impute. KNNImputer, Clean the Data']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b31c84d9-48b1-4a35-a5f5-3ab40ea6ddec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# Load the model, here we use our base sized model\n",
    "model = CrossEncoder(\"mixedbread-ai/mxbai-rerank-base-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4300c8f4-2727-4390-8139-7ae2dd616c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.rank(query, top_k_chunks, return_documents=True, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a2b4a251-96db-4855-9995-8e8f87b8865c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'corpus_id': 0,\n",
       "  'score': np.float32(0.98926395),\n",
       "  'text': 'Ridge RegressionRidge regression (also called Tikhonov regularization) is a regularizedversion of linear regression: a regularization term equal to αm∑i=1nθi2 isadded to the MSE. This forces the learning algorithm to not only fit the databut also keep the model weights as small as possible. Note that theregularization term should only be added to the cost function during training. Once the model is trained, you want to use the unregularized MSE (or theRMSE) to evaluate the model’s performance. The hyperparameter α controls how much you want to regularize the model. If α = 0, then ridge regression is just linear regression. If α is very large, thenall weights end up very close to zero and the result is a flat line going throughthe data’s mean. Equation 4-8 presents the ridge regression cost function.\\u2060Equation 4-8. Ridge regression cost functionJ(θ)=MSE(θ)+αm∑i=1nθi2Note that the bias term θ is not regularized (the sum starts at i = 1, not 0). Ifwe define w as the vector of feature weights (θ to θ ), then the regularizationterm is equal to α(∥ w ∥) / m, where ∥ w ∥ represents the ℓ norm ofthe weight vector.\\u2060 For batch gradient descent, just add 2αw / m to the partof the MSE gradient vector that corresponds to the feature weights, withoutadding anything to the gradient of the bias term (see Equation 4-6). WARNINGIt is important to scale the data (e.g., using a StandardScaler) before performing ridgeregression, as it is sensitive to the scale of the input features. This is true of mostregularized models.'},\n",
       " {'corpus_id': 3,\n",
       "  'score': np.float32(0.93800503),\n",
       "  'text': 'Figure 4-19. Lasso versus ridge regularizationThe two bottom plots show the same thing but with an ℓ penalty instead. Inthe bottom-left plot, you can see that the ℓ loss decreases as we get closer tothe origin, so gradient descent just takes a straight path toward that point. Inthe bottom-right plot, the contours represent ridge regression’s cost function(i.e.,an MSE cost function plus an ℓ loss). As you can see, the gradients getsmaller as the parameters approach the global optimum, so gradient descentnaturally slows down. This limits the bouncing around, which helps ridgeconverge faster than lasso regression. Also note that the optimal parameters(represented by the red square) get closer and closer to the origin when youincrease α, but they never get eliminated entirely. TIPTo keep gradient descent from bouncing around the optimum at the end when using lasso222'},\n",
       " {'corpus_id': 1,\n",
       "  'score': np.float32(0.91105497),\n",
       "  'text': 'PolynomialFeatures(degree=10), then it is scaled using a StandardScaler, andfinally the ridge models are applied to the resulting features: this ispolynomial regression with ridge regularization. Note how increasing α leadsto flatter (i.e., less extreme, more reasonable) predictions, thus reducing themodel’s variance but increasing its bias. Figure 4-17. Linear (left) and a polynomial (right) models, both with various levels of ridgeregularizationAs with linear regression, we can perform ridge regression either bycomputing a closed-form equation or by performing gradient descent. Thepros and cons are the same. Equation 4-9 shows the closed-form solution,where A is the (n + 1) × (n + 1) identity matrix,\\u2060 except with a 0 in the top-left cell, corresponding to the bias term. Equation 4-9. Ridge regression closed-form solutionθ ^ = (X ⊺ X+αA) -1  X ⊺  yHere is how to perform ridge regression with Scikit-Learn using a closed-form solution (a variant of Equation 4-9 that uses a matrix factorizationtechnique by André-Louis Cholesky):>>> from sklearn.linear_model import Ridge>>> ridge_reg = Ridge(alpha=0.1, solver=\"cholesky\")>>> ridge_reg.fit(X, y)>>> ridge_reg.predict([[1.5]])array([[1.55325833]])910'},\n",
       " {'corpus_id': 5,\n",
       "  'score': np.float32(0.56317425),\n",
       "  'text': 'Lasso RegressionLeast absolute shrinkage and selection operator regression (usually simplycalled lasso regression) is another regularized version of linear regression:just like ridge regression, it adds a regularization term to the cost function,but it uses the ℓ norm of the weight vector instead of the square of the ℓnorm (see Equation 4-10). Notice that the ℓ norm is multiplied by 2α,whereas the ℓ norm was multiplied by α / m in ridge regression. Thesefactors were chosen to ensure that the optimal α value is independent fromthe training set size: different norms lead to different factors (see Scikit-Learnissue #15657 for more details). Equation 4-10. Lasso regression cost functionJ(θ)=MSE(θ)+2α∑i=1nθiFigure 4-18 shows the same thing as Figure 4-17 but replaces the ridgemodels with lasso models and uses different α values. Figure 4-18. Linear (left) and polynomial (right) models, both using various levels of lassoregularizationAn important characteristic of lasso regression is that it tends to eliminate theweights of the least important features (i.e., set them to zero). For example,the dashed line in the righthand plot in Figure 4-18 (with α = 0.01) looksroughly cubic: all the weights for the high-degree polynomial features areequal to zero. In other words, lasso regression automatically performs feature1212'},\n",
       " {'corpus_id': 17,\n",
       "  'score': np.float32(0.5503044),\n",
       "  'text': 'Regularized Linear ModelsAs you saw in Chapters 1 and 2, a good way to reduce overfitting is toregularize the model (i.e., to constrain it): the fewer degrees of freedom ithas, the harder it will be for it to overfit the data. A simple way to regularizea polynomial model is to reduce the number of polynomial degrees. For a linear model, regularization is typically achieved by constraining theweights of the model. We will now look at ridge regression, lasso regression,and elastic net regression, which implement three different ways to constrainthe weights.'}]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b8e37380-e419-4dc8-b53e-307be45e01ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ridge RegressionRidge regression (also called Tikhonov regularization) is a regularizedversion of linear regression: a regularization term equal to αm∑i=1nθi2 isadded to the MSE. This forces the learning algorithm to not only fit the databut also keep the model weights as small as possible. Note that theregularization term should only be added to the cost function during training. Once the model is trained, you want to use the unregularized MSE (or theRMSE) to evaluate the model’s performance. The hyperparameter α controls how much you want to regularize the model. If α = 0, then ridge regression is just linear regression. If α is very large, thenall weights end up very close to zero and the result is a flat line going throughthe data’s mean. Equation 4-8 presents the ridge regression cost function.\\u2060Equation 4-8. Ridge regression cost functionJ(θ)=MSE(θ)+αm∑i=1nθi2Note that the bias term θ is not regularized (the sum starts at i = 1, not 0). Ifwe define w as the vector of feature weights (θ to θ ), then the regularizationterm is equal to α(∥ w ∥) / m, where ∥ w ∥ represents the ℓ norm ofthe weight vector.\\u2060 For batch gradient descent, just add 2αw / m to the partof the MSE gradient vector that corresponds to the feature weights, withoutadding anything to the gradient of the bias term (see Equation 4-6). WARNINGIt is important to scale the data (e.g., using a StandardScaler) before performing ridgeregression, as it is sensitive to the scale of the input features. This is true of mostregularized models.',\n",
       " 'PolynomialFeatures(degree=10), then it is scaled using a StandardScaler, andfinally the ridge models are applied to the resulting features: this ispolynomial regression with ridge regularization. Note how increasing α leadsto flatter (i.e., less extreme, more reasonable) predictions, thus reducing themodel’s variance but increasing its bias. Figure 4-17. Linear (left) and a polynomial (right) models, both with various levels of ridgeregularizationAs with linear regression, we can perform ridge regression either bycomputing a closed-form equation or by performing gradient descent. Thepros and cons are the same. Equation 4-9 shows the closed-form solution,where A is the (n + 1) × (n + 1) identity matrix,\\u2060 except with a 0 in the top-left cell, corresponding to the bias term. Equation 4-9. Ridge regression closed-form solutionθ ^ = (X ⊺ X+αA) -1  X ⊺  yHere is how to perform ridge regression with Scikit-Learn using a closed-form solution (a variant of Equation 4-9 that uses a matrix factorizationtechnique by André-Louis Cholesky):>>> from sklearn.linear_model import Ridge>>> ridge_reg = Ridge(alpha=0.1, solver=\"cholesky\")>>> ridge_reg.fit(X, y)>>> ridge_reg.predict([[1.5]])array([[1.55325833]])910',\n",
       " 'And using stochastic gradient descent:\\u2060>>> sgd_reg = SGDRegressor(penalty=\"l2\", alpha=0.1 / m, tol=None,...            max_iter=1000, eta0=0.01, random_state=42)...>>> sgd_reg.fit(X, y.ravel()) # y.ravel() because fit() expects 1D targets>>> sgd_reg.predict([[1.5]])array([1.55302613])The penalty hyperparameter sets the type of regularization term to use. Specifying \"l2\" indicates that you want SGD to add a regularization term tothe MSE cost function equal to alpha times the square of the ℓ norm of theweight vector. This is just like ridge regression, except there’s no division bym in this case; that’s why we passed alpha=0.1 / m, to get the same result asRidge(alpha=0.1). TIPThe RidgeCV class also performs ridge regression, but it automatically tuneshyperparameters using cross-validation. It’s roughly equivalent to using GridSearchCV,but it’s optimized for ridge regression and runs much faster. Several other estimators(mostly linear) also have efficient CV variants, such as LassoCV and ElasticNetCV.102',\n",
       " 'Figure 4-19. Lasso versus ridge regularizationThe two bottom plots show the same thing but with an ℓ penalty instead. Inthe bottom-left plot, you can see that the ℓ loss decreases as we get closer tothe origin, so gradient descent just takes a straight path toward that point. Inthe bottom-right plot, the contours represent ridge regression’s cost function(i.e.,an MSE cost function plus an ℓ loss). As you can see, the gradients getsmaller as the parameters approach the global optimum, so gradient descentnaturally slows down. This limits the bouncing around, which helps ridgeconverge faster than lasso regression. Also note that the optimal parameters(represented by the red square) get closer and closer to the origin when youincrease α, but they never get eliminated entirely. TIPTo keep gradient descent from bouncing around the optimum at the end when using lasso222',\n",
       " 'This is true of mostregularized models. Figure 4-17 shows several ridge models that were trained on some very noisylinear data using different α values. On the left, plain ridge models are used,leading to linear predictions. On the right, the data is first expanded using701n2 2228']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k_chunks[:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_scratch",
   "language": "python",
   "name": "rag_scratch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
